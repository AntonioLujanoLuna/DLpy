// Python Files Concatenated on 01/18/2025 19:03:31
// ----------------------------------------


// File: C:\Users\aluja\Desktop\DLpy\DLpy\__init__.py
// ----------------------------------------
"""
DLpy: A Deep Learning Library with DAG-based Autograd

This library provides a PyTorch-like interface for building and training neural networks,
with a focus on clear implementation and educational value.
"""

from .core import Tensor, Function, Context
from .ops import Add, Multiply, Reshape

__version__ = "0.1.0"

__all__ = [
    'Tensor',
    'Function',
    'Context',
    'Add',
    'Multiply',
    'Reshape',
]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\__init__.py
// ----------------------------------------
"""
Core functionality for DLpy.

This module contains the fundamental building blocks of the deep learning library.
"""

from .tensor import Tensor
from .function import Function
from .context import Context
from .autograd import AutogradEngine, get_autograd_engine

__all__ = ['Tensor', 'Function', 'Context', 'AutogradEngine', 'get_autograd_engine']

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\autograd.py
// ----------------------------------------
from typing import Dict, Set, List, Optional, Tuple, Union
import numpy as np
from collections import defaultdict
import warnings

class Edge:
    """
    Represents a directed edge in the computational graph.
    
    Each edge connects a source node (input tensor) to a destination node
    (output tensor) and stores gradient information for that connection.
    """
    
    def __init__(self, src: 'Node', dst: 'Node'):
        self.src = src
        self.dst = dst
        self.grad: Optional[np.ndarray] = None
        
class Node:
    """
    Represents a node in the computational graph.
    
    Each node corresponds to an operation in the computation and maintains
    connections to its inputs and outputs through edges.
    """
    
    def __init__(self, tensor: 'Tensor'):
        self.tensor = tensor
        self.in_edges: List[Edge] = []
        self.out_edges: List[Edge] = []
        self._backward_fn = tensor._backward_fn

class AutogradEngine:
    """
    Engine for managing automatic differentiation computations.
    
    This class handles the creation and execution of the computational graph,
    manages gradient computation and accumulation, and provides utilities for
    graph manipulation and visualization.
    """
    
    def __init__(self):
        self._nodes: Dict[int, Node] = {}
        self._edges: Set[Edge] = set()
        self._currently_computing_gradients = False
        
    def register_tensor(self, tensor: 'Tensor') -> None:
        """
        Registers a tensor with the autograd engine.
        
        Args:
            tensor: Tensor to register
        """
        if id(tensor) not in self._nodes:
            self._nodes[id(tensor)] = Node(tensor)
            
    def add_edge(self, src: 'Tensor', dst: 'Tensor') -> None:
        """
        Adds a directed edge between two tensors in the computational graph.
        
        Args:
            src: Source tensor
            dst: Destination tensor
        """
        src_node = self._nodes[id(src)]
        dst_node = self._nodes[id(dst)]
        
        edge = Edge(src_node, dst_node)
        src_node.out_edges.append(edge)
        dst_node.in_edges.append(edge)
        self._edges.add(edge)
        
    def backward(self, tensor: 'Tensor', gradient: Optional[np.ndarray] = None) -> None:
        """Executes backward pass starting from the given tensor."""
        if self._currently_computing_gradients:
            raise RuntimeError("Nested gradient computation detected")
            
        self._currently_computing_gradients = True
        try:
            # Initialize grad_dict as a regular dictionary
            grad_dict: Dict[int, np.ndarray] = {}
            
            # If no gradient is provided, assume it's 1 (for scalar outputs)
            if gradient is None:
                if tensor.data.shape == ():
                    grad_dict[id(tensor)] = np.array(1.0)
                else:
                    grad_dict[id(tensor)] = np.ones_like(tensor.data)
            else:
                grad_dict[id(tensor)] = gradient
            
            # Perform topological sort
            sorted_nodes = self._topological_sort(tensor)
            
            # Traverse nodes in reverse topological order
            for node in reversed(sorted_nodes):
                node_id = id(node.tensor)
                if node_id not in grad_dict or not node.tensor.requires_grad:
                    continue  # No gradient to propagate
                
                current_grad = grad_dict[node_id]
                
                if node.tensor._backward_fn is not None:
                    node.tensor._backward_fn(current_grad, grad_dict)
                
                # Accumulate gradients for leaf nodes
                if len(node.in_edges) == 0 and node.tensor.requires_grad:
                    if node.tensor.grad is None:
                        node.tensor.grad = current_grad
                    else:
                        try:
                            node.tensor.grad += current_grad
                        except ValueError:
                            # If shapes don't match, reshape current_grad
                            node.tensor.grad += current_grad.reshape(node.tensor.grad.shape)
        finally:
            self._currently_computing_gradients = False

    def _topological_sort(self, start_tensor: 'Tensor') -> List[Node]:
        """
        Performs topological sort on the computation graph.
        
        Args:
            start_tensor: Tensor to start the sort from
            
        Returns:
            List of nodes in topological order
            
        Raises:
            RuntimeError: If graph contains cycles
        """
        result: List[Node] = []
        visited: Set[Node] = set()
        temp_visited: Set[Node] = set()
        
        def visit(node: Node) -> None:
            if node in temp_visited:
                raise RuntimeError("Cycle detected in computation graph")
                
            if node not in visited:
                temp_visited.add(node)
                for edge in node.in_edges:
                    visit(edge.src)
                temp_visited.remove(node)
                visited.add(node)
                result.append(node)
                
        visit(self._nodes[id(start_tensor)])
        return result
            
    def clear(self) -> None:
        """Clears the computational graph."""
        self._nodes.clear()
        self._edges.clear()
    
    def validate_graph(self) -> List[str]:
        """
        Validates the computational graph structure.
        """
        warnings: List[str] = []
        
        # If no nodes in graph
        if not self._nodes:
            return warnings

        # Step 1: Find all nodes that are part of computations
        active_nodes = set()
        output_nodes = []
        for node in self._nodes.values():
            if not node.out_edges:  # Output node
                output_nodes.append(node)
            if node.in_edges or node.out_edges:  # Node is part of a computation
                active_nodes.add(node)

        # Step 2: Find all connected nodes starting from outputs
        connected_nodes = set()
        for output_node in output_nodes:
            stack = [output_node]
            while stack:
                curr = stack.pop()
                connected_nodes.add(curr)
                for edge in curr.in_edges:
                    if edge.src not in connected_nodes:
                        stack.append(edge.src)
                        
        # Step 3: Find nodes not connected to outputs
        all_nodes = set(self._nodes.values())
        unconnected_nodes = all_nodes - connected_nodes

        # Step 4: Find completely isolated nodes
        isolated_nodes = all_nodes - active_nodes

        # Add appropriate warnings
        if unconnected_nodes:
            warnings.append(f"Found {len(unconnected_nodes)} nodes not connected to any output")
            
        if isolated_nodes:
            warnings.append(f"Found {len(isolated_nodes)} isolated nodes")
            
        # Check gradient shapes
        for edge in self._edges:
            if edge.grad is not None:
                src_shape = edge.src.tensor.shape
                grad_shape = edge.grad.shape
                if src_shape != grad_shape:
                    warnings.append(
                        f"Gradient shape mismatch: grad shape {grad_shape} vs tensor shape {src_shape}"
                    )
                    
        return warnings


# Global autograd engine instance
_autograd_engine = AutogradEngine()

def get_autograd_engine() -> AutogradEngine:
    """Returns the global autograd engine instance."""
    return _autograd_engine

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\context.py
// ----------------------------------------
from typing import Any, Dict, List, Tuple
from dataclasses import dataclass, field

@dataclass
class Context:
    """
    Context class for storing information needed during the backward pass.
    
    The Context class serves as a storage mechanism for tensors and metadata that are 
    needed during backpropagation. It's passed to both forward and backward functions
    to maintain state between the two passes.
    
    Attributes:
        _saved_tensors: List of tensors saved during forward pass for use in backward pass
        _non_tensor_args: Dictionary of additional arguments needed for backward pass
        _intermediate_values: Dictionary storing intermediate computations
    """
    
    _saved_tensors: List[Any] = field(default_factory=list)
    _non_tensor_args: Dict[str, Any] = field(default_factory=dict)
    _intermediate_values: Dict[str, Any] = field(default_factory=dict)

    def save_for_backward(self, *args: Any) -> None:
        """
        Saves tensors that will be needed for the backward pass.
        
        Args:
            *args: Variable number of tensors to save
        """
        self._saved_tensors = list(args)

    def save_arguments(self, **kwargs: Any) -> None:
        """
        Saves additional arguments that will be needed for the backward pass.
        
        Args:
            **kwargs: Keyword arguments to save
        """
        self._non_tensor_args.update(kwargs)
        
    def store_intermediate(self, name: str, value: Any) -> None:
        """
        Stores intermediate values computed during forward pass that may be
        useful during backward pass or for debugging.
        
        Args:
            name: Identifier for the intermediate value
            value: The value to store
        """
        self._intermediate_values[name] = value

    @property
    def saved_tensors(self) -> Tuple[Any, ...]:
        """Returns the saved tensors as a tuple."""
        return tuple(self._saved_tensors)

    @property
    def saved_arguments(self) -> Dict[str, Any]:
        """Returns the saved non-tensor arguments."""
        return self._non_tensor_args.copy()
        
    def get_intermediate(self, name: str) -> Any:
        """
        Retrieves a stored intermediate value.
        
        Args:
            name: Identifier for the intermediate value
            
        Returns:
            The stored value
            
        Raises:
            KeyError: If no value exists for the given name
        """
        return self._intermediate_values[name]

    def clear(self) -> None:
        """Clears all saved data from the context."""
        self._saved_tensors.clear()
        self._non_tensor_args.clear()
        self._intermediate_values.clear()

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\function.py
// ----------------------------------------
from abc import ABC, abstractmethod
from typing import Any, Tuple, Optional, Dict
import numpy as np

from .context import Context
from .tensor import Tensor  # This will be implemented next

class Function(ABC):
    """
    Base class for all autograd operations.
    
    This class defines the interface for creating differentiable operations.
    Each operation should implement both a forward pass (computing the result)
    and a backward pass (computing gradients).
    
    The Function class follows a similar design pattern to PyTorch's autograd.Function,
    but with some simplifications and additional features for clarity and debugging.
    """
    
    requires_grad: bool = True
    
    @staticmethod
    @abstractmethod
    def forward(ctx: Context, *args: Any, **kwargs: Any) -> Tensor:
        """
        Performs the forward computation.
        
        Args:
            ctx: Context object for saving information needed in backward pass
            *args: Input tensors and other arguments
            **kwargs: Additional keyword arguments for the operation
            
        Returns:
            Result of the computation as a Tensor
        """
        raise NotImplementedError
        
    @staticmethod
    @abstractmethod
    def backward(ctx: Context, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        """
        Computes gradients of the operation with respect to its inputs.
        
        Args:
            ctx: Context object containing saved tensors from forward pass
            grad_output: Gradient of the loss with respect to the output
            grad_dict: Dictionary mapping tensor IDs to their gradients
        """
        raise NotImplementedError
        
    @classmethod
    def apply(cls, *args: Any, **kwargs: Any) -> Tensor:
        """
        Applies the function to the given inputs.
        
        This method:
        1. Creates a Context object for storing intermediate values
        2. Runs the forward pass
        3. Sets up the computational graph for gradient computation
        4. Returns the result
        """
        ctx = Context()
        result = cls.forward(ctx, *args, **kwargs)
        
        # Check if we need to compute gradients
        needs_grad = cls.requires_grad and any(
            isinstance(arg, Tensor) and arg.requires_grad 
            for arg in args
        )
        
        if needs_grad:
            def backward_fn(grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
                cls.backward(ctx, grad_output, grad_dict)
            
            result._backward_fn = backward_fn
            result.requires_grad_(True)
            
            # Get autograd engine and register edges
            from .autograd import get_autograd_engine
            engine = get_autograd_engine()
            for arg in args:
                if isinstance(arg, Tensor):
                    engine.add_edge(arg, result)
        
        return result  # Return result in all cases
        
        
    @staticmethod
    def verify_backward(
        forward_fn: Any,
        backward_fn: Any,
        inputs: Tuple[np.ndarray, ...],
        epsilon: float = 1e-6
    ) -> bool:
        """
        Verifies backward pass implementation using numerical gradients.
        
        This helper method compares analytically computed gradients with
        numerically computed gradients to check for correctness.
        
        Args:
            forward_fn: The forward pass function
            backward_fn: The backward pass function
            inputs: Tuple of input arrays
            epsilon: Small value for numerical gradient computation
            
        Returns:
            True if gradients match within tolerance, False otherwise
        """
        def compute_numerical_gradient(idx: int, inp: np.ndarray) -> np.ndarray:
            grad = np.zeros_like(inp)
            it = np.nditer(inp, flags=['multi_index'])
            
            while not it.finished:
                ix = it.multi_index
                old_value = inp[ix]
                
                # Compute f(x + epsilon)
                inp[ix] = old_value + epsilon
                pos_inputs = list(inputs)
                pos_inputs[idx] = inp.copy()
                pos_output = forward_fn(*pos_inputs)
                
                # Compute f(x - epsilon)
                inp[ix] = old_value - epsilon
                neg_inputs = list(inputs)
                neg_inputs[idx] = inp.copy()
                neg_output = forward_fn(*neg_inputs)
                
                # Restore original value
                inp[ix] = old_value
                
                # Compute numerical gradient
                grad[ix] = np.sum(pos_output - neg_output) / (2 * epsilon)
                it.iternext()
                
            return grad
            
        # Compute analytical gradients
        ctx = Context()
        output = forward_fn(*inputs)
        grad_output = np.ones_like(output)
        analytical_grads = backward_fn(ctx, grad_output)
        
        # Compute numerical gradients
        numerical_grads = tuple(
            compute_numerical_gradient(i, inp.copy()) 
            for i, inp in enumerate(inputs)
        )
        
        # Compare gradients
        for analytical, numerical in zip(analytical_grads, numerical_grads):
            if analytical is not None:
                rel_error = np.max(
                    np.abs(analytical - numerical) /
                    (np.maximum(np.abs(analytical), np.abs(numerical)) + epsilon)
                )
                if rel_error > 1e-5:
                    return False
                    
        return True

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\tensor.py
// ----------------------------------------
import numpy as np
from typing import Optional, Union, List, Tuple, Callable, Dict, Set
from numbers import Number

class Tensor:
    """
    A multidimensional array with autograd capabilities.
    
    The Tensor class wraps numpy arrays and adds automatic differentiation
    capabilities. It tracks the computational graph and enables gradient
    computation through backpropagation.
    
    Attributes:
        data: The underlying numpy array holding the tensor's values
        grad: Gradient of the loss with respect to this tensor
        requires_grad: Whether to compute gradients for this tensor
        _prev: Set of immediate predecessor nodes in computational graph
        _backward_fn: Function to compute gradients during backpropagation
        _is_leaf: Whether this tensor is a leaf node (created by user)
    """
    
    def __init__(
        self,
        data: Union[np.ndarray, List, Number],
        requires_grad: bool = False,
        dtype: Optional[np.dtype] = None
    ):
        # Convert scalars to scalar arrays with shape ()
        if isinstance(data, (int, float)):
            self.data = np.array(data, dtype=dtype or np.float64)  # Will have shape ()
        elif isinstance(data, Tensor):
            self.data = data.data
        elif isinstance(data, list):
            self.data = np.array(data, dtype=dtype)
        else:
            self.data = data.astype(dtype) if dtype else data
            
        self.grad: Optional[np.ndarray] = None
        self._requires_grad = requires_grad
        self._backward_fn: Optional[Callable] = None
        self._prev: Set['Tensor'] = set()
        self._is_leaf = True

        # Register with autograd engine
        from .autograd import get_autograd_engine
        engine = get_autograd_engine()
        engine.register_tensor(self)
        
        if requires_grad:
            self.zero_grad()

    @property
    def shape(self) -> Tuple[int, ...]:
        """Returns the shape of the tensor."""
        return self.data.shape
        
    @property
    def dtype(self) -> np.dtype:
        """Returns the data type of the tensor."""
        return self.data.dtype
        
    @property
    def requires_grad(self) -> bool:
        """Returns whether the tensor requires gradient computation."""
        return self._requires_grad
        
    def requires_grad_(self, requires_grad: bool = True) -> 'Tensor':
        """Sets gradient computation requirement and returns self."""
        self._requires_grad = requires_grad
        if requires_grad and self.grad is None:
            self.zero_grad()
        return self

    def zero_grad(self) -> None:
        """Zeros out the gradient."""
        if self.data.shape == ():  # For scalar tensors
            self.grad = np.zeros(1, dtype=np.float64)  # Force 1D array
        else:
            self.grad = np.zeros_like(self.data, dtype=np.float64)
        
    def backward(self, gradient: Optional[np.ndarray] = None) -> None:
        """
        Computes gradients of the loss with respect to this tensor.
        """
        if not self.requires_grad:
            return

        # Handle default gradient for scalar tensors
        if gradient is None:
            if np.prod(self.shape) == 1:
                if self.shape == ():  # scalar tensor
                    gradient = np.array(1.0)
                else:
                    gradient = np.ones(self.shape)
            else:
                raise RuntimeError("grad can be implicitly created only for scalar outputs")

        # Ensure gradient is numpy array
        if isinstance(gradient, (int, float)):
            gradient = np.array(gradient)
            
        # Ensure matching shapes for scalar case
        if self.shape == () and gradient.shape != ():
            gradient = gradient.sum()
        elif self.shape != () and gradient.shape == ():
            gradient = np.full(self.shape, gradient)

        # Get autograd engine and execute backward pass
        from .autograd import get_autograd_engine
        engine = get_autograd_engine()
        engine.backward(self, gradient)


    def __repr__(self) -> str:
        return f"Tensor({self.data}, requires_grad={self.requires_grad})"

    # Basic arithmetic operations that will be connected to Function implementations
    def __add__(self, other: Union['Tensor', Number]) -> 'Tensor':
        from ..ops.basic import Add
        return Add.apply(self, other)
        
    def __mul__(self, other: Union['Tensor', Number]) -> 'Tensor':
        from ..ops.basic import Multiply
        return Multiply.apply(self, other)
        
    def __matmul__(self, other: 'Tensor') -> 'Tensor':
        from ..ops.basic import MatMul
        return MatMul.apply(self, other)
        
    def __neg__(self) -> 'Tensor':
        return self * (-1)
        
    def __sub__(self, other: Union['Tensor', Number]) -> 'Tensor':
        return self + (-other)

    def reshape(self, *shape: int) -> 'Tensor':
        from ..ops.reshape import Reshape
        return Reshape.apply(self, shape)

    # Helper methods for numpy compatibility
    def numpy(self) -> np.ndarray:
        """Returns the underlying numpy array."""
        return self.data
        
    @classmethod
    def from_numpy(cls, array: np.ndarray, requires_grad: bool = False) -> 'Tensor':
        """Creates a Tensor from a numpy array."""
        return cls(array.copy(), requires_grad=requires_grad)

    # Shape manipulation methods
    def reshape(self, *shape: int) -> 'Tensor':
        """Returns a tensor with the same data and new shape."""
        from ..ops import Reshape
        return Reshape.apply(self, shape)

    def pow(self, exponent: Union['Tensor', float]) -> 'Tensor':
        """Returns tensor raised to the power of exponent."""
        from ..ops import Power
        return Power.apply(self, exponent)

    def div(self, other: Union['Tensor', float]) -> 'Tensor':
        """Returns self divided by other."""
        from ..ops import Divide
        return Divide.apply(self, other)

    def log(self) -> 'Tensor':
        """Returns the natural logarithm of the tensor."""
        from ..ops import Log
        return Log.apply(self)

    def exp(self) -> 'Tensor':
        """Returns e raised to the power of each element in the tensor."""
        from ..ops import Exp
        return Exp.apply(self)

    def sigmoid(self) -> 'Tensor':
        """Returns the sigmoid of the tensor."""
        from ..ops import Sigmoid
        return Sigmoid.apply(self)

    def tanh(self) -> 'Tensor':
        """Returns the hyperbolic tangent of the tensor."""
        from ..ops import Tanh
        return Tanh.apply(self)

    def sum(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':
        """Returns the sum of all elements in the tensor."""
        from ..ops import Sum
        return Sum.apply(self, axis, keepdims)

    def mean(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':
        """Returns the mean of all elements in the tensor."""
        from ..ops import Mean
        return Mean.apply(self, axis, keepdims)

    def max(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':
        """Returns the maximum value of all elements in the tensor."""
        from ..ops import Max
        return Max.apply(self, axis, keepdims)

    def t(self) -> 'Tensor':
        """Returns the transpose of the tensor."""
        from ..ops import Transpose
        return Transpose.apply(self)

    def transpose(self, *axes: int) -> 'Tensor':
        """Returns the transposed tensor."""
        from ..ops import Transpose
        return Transpose.apply(self, axes if axes else None)

    # Comparison operations
    def __gt__(self, other: Union['Tensor', float]) -> 'Tensor':
        from ..ops import Greater
        return Greater.apply(self, other)

    def __ge__(self, other: Union['Tensor', float]) -> 'Tensor':
        from ..ops import GreaterEqual
        return GreaterEqual.apply(self, other)

    def __lt__(self, other: Union['Tensor', float]) -> 'Tensor':
        from ..ops import Less
        return Less.apply(self, other)

    def __le__(self, other: Union['Tensor', float]) -> 'Tensor':
        from ..ops import LessEqual
        return LessEqual.apply(self, other)

    def __eq__(self, other: Union['Tensor', float]) -> 'Tensor':
        from ..ops import Equal
        return Equal.apply(self, other)

    def __ne__(self, other: Union['Tensor', float]) -> 'Tensor':
        from ..ops import NotEqual
        return NotEqual.apply(self, other)

    def __truediv__(self, other: Union['Tensor', float]) -> 'Tensor':
        """Implements division using the / operator."""
        from ..ops import Divide
        return Divide.apply(self, other)

    def __pow__(self, exponent: Union['Tensor', float]) -> 'Tensor':
        """Implements power using the ** operator."""
        from ..ops import Power
        return Power.apply(self, exponent)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\__init__.py
// ----------------------------------------
"""
Neural network module for DLpy.

This module contains all components needed for building neural networks.
"""

from .modules import Module
from .linear import Linear
from .activations import (
    relu, leaky_relu, elu, gelu, sigmoid, tanh,
    ReLU, LeakyReLU, ELU, GELU, Sigmoid, Tanh
)

from .conv2d import Conv2d 

__all__ = [
    # Base module
    'Module',
    
    # Layers
    'Linear',
    
    # Activation functions
    'relu',
    'leaky_relu',
    'elu',
    'gelu',
    'sigmoid',
    'tanh',
    'ReLU',
    'LeakyReLU',
    'ELU',
    'GELU',
    'Sigmoid',
    'Tanh',

    # Convolutional layers
    'Conv2d'
]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\activations.py
// ----------------------------------------
"""
Activation functions module for DLpy.

This module contains all standard activation functions used in neural networks.
Each activation function is implemented as a Function subclass for autograd support.
"""

from typing import Dict, Optional
import numpy as np
from ..core import Function, Tensor

class ReLU(Function):
    """
    Rectified Linear Unit activation function.
    
    Forward: f(x) = max(0, x)
    Backward: f'(x) = 1 if x > 0 else 0
    """
    
    @staticmethod
    def forward(ctx, x):
        if not isinstance(x, Tensor):
            x = Tensor(x)
            
        ctx.save_for_backward(x)
        return Tensor(np.maximum(0, x.data))
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        x, = ctx.saved_tensors
        if x.requires_grad:
            grad = grad_output * (x.data > 0)
            grad_dict[id(x)] = grad

class LeakyReLU(Function):
    """
    Leaky Rectified Linear Unit activation function.
    
    Forward: f(x) = x if x > 0 else negative_slope * x
    Backward: f'(x) = 1 if x > 0 else negative_slope
    
    Args:
        negative_slope: Controls slope for negative values. Default: 0.01
    """
    
    @staticmethod
    def forward(ctx, x, negative_slope: float = 0.01):
        if not isinstance(x, Tensor):
            x = Tensor(x)
            
        ctx.save_for_backward(x)
        ctx.save_arguments(negative_slope=negative_slope)
        
        return Tensor(np.where(x.data > 0, x.data, negative_slope * x.data))
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        x, = ctx.saved_tensors
        negative_slope = ctx.saved_arguments['negative_slope']
        
        if x.requires_grad:
            grad = grad_output * np.where(x.data > 0, 1.0, negative_slope)
            grad_dict[id(x)] = grad

class ELU(Function):
    """
    Exponential Linear Unit activation function.
    
    Forward: f(x) = x if x > 0 else alpha * (exp(x) - 1)
    Backward: f'(x) = 1 if x > 0 else alpha * exp(x)
    
    Args:
        alpha: Controls the value to which an ELU saturates for negative inputs. Default: 1.0
    """
    
    @staticmethod
    def forward(ctx, x, alpha: float = 1.0):
        if not isinstance(x, Tensor):
            x = Tensor(x)
            
        ctx.save_for_backward(x)
        ctx.save_arguments(alpha=alpha)
        
        return Tensor(np.where(x.data > 0, x.data, alpha * (np.exp(x.data) - 1)))
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        x, = ctx.saved_tensors
        alpha = ctx.saved_arguments['alpha']
        
        if x.requires_grad:
            grad = grad_output * np.where(x.data > 0, 1.0, alpha * np.exp(x.data))
            grad_dict[id(x)] = grad

class GELU(Function):
    """
    Gaussian Error Linear Unit activation function.
    
    Forward: f(x) = x * Φ(x)
    where Φ(x) is the Gaussian cumulative distribution function.
    
    This implementation uses the approximation:
    f(x) ≈ 0.5x * (1 + tanh(√(2/π) * (x + 0.044715x³)))
    """
    
    @staticmethod
    def forward(ctx, x):
        if not isinstance(x, Tensor):
            x = Tensor(x)
            
        # Constants for the approximation
        sqrt_2_over_pi = np.sqrt(2 / np.pi)
        coeff = 0.044715
        
        # Compute intermediate values
        x_cubed = x.data ** 3
        inner = sqrt_2_over_pi * (x.data + coeff * x_cubed)
        tanh_inner = np.tanh(inner)
        
        # Compute output
        result = 0.5 * x.data * (1 + tanh_inner)
        
        # Save for backward pass
        ctx.save_for_backward(x)
        ctx.save_arguments(tanh_inner=tanh_inner)
        
        return Tensor(result)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        x, = ctx.saved_tensors
        tanh_inner = ctx.saved_arguments['tanh_inner']
        
        if x.requires_grad:
            sqrt_2_over_pi = np.sqrt(2 / np.pi)
            coeff = 0.044715
            
            # Compute derivative
            x_cubed = x.data ** 3
            inner = sqrt_2_over_pi * (x.data + coeff * x_cubed)
            
            # d/dx[GELU(x)] = 0.5 * (1 + tanh(inner)) + 
            #                 0.5x * (1 - tanh²(inner)) * sqrt(2/π) * (1 + 3 * 0.044715x²)
            grad = 0.5 * (1 + tanh_inner)
            grad += 0.5 * x.data * (1 - tanh_inner ** 2) * sqrt_2_over_pi * (1 + 3 * coeff * x.data ** 2)
            
            grad_dict[id(x)] = grad_output * grad

class Sigmoid(Function):
    """
    Sigmoid activation function.
    
    Forward: f(x) = 1 / (1 + exp(-x))
    Backward: f'(x) = f(x) * (1 - f(x))
    """
    
    @staticmethod
    def forward(ctx, x):
        if not isinstance(x, Tensor):
            x = Tensor(x)
            
        # Compute sigmoid with numerical stability
        x_data = x.data
        exp_neg_x = np.exp(-np.abs(x_data))
        sigmoid_x = np.where(x_data >= 0, 
                           1 / (1 + exp_neg_x),
                           exp_neg_x / (1 + exp_neg_x))
        
        ctx.save_for_backward(x)
        ctx.save_arguments(sigmoid_x=sigmoid_x)
        return Tensor(sigmoid_x)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        x, = ctx.saved_tensors
        sigmoid_x = ctx.saved_arguments['sigmoid_x']
        
        if x.requires_grad:
            grad = grad_output * sigmoid_x * (1 - sigmoid_x)
            grad_dict[id(x)] = grad

class Tanh(Function):
    """
    Hyperbolic tangent activation function.
    
    Forward: f(x) = tanh(x)
    Backward: f'(x) = 1 - tanh²(x)
    """
    
    @staticmethod
    def forward(ctx, x):
        if not isinstance(x, Tensor):
            x = Tensor(x)
            
        tanh_x = np.tanh(x.data)
        ctx.save_for_backward(x)
        ctx.save_arguments(tanh_x=tanh_x)
        return Tensor(tanh_x)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        x, = ctx.saved_tensors
        tanh_x = ctx.saved_arguments['tanh_x']
        
        if x.requires_grad:
            grad = grad_output * (1 - tanh_x ** 2)
            grad_dict[id(x)] = grad

# Add convenience functions for each activation
def relu(x: Tensor) -> Tensor:
    """Applies ReLU activation function."""
    return ReLU.apply(x)

def leaky_relu(x: Tensor, negative_slope: float = 0.01) -> Tensor:
    """Applies Leaky ReLU activation function."""
    return LeakyReLU.apply(x, negative_slope)

def elu(x: Tensor, alpha: float = 1.0) -> Tensor:
    """Applies ELU activation function."""
    return ELU.apply(x, alpha)

def gelu(x: Tensor) -> Tensor:
    """Applies GELU activation function."""
    return GELU.apply(x)

def sigmoid(x: Tensor) -> Tensor:
    """Applies Sigmoid activation function."""
    return Sigmoid.apply(x)

def tanh(x: Tensor) -> Tensor:
    """Applies Tanh activation function."""
    return Tanh.apply(x)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\conv2d.py
// ----------------------------------------
from typing import Tuple, Optional, Union
import numpy as np
from ..core import Tensor
from .modules import Module
from ..ops.cnn import Conv2dFunction

def _pair(x: Union[int, Tuple[int, int]]) -> Tuple[int, int]:
    """Convert input to a pair of values."""
    if isinstance(x, tuple):
        return x
    return (x, x)

class Conv2d(Module):
    """
    Applies a 2D convolution over an input signal composed of several input planes.
    
    Args:
        in_channels (int): Number of channels in the input image
        out_channels (int): Number of channels produced by the convolution
        kernel_size (int or tuple): Size of the convolving kernel
        stride (int or tuple, optional): Stride of the convolution. Default: 1
        padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0
        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 
        bias (bool, optional): If True, adds a learnable bias to the output. Default: True
        
    Shape:
        - Input: (N, C_in, H, W)
        - Output: (N, C_out, H_out, W_out)
          where
          H_out = (H + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1
          W_out = (W + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1
    """
    
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[int, Tuple[int, int]],
        stride: Union[int, Tuple[int, int]] = 1,
        padding: Union[int, Tuple[int, int]] = 0,
        dilation: Union[int, Tuple[int, int]] = 1,
        groups: int = 1,
        bias: bool = True
    ):
        super().__init__()
        
        if in_channels % groups != 0:
            raise ValueError('in_channels must be divisible by groups')
        if out_channels % groups != 0:
            raise ValueError('out_channels must be divisible by groups')
            
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = _pair(kernel_size)
        self.stride = _pair(stride)
        self.padding = _pair(padding)
        self.dilation = _pair(dilation)
        self.groups = groups
        
        # Initialize weights using He initialization
        # Adjust fan_in to account for groups
        fan_in = in_channels // groups * self.kernel_size[0] * self.kernel_size[1]
        bound = np.sqrt(2.0 / fan_in)
        weight_shape = (out_channels, in_channels // groups, *self.kernel_size)
        weight = Tensor(
            np.random.uniform(-bound, bound, weight_shape),
            requires_grad=True
        )
        self.register_parameter('weight', weight)
        
        if bias:
            # Initialize bias to zero
            bias_data = np.zeros(out_channels)
            self.register_parameter('bias', Tensor(bias_data, requires_grad=True))
        else:
            self.register_parameter('bias', None)
            
    def forward(self, x: Tensor) -> Tensor:
        """
        Forward pass of the convolution layer.
        
        Args:
            x: Input tensor of shape (N, C_in, H, W)
            
        Returns:
            Output tensor of shape (N, C_out, H_out, W_out)
        """
        return Conv2dFunction.apply(
            x, self.weight, self.bias,
            self.stride, self.padding,
            self.dilation, self.groups
        )
        
    def extra_repr(self) -> str:
        """Returns a string with extra representation information."""
        s = (f'{self.in_channels}, {self.out_channels}, '
             f'kernel_size={self.kernel_size}')
        
        if self.stride != (1, 1):
            s += f', stride={self.stride}'
        if self.padding != (0, 0):
            s += f', padding={self.padding}'
        if self.dilation != (1, 1):
            s += f', dilation={self.dilation}'
        if self.groups != 1:
            s += f', groups={self.groups}'
        if self.bias is None:
            s += ', bias=False'
        return s

    @staticmethod
    def calc_output_shape(
        input_shape: Tuple[int, ...],
        out_channels: int,
        kernel_size: Tuple[int, int],
        stride: Tuple[int, int],
        padding: Tuple[int, int],
        dilation: Tuple[int, int]
    ) -> Tuple[int, int, int, int]:
        """
        Calculate the output shape of the convolution.
        
        Args:
            input_shape: Input shape (N, C_in, H, W)
            out_channels: Number of output channels
            kernel_size: Size of the kernel
            stride: Stride of the convolution
            padding: Zero-padding added to both sides
            dilation: Spacing between kernel elements
            
        Returns:
            Output shape (N, C_out, H_out, W_out)
        """
        N, _, H, W = input_shape
        
        H_out = ((H + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) 
                // stride[0] + 1)
        W_out = ((W + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) 
                // stride[1] + 1)
        
        return (N, out_channels, H_out, W_out)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\linear.py
// ----------------------------------------
from typing import Optional, Dict
import numpy as np
from ..core import Tensor, Function
from .modules import Module

class LinearFunction(Function):
    @staticmethod
    def forward(ctx, input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        # Save tensors needed for backward pass
        ctx.save_for_backward(input, weight, bias)
        
        # Compute output: y = xW^T + b
        output = input.data @ weight.data
        if bias is not None:
            output += bias.data
            
        return Tensor(output)
    
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        # Retrieve saved tensors
        input, weight, bias = ctx.saved_tensors
        
        # Compute gradient with respect to input: dx = dout @ W
        if input.requires_grad:
            grad_dict[id(input)] = grad_output @ weight.data.T
            
        # Compute gradient with respect to weight: dW = x^T @ dout
        if weight.requires_grad:
            grad_dict[id(weight)] = input.data.T @ grad_output
            
        # Compute gradient with respect to bias: db = sum(dout, dim=0)
        if bias is not None and bias.requires_grad:
            grad_dict[id(bias)] = grad_output.sum(axis=0)

class Linear(Module):
    """
    Applies a linear transformation to the incoming data: y = xW^T + b
    
    Args:
        in_features: size of each input sample
        out_features: size of each output sample
        bias: If set to False, the layer will not learn an additive bias
        
    Shape:
        - Input: (batch_size, in_features)
        - Output: (batch_size, out_features)
        
    Attributes:
        weight: the learnable weights of shape (in_features, out_features)
        bias: the learnable bias of shape (out_features,)
    """
    
    def __init__(self, in_features: int, out_features: int, bias: bool = True):
        super().__init__()
        
        self.in_features = in_features
        self.out_features = out_features
        
        # Initialize weights using He initialization
        bound = np.sqrt(2.0 / in_features)
        weight = Tensor(
            np.random.uniform(-bound, bound, (in_features, out_features)),
            requires_grad=True
        )
        self.register_parameter('weight', weight)
        
        if bias:
            bias = Tensor(np.zeros(out_features), requires_grad=True)
            self.register_parameter('bias', bias)
        else:
            self.register_parameter('bias', None)
            
    def forward(self, input: Tensor) -> Tensor:
        """Forward pass of the linear layer."""
        return LinearFunction.apply(input, self.weight, self.bias)
            
    def extra_repr(self) -> str:
        """Extra information to add to the string representation."""
        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\modules.py
// ----------------------------------------
from typing import Iterator, Dict, Any, Optional, Union
from collections import OrderedDict
from ..core import Tensor

class Module:
    """
    Base class for all neural network modules.
    
    Your models should also subclass this class.
    Modules can also contain other Modules, allowing to nest them in
    a tree structure.
    """
    
    def __init__(self):
        """Initialize the module."""
        # First set these directly to avoid triggering __setattr__
        object.__setattr__(self, 'training', True)
        object.__setattr__(self, '_parameters', OrderedDict())
        object.__setattr__(self, '_buffers', OrderedDict())
        object.__setattr__(self, '_modules', OrderedDict())
        
    def register_parameter(self, name: str, param: Optional[Tensor]) -> None:
        """Add a parameter to the module.
        
        Args:
            name: Name of the parameter
            param: The parameter tensor to register
        """
        if '_parameters' not in self.__dict__:
            raise TypeError(
                "cannot assign parameter before Module.__init__() call"
            )
            
        if param is not None and not isinstance(param, Tensor):
            raise TypeError(f"Parameter {name} must be a Tensor, not {type(param)}")
            
        self._parameters[name] = param
        
    def register_buffer(self, name: str, tensor: Optional[Tensor]) -> None:
        """Add a persistent buffer to the module.
        
        Buffers are typically used for running statistics in modules like BatchNorm.
        
        Args:
            name: Name of the buffer
            tensor: The tensor to register as a buffer
        """
        if '_buffers' not in self.__dict__:
            raise TypeError(
                "cannot assign buffer before Module.__init__() call"
            )
            
        if tensor is not None and not isinstance(tensor, Tensor):
            raise TypeError(f"Buffer {name} must be a Tensor, not {type(tensor)}")
            
        self._buffers[name] = tensor
        
    def add_module(self, name: str, module: Optional['Module']) -> None:
        """Add a child module to the current module.
        
        Args:
            name: Name of the child module
            module: The module to add
        """
        if not isinstance(module, (Module, type(None))):
            raise TypeError(f"{name} is not a Module subclass")
            
        if '_modules' not in self.__dict__:
            raise TypeError(
                "cannot assign module before Module.__init__() call"
            )
            
        self._modules[name] = module
        
    def __getattr__(self, name: str) -> Any:
        """Custom getattr that looks through parameters, buffers, and modules."""
        if '_parameters' in self.__dict__:
            _parameters = self.__dict__['_parameters']
            if name in _parameters:
                return _parameters[name]
                
        if '_buffers' in self.__dict__:
            _buffers = self.__dict__['_buffers']
            if name in _buffers:
                return _buffers[name]
                
        if '_modules' in self.__dict__:
            modules = self.__dict__['_modules']
            if name in modules:
                return modules[name]
                
        raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
        
    def __setattr__(self, name: str, value: Any) -> None:
        """Custom setattr that handles parameter registration."""
        # Handle special module attributes first
        if name in ['training']:
            object.__setattr__(self, name, value)
            return
            
        if isinstance(value, Tensor):
            if not hasattr(self, '_parameters'):
                raise TypeError(
                    "cannot assign parameters before Module.__init__() call"
                )
            self.register_parameter(name, value)
        elif isinstance(value, Module):
            if not hasattr(self, '_modules'):
                raise TypeError(
                    "cannot assign module before Module.__init__() call"
                )
            self.add_module(name, value)
        else:
            object.__setattr__(self, name, value)
            
    def parameters(self) -> Iterator[Tensor]:
        """Returns an iterator over module parameters."""
        for param in self._parameters.values():
            if param is not None:
                yield param
        for module in self._modules.values():
            if module is not None:
                yield from module.parameters()
                
    def named_parameters(self) -> Iterator[tuple[str, Tensor]]:
        """Returns an iterator over module parameters, yielding both the
        name of the parameter as well as the parameter itself."""
        for name, param in self._parameters.items():
            if param is not None:
                yield name, param
        for mname, module in self._modules.items():
            if module is not None:
                for name, param in module.named_parameters():
                    yield f"{mname}.{name}", param
                    
    def train(self, mode: bool = True) -> 'Module':
        """Sets the module in training mode."""
        self.training = mode
        for module in self._modules.values():
            if module is not None:
                module.train(mode)
        return self
        
    def eval(self) -> 'Module':
        """Sets the module in evaluation mode."""
        return self.train(False)
        
    def __call__(self, *args, **kwargs):
        return self.forward(*args, **kwargs)
        
    def forward(self, *args, **kwargs):
        """Define the computation performed at every call."""
        raise NotImplementedError
        
    def __repr__(self):
        """Returns a string representation of the module."""
        extra_lines = []
        extra_repr = self.extra_repr()
        if extra_repr:
            extra_lines = extra_repr.split('\n')
            
        child_lines = []
        for key, module in self._modules.items():
            mod_str = repr(module)
            mod_str = _addindent(mod_str, 2)
            child_lines.append('(' + key + '): ' + mod_str)
            
        lines = extra_lines + child_lines
        
        main_str = self.__class__.__name__ + '('
        if lines:
            main_str += '\n  ' + '\n  '.join(lines) + '\n'
        main_str += ')'
        return main_str
        
    def extra_repr(self) -> str:
        """Set the extra representation of the module."""
        return ''

def _addindent(s_: str, numSpaces: int) -> str:
    """Helper for indenting multiline strings."""
    s = s_.split('\n')
    if len(s) == 1:
        return s_
    first = s.pop(0)
    s = [(numSpaces * ' ') + line for line in s]
    return '\n'.join([first] + s)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\sequential.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\__init__.py
// ----------------------------------------
"""
Operations module for DLpy.

This module contains all the mathematical operations that can be performed on tensors.
"""

from .basic import Add, Multiply
from .reshape import Reshape
from .power import Power, Divide
from .elementwise import Log, Exp
from .reduction import Sum, Mean, Max
from .matrix import (
    Transpose,
    Greater,
    GreaterEqual,
    Less,
    LessEqual,
    Equal,
    NotEqual
)
from .loss import (
    MSELoss,
    CrossEntropyLoss,
    BinaryCrossEntropyLoss,
    L1Loss,
    HuberLoss,
    KLDivLoss,
    CosineSimilarityLoss,
    HingeLoss,
    FocalLoss
)

from .cnn import Conv2dFunction

__all__ = [
    # Basic operations
    'Add',
    'Multiply',
    'Reshape',
    
    # Power operations
    'Power',
    'Divide',
    
    # Element-wise operations
    'Log',
    'Exp',
    
    # Reduction operations
    'Sum',
    'Mean',
    'Max',
    
    # Matrix operations
    'Transpose',
    
    # Comparison operations
    'Greater',
    'GreaterEqual',
    'Less',
    'LessEqual',
    'Equal',
    'NotEqual',

    # Loss functions
    'MSELoss',
    'CrossEntropyLoss',
    'BinaryCrossEntropyLoss',
    'L1Loss',
    'HuberLoss',
    'KLDivLoss',
    'CosineSimilarityLoss',
    'HingeLoss',
    'FocalLoss',

    # CNN operations
    'Conv2dFunction'
]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\basic.py
// ----------------------------------------
from typing import Dict  # Add this import at the top
from ..core.function import Function
from ..core.tensor import Tensor
import numpy as np

class Add(Function):
    @staticmethod
    def forward(ctx, a, b):
        if not isinstance(a, Tensor):
            a = Tensor(a)
        if not isinstance(b, Tensor):
            b = Tensor(b)
            
        shape_a = a.data.shape
        shape_b = b.data.shape

        # Check valid broadcasting manually
        if len(shape_a) == 2 and shape_a[0] == 1 and len(shape_b) == 1:
            # Special case: (1,N) matrix with (M,) vector requires N==M
            if shape_a[1] != shape_b[0]:
                raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
        elif len(shape_a) == 1 and len(shape_b) == 2 and shape_b[0] == 1:
            # Special case: (N,) vector with (1,M) matrix requires N==M
            if shape_a[0] != shape_b[1]:
                raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
                
        # Save tensors for backward pass
        ctx.save_for_backward(a, b)
        
        # If we get here, try the operation
        try:
            result = a.data + b.data
            return Tensor(result)
        except ValueError:
            raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        a, b = ctx.saved_tensors

        if a.requires_grad:
            grad_a = grad_output
            grad_a = Add._reduce_grad(grad_a, a.data.shape)
            if id(a) not in grad_dict or grad_dict[id(a)] is None:
                grad_dict[id(a)] = grad_a
            else:
                grad_dict[id(a)] += grad_a  # Accumulate gradients

        if b.requires_grad:
            grad_b = grad_output
            grad_b = Add._reduce_grad(grad_b, b.data.shape)
            if id(b) not in grad_dict or grad_dict[id(b)] is None:
                grad_dict[id(b)] = grad_b
            else:
                grad_dict[id(b)] += grad_b  # Accumulate gradients

    @staticmethod
    def _reduce_grad(grad, target_shape):
        """
        Reduces the gradient to match the target shape by summing over broadcasted dimensions.
        """
        # Convert target_shape to a tuple if it's not
        if not isinstance(target_shape, tuple):
            target_shape = tuple(target_shape)
        
        # Align the dimensions by prepending 1s if necessary
        grad_shape = grad.shape
        target_shape = (1,) * (len(grad_shape) - len(target_shape)) + target_shape
        for axis, (grad_dim, target_dim) in enumerate(zip(grad_shape, target_shape)):
            if target_dim == 1 and grad_dim != 1:
                grad = grad.sum(axis=axis, keepdims=True)
        return grad

class Multiply(Function):
    @staticmethod
    def forward(ctx, a, b):
        if not isinstance(a, Tensor):
            a = Tensor(a)
        if not isinstance(b, Tensor):
            b = Tensor(b)
            
        shape_a = a.data.shape
        shape_b = b.data.shape
        
        # Check if shapes can be broadcast according to NumPy rules
        try:
            # Test broadcast compatibility without actually performing the operation
            np.broadcast_shapes(shape_a, shape_b)
            # If we get here, shapes are compatible
            result = a.data * b.data
            ctx.save_for_backward(a, b)
            return Tensor(result)
        except ValueError:
            raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        a, b = ctx.saved_tensors

        if a.requires_grad:
            grad_a = grad_output * b.data
            grad_a = Multiply._reduce_grad(grad_a, a.data.shape)
            if id(a) not in grad_dict or grad_dict[id(a)] is None:
                grad_dict[id(a)] = grad_a
            else:
                grad_dict[id(a)] += grad_a  # Accumulate gradients

        if b.requires_grad:
            grad_b = grad_output * a.data
            grad_b = Multiply._reduce_grad(grad_b, b.data.shape)
            if id(b) not in grad_dict or grad_dict[id(b)] is None:
                grad_dict[id(b)] = grad_b
            else:
                grad_dict[id(b)] += grad_b  # Accumulate gradients

    @staticmethod
    def _reduce_grad(grad, target_shape):
        """
        Reduces the gradient to match the target shape by summing over broadcasted dimensions.
        """
        # Convert target_shape to a tuple if it's not
        if not isinstance(target_shape, tuple):
            target_shape = tuple(target_shape)
        
        # Align the dimensions by prepending 1s if necessary
        grad_shape = grad.shape
        target_shape = (1,) * (len(grad_shape) - len(target_shape)) + target_shape
        for axis, (grad_dim, target_dim) in enumerate(zip(grad_shape, target_shape)):
            if target_dim == 1 and grad_dim != 1:
                grad = grad.sum(axis=axis, keepdims=True)
        return grad

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\cnn.py
// ----------------------------------------
from typing import Tuple, Dict, Optional, Union, List
import numpy as np
from ..core import Function, Tensor, Context

class ConvMode:
    """Enumeration of convolution modes."""
    STANDARD = "standard"
    TRANSPOSED = "transposed"
    DEFORMABLE = "deformable"

def _validate_conv_params(
    x_shape: tuple,
    weight_shape: tuple,
    stride: tuple,
    padding: tuple,
    dilation: tuple,
    groups: int,
    mode: str = ConvMode.STANDARD,
    offset: Optional[Tensor] = None,
    weight: Optional[Tensor] = None, # Added weight parameter
    mask: Optional[Tensor] = None
) -> None:
    """Validates convolution parameters."""
    N, C_in, H, W = x_shape
    C_out, C_in_per_group, kH, kW = weight_shape

    if C_in % groups != 0:
        raise ValueError(f"Input channels ({C_in}) must be divisible by groups ({groups})")
        
    if C_out % groups != 0:
        raise ValueError(f"Output channels ({C_out}) must be divisible by groups ({groups})")
        
    if C_in_per_group != C_in // groups:
        raise ValueError(f"Expected {C_in // groups} input channels per group, got {C_in_per_group}")
        
    if mode not in [ConvMode.STANDARD, ConvMode.TRANSPOSED, ConvMode.DEFORMABLE]:
        raise ValueError(f"Invalid convolution mode: {mode}")
        
    if mode == ConvMode.DEFORMABLE:
        # Check for offset either in direct parameter or weight.offset
        offset_tensor = offset
        if offset_tensor is None and weight is not None:
            offset_tensor = getattr(weight, 'offset', None)
            
        if offset_tensor is None:
            raise ValueError("Deformable convolution requires offset parameter")
        
        # Calculate output size
        H_out = (H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0] + 1
        W_out = (W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1] + 1
        
        # Check offset tensor shape
        expected_offset_shape = (N, 2 * kH * kW, H_out, W_out)
        if offset_tensor.shape != expected_offset_shape:
            raise ValueError(f"Expected offset shape {expected_offset_shape}, got {offset_tensor.shape}")
            
        # Check mask tensor shape if provided
        if mask is not None:
            expected_mask_shape = (N, kH * kW, H_out, W_out)
            if mask.shape != expected_mask_shape:
                raise ValueError(f"Expected mask shape {expected_mask_shape}, got {mask.shape}")


def _pad_input(x: np.ndarray, padding: Tuple[int, int]) -> np.ndarray:
    """
    Pads input tensor with zeros.
    
    Args:
        x: Input tensor
        padding: (padding_height, padding_width)
    """
    if padding[0] == 0 and padding[1] == 0:
        return x
    pad_width = ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1]))
    return np.pad(x, pad_width, mode='constant', constant_values=0)

def _get_output_shape(input_shape: Tuple[int, ...], kernel_size: Tuple[int, int],
                    stride: Tuple[int, int], padding: Tuple[int, int],
                    dilation: Tuple[int, int], mode: str = ConvMode.STANDARD) -> Tuple[int, int]:
    """
    Calculates output shape for different convolution types.
    """
    if mode == ConvMode.STANDARD:
        H = ((input_shape[2] + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) 
             // stride[0] + 1)
        W = ((input_shape[3] + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) 
             // stride[1] + 1)
    elif mode == ConvMode.TRANSPOSED:
        H = (input_shape[2] - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + 1
        W = (input_shape[3] - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + 1
    else:  # Deformable follows standard conv shape
        H = ((input_shape[2] + 2 * padding[0] - kernel_size[0]) // stride[0] + 1)
        W = ((input_shape[3] + 2 * padding[1] - kernel_size[1]) // stride[1] + 1)
    return H, W

def _get_deformable_offsets(offset_tensor: np.ndarray, kernel_size: Tuple[int, int],
                         input_shape: Tuple[int, ...]) -> np.ndarray:
    """
    Computes sampling locations for deformable convolution.
    
    Args:
        offset_tensor: Offset values of shape (N, 2*kH*kW, H_out, W_out)
        kernel_size: Size of the convolving kernel (kH, kW)
        input_shape: Shape of input tensor (N, C, H, W)
        
    Returns:
        Sampling locations of shape (N, H_out*W_out, kH*kW, 2)
    """
    N, C, H, W = input_shape
    kH, kW = kernel_size
    H_out, W_out = offset_tensor.shape[2:]
    
    # Convert memoryview to numpy array if needed
    if isinstance(offset_tensor, memoryview):
        offset_tensor = np.array(offset_tensor)
    
    # Generate base sampling grid for each output position
    base_h = np.arange(kH)
    base_w = np.arange(kW)
    mesh_h, mesh_w = np.meshgrid(base_h, base_w, indexing='ij')
    base_grid = np.stack([mesh_h, mesh_w], axis=-1)  # (kH, kW, 2)
    
    # Reshape for broadcasting
    kHW = kH * kW
    base_grid = base_grid.reshape(-1, 2).T  # (2, kH*kW)
    base_grid = base_grid[None, None, :, :]  # (1, 1, 2, kH*kW)
    
    # Reshape offset tensor to (N, H_out*W_out, 2, kH*kW)
    offset_tensor = offset_tensor.reshape(N, 2, kHW, H_out, W_out)
    offset_tensor = offset_tensor.transpose(0, 3, 4, 1, 2)  # (N, H_out, W_out, 2, kH*kW)
    offset_tensor = offset_tensor.reshape(N, H_out * W_out, 2, kHW)
    
    # Broadcast base grid to match offset tensor shape
    base_grid = np.broadcast_to(base_grid, (N, H_out * W_out, 2, kHW))
    
    # Add offsets to base grid
    sampling_locations = base_grid + offset_tensor
    
    # Ensure output is in correct format (N, H_out*W_out, kH*kW, 2)
    return sampling_locations.transpose(0, 1, 3, 2)

def _bilinear_interpolate(input: np.ndarray, points: np.ndarray, align_corners: bool = True) -> np.ndarray:
    """
    Performs bilinear interpolation on the input tensor at specified points.
    
    Args:
        input: Input tensor (N, C, H, W)
        points: Points to sample (N, P, 2) in normalized coordinates [-1, 1]
        align_corners: Whether to align corners in interpolation
        
    Returns:
        Interpolated values (N, C, P)
    """
    N, C, H, W = input.shape
    
    # Ensure points is correct shape (N, P, 2)
    if points.ndim == 4:
        points = points.reshape(points.shape[0], -1, 2)
    
    _, P, _ = points.shape
    
    # Convert normalized coordinates to pixel coordinates
    if align_corners:
        x = (points[..., 0] + 1) * (W - 1) / 2
        y = (points[..., 1] + 1) * (H - 1) / 2
    else:
        x = ((points[..., 0] + 1) * W - 1) / 2
        y = ((points[..., 1] + 1) * H - 1) / 2
    
    # Get corner indices
    x0 = np.floor(x).astype(np.int32)
    x1 = x0 + 1
    y0 = np.floor(y).astype(np.int32)
    y1 = y0 + 1
    
    # Clip to image boundaries
    x0 = np.clip(x0, 0, W - 1)
    x1 = np.clip(x1, 0, W - 1)
    y0 = np.clip(y0, 0, H - 1)
    y1 = np.clip(y1, 0, H - 1)
    
    # Calculate interpolation weights
    wa = (x1 - x) * (y1 - y)
    wb = (x1 - x) * (y - y0)
    wc = (x - x0) * (y1 - y)
    wd = (x - x0) * (y - y0)
    
    # Reshape weights for broadcasting
    wa = wa[..., None]
    wb = wb[..., None]
    wc = wc[..., None]
    wd = wd[..., None]
    
    # Gather corner values and compute weighted sum
    output = np.zeros((N, C, P))
    for n in range(N):
        output[n] = (wa[n] * input[n, :, y0[n], x0[n]] +
                    wb[n] * input[n, :, y1[n], x0[n]] +
                    wc[n] * input[n, :, y0[n], x1[n]] +
                    wd[n] * input[n, :, y1[n], x1[n]])
    
    return output

def _im2col_dilated(x: np.ndarray, kernel_size: Tuple[int, int],
                    stride: Tuple[int, int], dilation: Tuple[int, int],
                    mode: str = ConvMode.STANDARD,
                    sampling_locations: Optional[np.ndarray] = None) -> np.ndarray:
    """
    Rearranges dilated image blocks into columns with support for different convolution types.
    """
    N, C, H, W = x.shape
    kH, kW = kernel_size
    dH, dW = dilation
    
    # Calculate output size based on mode
    out_h, out_w = _get_output_shape((N, C, H, W), kernel_size, stride, (0, 0), dilation, mode)
    
    # Initialize output array
    cols = np.zeros((C * kH * kW, N * out_h * out_w))
    
    if mode == ConvMode.DEFORMABLE and sampling_locations is not None:
        for n in range(N):
            for h_out in range(out_h):
                for w_out in range(out_w):
                    # Reshape to (1, kH*kW, 2) for bilinear interpolation
                    loc = sampling_locations[n, h_out*out_w + w_out].reshape(1, -1, 2)
                    
                    # Sample using bilinear interpolation
                    sampled_values = _bilinear_interpolate(
                        x[n:n+1],
                        loc
                    )
                    
                    # Store in cols array
                    idx = n * out_h * out_w + h_out * out_w + w_out
                    cols[:, idx] = sampled_values.reshape(-1)
    else:
        # Standard or transposed convolution
        for h_out in range(out_h):
            for w_out in range(out_w):
                for c in range(C):
                    for i in range(kH):
                        for j in range(kW):
                            if mode == ConvMode.STANDARD:
                                h_in = h_out * stride[0] + i * dH
                                w_in = w_out * stride[1] + j * dW
                            else:  # TRANSPOSED
                                h_in = h_out * dH + i * stride[0]
                                w_in = w_out * dW + j * stride[1]
                            
                            col_idx = (c * kH * kW + i * kW + j)
                            row_idx = h_out * out_w + w_out
                            
                            for n in range(N):
                                if 0 <= h_in < H and 0 <= w_in < W:
                                    cols[col_idx, n * out_h * out_w + row_idx] = x[n, c, h_in, w_in]
                                    
    return cols

def _compute_conv_output_shape(input_size: int, kernel_size: int, stride: int,
                             padding: int, dilation: int) -> int:
    """Computes output dimension for a single axis."""
    numerator = input_size + 2 * padding - dilation * (kernel_size - 1) - 1
    return numerator // stride + 1

def _compute_conv_grad_input_padding(grad_output_size: int, input_size: int,
                                   kernel_size: int, stride: int, padding: int,
                                   dilation: int) -> Tuple[int, int]:
    """Computes padding needed for gradient computation."""
    grad_input_padding = kernel_size - 1 - padding
    return grad_input_padding

def _compute_output_padding(input_size: int, output_size: int, kernel_size: int,
                          stride: int, padding: int, dilation: int) -> int:
    """Computes additional padding needed for transposed convolution."""
    expected_output = (input_size - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + 1
    return output_size - expected_output

def _unfold(input_tensor: np.ndarray,
           kernel_size: Tuple[int, ...],
           dilation: Tuple[int, ...],
           padding: Tuple[int, ...],
           stride: Tuple[int, ...]) -> np.ndarray:
    """Extracts sliding local blocks from input tensor."""
    N, C, H, W = input_tensor.shape
    kH, kW = kernel_size
    
    # Apply padding if needed
    if padding[0] > 0 or padding[1] > 0:
        input_tensor = np.pad(input_tensor,
                          ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])),
                          mode='constant')
    
    # Calculate output dimensions
    H_out = ((H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
    W_out = ((W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
    
    # Initialize output array with correct shape
    output = np.zeros((C * kH * kW, N * H_out * W_out))
    
    # Extract patches
    for h in range(H_out):
        for w in range(W_out):
            for i in range(kH):
                for j in range(kW):
                    h_start = h * stride[0] + i * dilation[0]
                    w_start = w * stride[1] + j * dilation[1]
                    
                    # Extract patch for all channels and batches
                    patch = input_tensor[:, :, h_start:h_start+1, w_start:w_start+1]
                    
                    # Place in output array
                    row_idx = (i * kW + j) * C + np.arange(C)
                    col_idx = h * W_out + w + np.arange(N) * H_out * W_out
                    output[row_idx[:, None], col_idx] = patch.reshape(N, C).T
    
    return output

def _fold(input: np.ndarray,
         output_size: Tuple[int, ...],
         kernel_size: Tuple[int, ...],
         dilation: Tuple[int, ...],
         padding: Tuple[int, ...],
         stride: Tuple[int, ...]) -> np.ndarray:
    """Combines an array of sliding local blocks into a large tensor."""
    H, W = output_size
    kH, kW = kernel_size
    C = input.shape[0] // (kH * kW)
    N = input.shape[1] // ((H + 2 * padding[0] - kH + 1) * (W + 2 * padding[1] - kW + 1))
    
    # Initialize output tensor
    output = np.zeros((N, C, H + 2 * padding[0], W + 2 * padding[1]))
    divisor = np.zeros_like(output)  # For averaging overlapping values
    
    # Calculate output dimensions
    H_out = ((H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
    W_out = ((W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
    
    # Fold patches back
    for h in range(H_out):
        for w in range(W_out):
            for i in range(kH):
                for j in range(kW):
                    h_start = h * stride[0] + i * dilation[0]
                    w_start = w * stride[1] + j * dilation[1]
                    
                    row_idx = (i * kW + j) * C + np.arange(C)
                    col_idx = h * W_out + w + np.arange(N) * H_out * W_out
                    
                    patch = input[row_idx[:, None], col_idx].T.reshape(N, C, 1, 1)
                    output[:, :, h_start:h_start+1, w_start:w_start+1] += patch
                    divisor[:, :, h_start:h_start+1, w_start:w_start+1] += 1
    
    # Average overlapping values
    output = np.divide(output, divisor, where=divisor != 0)
    
    # Remove padding if necessary
    if padding[0] > 0 or padding[1] > 0:
        output = output[:, :, padding[0]:-padding[0] if padding[0] > 0 else None,
                       padding[1]:-padding[1] if padding[1] > 0 else None]
    
    return output

def _dilate(input: np.ndarray, dilation: Tuple[int, ...]) -> np.ndarray:
    """
    Dilates the input tensor by inserting zeros between elements.
    
    Args:
        input: Input tensor
        dilation: Dilation factors for each dimension
        
    Returns:
        Dilated tensor
    """
    if all(d == 1 for d in dilation):
        return input

    N, C, H, W = input.shape
    dH, dW = dilation
    
    H_dilated = H + (H - 1) * (dH - 1)
    W_dilated = W + (W - 1) * (dW - 1)
    
    output = np.zeros((N, C, H_dilated, W_dilated))
    output[:, :, ::dH, ::dW] = input
    
    return output

def _bilinear_interpolate(input: np.ndarray,
                       points: np.ndarray,
                       align_corners: bool = True) -> np.ndarray:
    """
    Performs bilinear interpolation on the input tensor at specified points.
    
    Args:
        input: Input tensor (N, C, H, W)
        points: Points to sample (N, P, 2) in normalized coordinates [-1, 1]
        align_corners: Whether to align corners in interpolation
        
    Returns:
        Interpolated values (N, C, P)
    """ 
    N, C, H, W = input.shape
    _, P, _ = points.shape

    # Convert normalized coordinates to pixel coordinates
    if align_corners:
        x = (points[..., 0] + 1) * (W - 1) / 2
        y = (points[..., 1] + 1) * (H - 1) / 2
    else:
        x = ((points[..., 0] + 1) * W - 1) / 2
        y = ((points[..., 1] + 1) * H - 1) / 2

    # Get corner indices
    x0 = np.floor(x).astype(np.int32)
    x1 = x0 + 1
    y0 = np.floor(y).astype(np.int32)
    y1 = y0 + 1

    # Clip to image boundaries
    x0 = np.clip(x0, 0, W - 1)
    x1 = np.clip(x1, 0, W - 1)
    y0 = np.clip(y0, 0, H - 1)
    y1 = np.clip(y1, 0, H - 1)

    # Calculate interpolation weights
    wa = (x1 - x) * (y1 - y)
    wb = (x1 - x) * (y - y0)
    wc = (x - x0) * (y1 - y)
    wd = (x - x0) * (y - y0)

    # Gather corner values
    Ia = np.zeros((N, C, P))
    Ib = np.zeros((N, C, P))
    Ic = np.zeros((N, C, P))
    Id = np.zeros((N, C, P))

    for n in range(N):
        for p in range(P):
            Ia[n, :, p] = input[n, :, y0[n, p], x0[n, p]]
            Ib[n, :, p] = input[n, :, y1[n, p], x0[n, p]]
            Ic[n, :, p] = input[n, :, y0[n, p], x1[n, p]]
            Id[n, :, p] = input[n, :, y1[n, p], x1[n, p]]

    # Reshape weights for broadcasting
    wa = wa.reshape(N, 1, P)
    wb = wb.reshape(N, 1, P)
    wc = wc.reshape(N, 1, P)
    wd = wd.reshape(N, 1, P)

    # Interpolate
    out = wa * Ia + wb * Ib + wc * Ic + wd * Id
    return out

def _bilinear_interpolate_gradient(grad_output: np.ndarray,
                                points: np.ndarray,
                                input_size: Tuple[int, ...],
                                align_corners: bool = True) -> Tuple[np.ndarray, np.ndarray]:
    """
    Computes gradients for bilinear interpolation.
    
    Args:
        grad_output: Gradient of loss with respect to interpolated values (can be any shape)
        points: Points that were sampled (N, P, 2)
        input_size: Size of the input tensor (H, W)
        align_corners: Whether corners were aligned in interpolation
    
    Returns:
        Tuple of gradients with respect to input and points
    """
    # Ensure grad_output is properly shaped (N, C, P)
    if grad_output.ndim == 1:
        grad_output = grad_output.reshape(1, 1, -1)
    elif grad_output.ndim == 2:
        grad_output = grad_output.reshape(1, *grad_output.shape)
        
    N, C, P = grad_output.shape
    H, W = input_size
    
    # Ensure points is properly shaped
    if points.ndim == 2:
        points = points.reshape(1, *points.shape)
    
    # Convert normalized coordinates to pixel coordinates
    if align_corners:
        x = (points[..., 0] + 1) * (W - 1) / 2
        y = (points[..., 1] + 1) * (H - 1) / 2
    else:
        x = ((points[..., 0] + 1) * W - 1) / 2
        y = ((points[..., 1] + 1) * H - 1) / 2
    
    # Get corner indices
    x0 = np.floor(x).astype(np.int32)
    x1 = x0 + 1
    y0 = np.floor(y).astype(np.int32)
    y1 = y0 + 1
    
    # Clip to image boundaries
    x0 = np.clip(x0, 0, W - 1)
    x1 = np.clip(x1, 0, W - 1)
    y0 = np.clip(y0, 0, H - 1)
    y1 = np.clip(y1, 0, H - 1)
    
    # Compute weights for bilinear interpolation
    wa = (x1 - x) * (y1 - y)
    wb = (x1 - x) * (y - y0)
    wc = (x - x0) * (y1 - y)
    wd = (x - x0) * (y - y0)
    
    # Initialize gradients
    grad_input = np.zeros((N, C, H, W))
    grad_points = np.zeros_like(points)
    
    # Compute gradients with respect to input
    for n in range(N):
        for c in range(C):
            for p in range(P):
                grad = grad_output[n, c, p]
                grad_input[n, c, y0[n, p], x0[n, p]] += grad * wa[n, p]
                grad_input[n, c, y1[n, p], x0[n, p]] += grad * wb[n, p]
                grad_input[n, c, y0[n, p], x1[n, p]] += grad * wc[n, p]
                grad_input[n, c, y1[n, p], x1[n, p]] += grad * wd[n, p]
    
    # Compute gradients with respect to points
    if align_corners:
        dx = (W - 1) / 2
        dy = (H - 1) / 2
    else:
        dx = W / 2
        dy = H / 2
        
    for n in range(N):
        for p in range(P):
            grad = grad_output[n, :, p].sum()  # Sum over channels
            
            # Gradient with respect to x
            gx = (grad * (
                (y1[n, p] - y[n, p]) * (input[:, :, y0[n, p], x1[n, p]] - input[:, :, y0[n, p], x0[n, p]]).sum() +
                (y[n, p] - y0[n, p]) * (input[:, :, y1[n, p], x1[n, p]] - input[:, :, y1[n, p], x0[n, p]]).sum()
            )) * dx
            
            # Gradient with respect to y
            gy = (grad * (
                (x1[n, p] - x[n, p]) * (input[:, :, y1[n, p], x0[n, p]] - input[:, :, y0[n, p], x0[n, p]]).sum() +
                (x[n, p] - x0[n, p]) * (input[:, :, y1[n, p], x1[n, p]] - input[:, :, y0[n, p], x1[n, p]]).sum()
            )) * dy
            
            grad_points[n, p] = [gx, gy]
    
    return grad_input, grad_points

def _generate_grid(batch_size: int, height: int, width: int,
                 align_corners: bool = True) -> np.ndarray:
    """
    Generates a coordinate grid for grid sampling.
    
    Args:
        batch_size: Number of samples in batch
        height: Height of the grid
        width: Width of the grid
        align_corners: Whether to align corners
        
    Returns:
        Grid tensor of shape (N, H, W, 2) with normalized coordinates
    """
    if align_corners:
        x = np.linspace(-1, 1, width)
        y = np.linspace(-1, 1, height)
    else:
        x = np.linspace(-1 + (1/width), 1 - (1/width), width)
        y = np.linspace(-1 + (1/height), 1 - (1/height), height)

    x_coords, y_coords = np.meshgrid(x, y)
    grid = np.stack([x_coords, y_coords], axis=-1)
    grid = np.tile(grid[None], (batch_size, 1, 1, 1))
    
    return grid

def _deform_grid(grid: np.ndarray, offset: np.ndarray) -> np.ndarray:
    """
    Deforms a regular grid using offset values.
    
    Args:
        grid: Regular coordinate grid (N, H, W, 2)
        offset: Offset values for deformation (N, 2, H, W)
        
    Returns:
        Deformed grid (N, H, W, 2)
    """
    N, H, W, _ = grid.shape
    
    # Reshape offset to match grid shape
    offset = offset.transpose(0, 2, 3, 1)
    
    # Add offset to grid
    deformed_grid = grid + offset
    
    # Clamp values to [-1, 1] to ensure valid sampling
    return np.clip(deformed_grid, -1, 1)

def _modulated_deform_grid(grid: np.ndarray, offset: np.ndarray, 
                        mask: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    Deforms a regular grid using offset values and modulation mask.
    Used in Deformable ConvNets v2.
    
    Args:
        grid: Regular coordinate grid (N, H, W, 2)
        offset: Offset values for deformation (N, 2, H, W)
        mask: Modulation mask (N, 1, H, W)
        
    Returns:
        Tuple of deformed grid and modulation mask
    """
    # Deform grid
    deformed_grid = _deform_grid(grid, offset)
    
    # Reshape mask to match sampling points
    mask = mask.transpose(0, 2, 3, 1)
    
    return deformed_grid, mask

def _compute_indices_weights(points: np.ndarray, size: Tuple[int, int]) -> Tuple[np.ndarray, ...]:
    """
    Computes indices and weights for bilinear interpolation.
    
    Args:
        points: Sampling points (N, H, W, 2)
        size: Size of the input feature map (H, W)
        
    Returns:
        Tuple of indices and weights for bilinear interpolation
    """
    H, W = size
    
    # Convert points to pixel coordinates
    x = (points[..., 0] + 1) * (W - 1) / 2
    y = (points[..., 1] + 1) * (H - 1) / 2
    
    # Get corner indices
    x0 = np.floor(x).astype(np.int32)
    x1 = x0 + 1
    y0 = np.floor(y).astype(np.int32)
    y1 = y0 + 1
    
    # Clip to image boundaries
    x0 = np.clip(x0, 0, W - 1)
    x1 = np.clip(x1, 0, W - 1)
    y0 = np.clip(y0, 0, H - 1)
    y1 = np.clip(y1, 0, H - 1)
    
    # Compute weights
    wa = (x1 - x) * (y1 - y)
    wb = (x1 - x) * (y - y0)
    wc = (x - x0) * (y1 - y)
    wd = (x - x0) * (y - y0)
    
    return (x0, x1, y0, y1), (wa, wb, wc, wd)

def _apply_deform_conv(input: np.ndarray, weight: np.ndarray, offset: np.ndarray,
                    stride: Tuple[int, int], padding: Tuple[int, int],
                    dilation: Tuple[int, int], mask: Optional[np.ndarray] = None) -> np.ndarray:
    """
    Applies deformable convolution operation.
    
    Args:
        input: Input feature map (N, C_in, H, W)
        weight: Convolution weights (C_out, C_in, kH, kW)
        offset: Sampling offsets (N, 2*kH*kW, H_out, W_out)
        stride: Convolution stride
        padding: Zero-padding size
        dilation: Dilation rate
        mask: Optional modulation mask for v2 (N, kH*kW, H_out, W_out)
        
    Returns:
        Output feature map (N, C_out, H_out, W_out)
    """
    N, C_in, H, W = input.shape
    C_out, _, kH, kW = weight.shape
    H_out = (H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0] + 1
    W_out = (W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1] + 1
    
    # Generate sampling grid
    grid = _generate_grid(N, H_out, W_out)
    
    # Deform grid using offsets
    if mask is not None:
        deformed_grid, modulation = _modulated_deform_grid(grid, offset, mask)
    else:
        deformed_grid = _deform_grid(grid, offset)
        modulation = None
    
    # Get sampling indices and weights
    indices, weights = _compute_indices_weights(deformed_grid, (H, W))
    x0, x1, y0, y1 = indices
    wa, wb, wc, wd = weights
    
    # Initialize output
    output = np.zeros((N, C_out, H_out, W_out))
    
    # Apply convolution with deformed sampling
    for i in range(kH):
        for j in range(kW):
            # Get values from input feature map
            values = (wa[..., None] * input[:, :, y0, x0] +
                     wb[..., None] * input[:, :, y1, x0] +
                     wc[..., None] * input[:, :, y0, x1] +
                     wd[..., None] * input[:, :, y1, x1])
                     
            # Apply modulation if available
            if modulation is not None:
                values = values * modulation[:, i*kW + j, ..., None]
                
            # Accumulate weighted values
            for cout in range(C_out):
                output[:, cout] += np.sum(values * weight[cout, :, i, j], axis=1)
    
    return output

class Conv2dFunction(Function):
    """
    Implements 2D convolution with support for:
    - Asymmetric kernels
    - Different strides for height and width
    - Different padding for height and width
    - Different dilation rates for height and width
    - Transposed convolution
    - Deformable convolution
    - Grouped convolution
    """
    @staticmethod
    def forward(ctx: Context, x: Tensor, weight: Tensor, bias: Optional[Tensor] = None,
            stride: Tuple[int, int] = (1, 1), padding: Tuple[int, int] = (0, 0),
            dilation: Tuple[int, int] = (1, 1), groups: int = 1,
            mode: str = ConvMode.STANDARD, offset: Optional[Tensor] = None,
            mask: Optional[Tensor] = None) -> Tensor:
        """Forward pass of flexible 2D convolution."""
        # Validate parameters
        _validate_conv_params(x.shape, weight.shape, stride, padding, dilation, groups, 
                            mode, offset, weight, mask)
        
        # Get required offset tensor and compute sampling locations for deformable conv
        if mode == ConvMode.DEFORMABLE:
            offset_tensor = offset if offset is not None else weight.offset
            sampling_locations = _get_deformable_offsets(
                offset_tensor.data,
                weight.shape[2:],
                x.shape
            )
            # Ensure sampling_locations has correct shape for _im2col_dilated
            N, HW, kHW, _ = sampling_locations.shape
            H_out = W_out = int(np.sqrt(HW))  # Assuming square output
            sampling_locations = sampling_locations.reshape(N, H_out * W_out, kHW, 2)
        else:
            sampling_locations = None
            offset_tensor = None  # Explicitly set to None if not deformable
        
        # Save tensors and info for backward pass
        if mode == ConvMode.DEFORMABLE:
            ctx.save_for_backward(x, weight, bias, offset_tensor)
        else:
            ctx.save_for_backward(x, weight, bias)
            
        ctx.save_arguments(stride=stride, padding=padding, dilation=dilation, 
                        groups=groups, mode=mode, sampling_locations=sampling_locations)
        
        # Process each group
        x_padded = _pad_input(x.data, padding)
        C_in_per_group = x.shape[1] // groups
        C_out_per_group = weight.shape[0] // groups
        H_out, W_out = _get_output_shape(x.shape, weight.shape[2:], stride, padding, dilation, mode)
        output = np.zeros((x.shape[0], weight.shape[0], H_out, W_out))
        
        for g in range(groups):
            x_g = x_padded[:, g*C_in_per_group:(g+1)*C_in_per_group]
            w_g = weight.data[g*C_out_per_group:(g+1)*C_out_per_group]
            
            # Convert input patches to columns
            x_cols = _im2col_dilated(x_g, weight.shape[2:], stride, dilation, mode, 
                                sampling_locations)
            
            # Reshape weight for matrix multiplication
            w_reshaped = w_g.reshape(C_out_per_group, -1)
            
            # Perform convolution
            out = w_reshaped @ x_cols
            
            # Reshape output
            output[:, g*C_out_per_group:(g+1)*C_out_per_group] = out.reshape(
                C_out_per_group, x.shape[0], *output.shape[2:]
            ).transpose(1, 0, 2, 3)
        
        # Add bias if present
        if bias is not None:
            output += bias.data.reshape(1, -1, 1, 1)
            
        return Tensor(output)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        """
        Backward pass of 2D convolution.
        
        Computes gradients for:
        - Input tensor (x)
        - Weight tensor (w)
        - Bias (if present)
        - Offsets (for deformable convolution)
        """
        # Get saved tensors and arguments
        saved_tensors = ctx.saved_tensors
        if len(saved_tensors) == 4:  # Deformable case
            x, weight, bias, offset_tensor = saved_tensors
        else:  # Standard case
            x, weight, bias = saved_tensors
            offset_tensor = None
            
        stride = ctx.saved_arguments['stride']
        padding = ctx.saved_arguments['padding']
        dilation = ctx.saved_arguments['dilation']
        groups = ctx.saved_arguments['groups']
        mode = ctx.saved_arguments['mode']
        sampling_locations = ctx.saved_arguments.get('sampling_locations', None)

        # Get dimensions
        N = x.shape[0]
        C_in_per_group = x.shape[1] // groups
        C_out_per_group = weight.shape[0] // groups
        out_h, out_w = _get_output_shape(x.shape, weight.shape[2:], stride, padding, dilation, mode)

        # Apply appropriate padding based on mode
        if mode == ConvMode.STANDARD:
            x_padded = _pad_input(x.data, padding)
        elif mode == ConvMode.TRANSPOSED:
            grad_output_padded = _pad_input(grad_output, padding)
        else:  # DEFORMABLE
            x_padded = _pad_input(x.data, padding)

        # Compute input gradient if required
        if x.requires_grad:
            if mode == ConvMode.STANDARD:
                grad_x_padded = np.zeros_like(x_padded)
                for g in range(groups):
                    w_g = weight.data[g*C_out_per_group:(g+1)*C_out_per_group]
                    grad_out_g = grad_output[:, g*C_out_per_group:(g+1)*C_out_per_group]
                    
                    # Reshape weight and gradient for matrix multiplication
                    w_reshaped = w_g.reshape(C_out_per_group, -1).T
                    grad_out_reshaped = grad_out_g.reshape(N, -1).T
                    
                    # Compute gradient using transposed operations
                    grad_cols = w_reshaped @ grad_out_reshaped.reshape(C_out_per_group, -1)
                    grad_x_g = _col2im_dilated(
                        grad_cols,
                        x_padded[:, g*C_in_per_group:(g+1)*C_in_per_group].shape,
                        weight.shape[2:],
                        stride,
                        dilation
                    )
                    grad_x_padded[:, g*C_in_per_group:(g+1)*C_in_per_group] = grad_x_g

                # Remove padding
                if padding[0] > 0 or padding[1] > 0:
                    grad_dict[id(x)] = grad_x_padded[:, :,
                                                padding[0]:grad_x_padded.shape[2]-padding[0],
                                                padding[1]:grad_x_padded.shape[3]-padding[1]]
                else:
                    grad_dict[id(x)] = grad_x_padded

            elif mode == ConvMode.TRANSPOSED:
                grad_x = np.zeros_like(x.data)
                for g in range(groups):
                    w_g = weight.data[g*C_out_per_group:(g+1)*C_out_per_group]
                    grad_out_g = grad_output_padded[:, g*C_out_per_group:(g+1)*C_out_per_group]
                    
                    # For transposed conv, swap stride and dilation
                    grad_cols = _im2col_dilated(
                        grad_out_g,
                        weight.shape[2:],
                        dilation,  # Use dilation as stride
                        stride,    # Use stride as dilation
                        ConvMode.STANDARD
                    )
                    
                    # Compute gradient with flipped weights
                    w_flipped = np.flip(np.flip(w_g, 2), 3).transpose(1, 0, 2, 3)
                    w_reshaped = w_flipped.reshape(-1, C_out_per_group)
                    grad_x[:, g*C_in_per_group:(g+1)*C_in_per_group] = \
                        (w_reshaped @ grad_cols).reshape(N, C_in_per_group, *x.shape[2:])
                        
                grad_dict[id(x)] = grad_x

            else:  # DEFORMABLE
                grad_x = np.zeros_like(x.data)
                for g in range(groups):
                    w_g = weight.data[g*C_out_per_group:(g+1)*C_out_per_group]
                    grad_out_g = grad_output[:, g*C_out_per_group:(g+1)*C_out_per_group]
                    
                    grad_standard = np.zeros_like(x_padded)
                    w_reshaped = w_g.reshape(C_out_per_group, -1).T
                    grad_out_reshaped = grad_out_g.reshape(N, -1).T
                    grad_cols = w_reshaped @ grad_out_reshaped.reshape(C_out_per_group, -1)
                    
                    # Apply deformable sampling gradients
                    N_out = sampling_locations.shape[1]  # H_out * W_out
                    for n in range(N):
                        for i in range(N_out):
                            # Get sampling locations for this position
                            loc = sampling_locations[n, i].reshape(1, -1, 2)
                            # Compute gradient contribution
                            grad_input, _ = _bilinear_interpolate_gradient(
                                grad_cols[:, n*N_out + i].reshape(-1, 1),
                                loc,
                                x.shape[2:]
                            )
                            grad_standard[n, :] += grad_input
                    
                    grad_x[:, g*C_in_per_group:(g+1)*C_in_per_group] = \
                        grad_standard[:, g*C_in_per_group:(g+1)*C_in_per_group]
                        
                grad_dict[id(x)] = grad_x

        # Compute weight gradient if required
        if weight.requires_grad:
            grad_weight = np.zeros_like(weight.data)
            for g in range(groups):
                if mode == ConvMode.STANDARD:
                    x_g = x_padded[:, g*C_in_per_group:(g+1)*C_in_per_group]
                    grad_out_g = grad_output[:, g*C_out_per_group:(g+1)*C_out_per_group]
                    
                    # Use im2col for efficient computation
                    x_cols = _im2col_dilated(x_g, weight.shape[2:], stride, dilation, mode)
                    grad_out_reshaped = grad_out_g.reshape(N, C_out_per_group, -1)
                    
                    # Accumulate gradients for each batch
                    for n in range(N):
                        grad_w = grad_out_reshaped[n] @ x_cols[:, n*out_h*out_w:(n+1)*out_h*out_w].T
                        grad_weight[g*C_out_per_group:(g+1)*C_out_per_group] += \
                            grad_w.reshape(C_out_per_group, C_in_per_group, *weight.shape[2:])

                elif mode == ConvMode.TRANSPOSED:
                    x_g = x[:, g*C_in_per_group:(g+1)*C_in_per_group]
                    grad_out_g = grad_output_padded[:, g*C_out_per_group:(g+1)*C_out_per_group]
                    
                    # Compute gradient with rotated operations
                    x_cols = _im2col_dilated(
                        x_g.transpose(1, 0, 2, 3),
                        weight.shape[2:],
                        dilation,
                        stride,
                        ConvMode.STANDARD
                    )
                    grad_out_reshaped = grad_out_g.transpose(1, 0, 2, 3).reshape(C_out_per_group, -1)
                    grad_weight[g*C_out_per_group:(g+1)*C_out_per_group] = \
                        (grad_out_reshaped @ x_cols.T).reshape(C_out_per_group, C_in_per_group, 
                                                            *weight.shape[2:])

                else:  # DEFORMABLE
                    x_g = x_padded[:, g*C_in_per_group:(g+1)*C_in_per_group]
                    grad_out_g = grad_output[:, g*C_out_per_group:(g+1)*C_out_per_group]
                    
                    # Handle deformable sampling for each position
                    for n in range(N):
                        deform_cols = _im2col_dilated(
                            x_g[n:n+1],
                            weight.shape[2:],
                            stride,
                            dilation,
                            mode,
                            sampling_locations[n:n+1]
                        )
                        grad_out_n = grad_out_g[n].reshape(C_out_per_group, -1)
                        grad_weight[g*C_out_per_group:(g+1)*C_out_per_group] += \
                            (grad_out_n @ deform_cols.T).reshape(C_out_per_group, C_in_per_group, 
                                                            *weight.shape[2:])
            
            grad_dict[id(weight)] = grad_weight
            
            # Handle offset gradients for deformable convolution
            if offset_tensor is not None and offset_tensor.requires_grad:
                grad_offset = np.zeros_like(offset_tensor.data)
                N_out = sampling_locations.shape[1]  # H_out * W_out
                
                for n in range(N):
                    for i in range(N_out):
                        # Get sampling locations and gradients
                        loc = sampling_locations[n, i].reshape(1, -1, 2)  # shape: (1, kH*kW, 2)
                        grad = grad_output[n].reshape(C_out_per_group, -1)[:, i]  # shape: (C_out_per_group,)
                        
                        # Compute offset gradients
                        _, grad_points = _bilinear_interpolate_gradient(
                            grad,  # Will be reshaped internally
                            loc,
                            x.shape[2:],
                            align_corners=True
                        )
                        grad_offset[n, :, i//out_w, i%out_w] = grad_points.reshape(-1)
                        
                grad_dict[id(offset_tensor)] = grad_offset

        # Compute bias gradient if required
        if bias is not None and bias.requires_grad:
            grad_dict[id(bias)] = grad_output.sum(axis=(0, 2, 3))

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\elementwise.py
// ----------------------------------------
from typing import Dict
import numpy as np
from ..core import Function, Tensor

class Log(Function):
    """
    Natural logarithm operation.
    
    Forward: f(x) = ln(x)
    Backward: f'(x) = 1/x
    """
    @staticmethod
    def forward(ctx, x):
        if not isinstance(x, Tensor):
            x = Tensor(x)
            
        # Check for negative values
        if np.any(x.data <= 0):
            raise ValueError("Log of negative numbers or zero is undefined")
            
        ctx.save_for_backward(x)
        return Tensor(np.log(x.data))
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        x, = ctx.saved_tensors
        if x.requires_grad:
            # d/dx(log(x)) = 1/x
            grad_dict[id(x)] = grad_output / x.data

class Exp(Function):
    """
    Exponential operation.
    
    Forward: f(x) = exp(x)
    Backward: f'(x) = exp(x)
    """
    @staticmethod
    def forward(ctx, x):
        if not isinstance(x, Tensor):
            x = Tensor(x)
            
        result = np.exp(x.data)
        ctx.save_for_backward(x)  # Save x for backward pass
        ctx.save_arguments(exp_x=result)  # Save exp(x) as argument
        return Tensor(result)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        x, = ctx.saved_tensors
        exp_x = ctx.saved_arguments['exp_x']
        
        if x.requires_grad:
            # d/dx(exp(x)) = exp(x)
            grad_dict[id(x)] = grad_output * exp_x

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\loss.py
// ----------------------------------------
from typing import Dict, Optional
import numpy as np
from ..core import Function, Tensor

class MSELoss(Function):
    """
    Mean Squared Error Loss: L = 1/N * Σ(y - ŷ)²
    
    Args:
        reduction (str): Specifies the reduction to apply to the output:
            'mean' (default) | 'sum' | 'none'
    """
    
    @staticmethod
    def forward(ctx, predictions, targets, reduction='mean'):
        if not isinstance(predictions, Tensor):
            predictions = Tensor(predictions)
        if not isinstance(targets, Tensor):
            targets = Tensor(targets)
            
        if predictions.shape != targets.shape:
            raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
            
        diff = predictions.data - targets.data
        squared_diff = diff * diff
        
        if reduction == 'none':
            result = squared_diff
        elif reduction == 'sum':
            result = np.sum(squared_diff)
        elif reduction == 'mean':
            result = np.mean(squared_diff)
        else:
            raise ValueError(f"Invalid reduction method: {reduction}")
            
        ctx.save_for_backward(predictions, targets)
        ctx.save_arguments(reduction=reduction)
        return Tensor(result)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        predictions, targets = ctx.saved_tensors
        reduction = ctx.saved_arguments['reduction']
        
        diff = predictions.data - targets.data
        
        if reduction == 'mean':
            grad = grad_output * 2 * diff / np.prod(diff.shape)
        elif reduction == 'sum':
            grad = grad_output * 2 * diff
        else:  # 'none'
            grad = grad_output * 2 * diff
            
        if predictions.requires_grad:
            grad_dict[id(predictions)] = grad

class CrossEntropyLoss(Function):
    """
    Cross Entropy Loss with built-in LogSoftmax: L = -Σ y_true * log(softmax(y_pred))
    
    Args:
        reduction (str): Specifies the reduction to apply to the output:
            'mean' (default) | 'sum' | 'none'
    """
    
    @staticmethod
    def _log_softmax(x):
        # Compute log(softmax(x)) in a numerically stable way
        max_x = np.max(x, axis=1, keepdims=True)
        exp_x = np.exp(x - max_x)
        sum_exp_x = np.sum(exp_x, axis=1, keepdims=True)
        return (x - max_x) - np.log(sum_exp_x)
        
    @staticmethod
    def forward(ctx, predictions, targets, reduction='mean'):
        if not isinstance(predictions, Tensor):
            predictions = Tensor(predictions)
        if not isinstance(targets, Tensor):
            targets = Tensor(targets)
            
        log_softmax = CrossEntropyLoss._log_softmax(predictions.data)
        nll_loss = -np.sum(targets.data * log_softmax, axis=1)
        
        if reduction == 'none':
            result = nll_loss
        elif reduction == 'sum':
            result = np.sum(nll_loss)
        elif reduction == 'mean':
            result = np.mean(nll_loss)
        else:
            raise ValueError(f"Invalid reduction method: {reduction}")
            
        ctx.save_for_backward(predictions, targets)
        ctx.save_arguments(reduction=reduction, log_softmax=log_softmax)
        return Tensor(result)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        predictions, targets = ctx.saved_tensors
        reduction = ctx.saved_arguments['reduction']
        log_softmax = ctx.saved_arguments['log_softmax']
        
        grad_output = np.array(grad_output)
        if reduction == 'mean':
            grad_output = grad_output / len(targets.data)
        
        softmax = np.exp(log_softmax)
        grad = grad_output.reshape(-1, 1) * (softmax - targets.data)
            
        if predictions.requires_grad:
            grad_dict[id(predictions)] = grad

class BinaryCrossEntropyLoss(Function):
    """
    Binary Cross Entropy Loss: L = -Σ (y * log(p) + (1-y) * log(1-p))
    
    Args:
        reduction (str): Specifies the reduction to apply to the output:
            'mean' (default) | 'sum' | 'none'
        eps (float): Small value for numerical stability
    """
    
    @staticmethod
    def forward(ctx, predictions, targets, reduction='mean', eps=1e-7):
        if not isinstance(predictions, Tensor):
            predictions = Tensor(predictions)
        if not isinstance(targets, Tensor):
            targets = Tensor(targets)

        # Check valid probability values
        if np.any(predictions.data < 0) or np.any(predictions.data > 1):
            raise ValueError("Predictions must be in range [0, 1]")
            
        # Clip predictions to prevent log(0)
        predictions_clipped = np.clip(predictions.data, eps, 1 - eps)
        
        loss = -(targets.data * np.log(predictions_clipped) + 
                (1 - targets.data) * np.log(1 - predictions_clipped))
                
        if reduction == 'none':
            result = loss
        elif reduction == 'sum':
            result = float(np.sum(loss))  # Convert to scalar
        elif reduction == 'mean':
            result = float(np.mean(loss))  # Convert to scalar
        else:
            raise ValueError(f"Invalid reduction method: {reduction}")
            
        ctx.save_for_backward(predictions, targets)
        ctx.save_arguments(reduction=reduction, eps=eps)
        return Tensor(result)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        predictions, targets = ctx.saved_tensors
        reduction = ctx.saved_arguments['reduction']
        eps = ctx.saved_arguments['eps']
        
        predictions_clipped = np.clip(predictions.data, eps, 1 - eps)
        
        grad = grad_output * (predictions_clipped - targets.data) / (
            predictions_clipped * (1 - predictions_clipped))
            
        if reduction == 'mean':
            grad = grad / np.prod(targets.shape)
            
        if predictions.requires_grad:
            grad_dict[id(predictions)] = grad

class L1Loss(Function):
    """
    L1 Loss (Mean Absolute Error): L = |y - ŷ|
    
    Args:
        reduction (str): Specifies the reduction to apply to the output:
            'mean' (default) | 'sum' | 'none'
    """
    
    @staticmethod
    def forward(ctx, predictions, targets, reduction='mean'):
        if not isinstance(predictions, Tensor):
            predictions = Tensor(predictions)
        if not isinstance(targets, Tensor):
            targets = Tensor(targets)
            
        if predictions.shape != targets.shape:
            raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
            
        diff = predictions.data - targets.data
        abs_diff = np.abs(diff)
        
        if reduction == 'none':
            result = abs_diff
        elif reduction == 'sum':
            result = np.sum(abs_diff)
        elif reduction == 'mean':
            result = np.mean(abs_diff)
        else:
            raise ValueError(f"Invalid reduction method: {reduction}")
            
        ctx.save_for_backward(predictions, targets)
        ctx.save_arguments(reduction=reduction)
        return Tensor(result)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        predictions, targets = ctx.saved_tensors
        reduction = ctx.saved_arguments['reduction']
        
        diff = predictions.data - targets.data
        grad = np.sign(diff)
        
        if reduction == 'mean':
            grad = grad * grad_output / np.prod(diff.shape)
        else:  # 'sum' or 'none'
            grad = grad * grad_output
            
        if predictions.requires_grad:
            grad_dict[id(predictions)] = grad

class KLDivLoss(Function):
    """
    Kullback-Leibler Divergence Loss.
    KL divergence measures the relative entropy between two probability distributions.
    
    Args:
        reduction (str): Specifies the reduction to apply to the output:
            'mean' (default) | 'sum' | 'none'
        log_target (bool): If True, target is expected to be log-probabilities
    """
    
    @staticmethod
    def forward(ctx, predictions, targets, reduction='mean', log_target=False):
        if not isinstance(predictions, Tensor):
            predictions = Tensor(predictions)
        if not isinstance(targets, Tensor):
            targets = Tensor(targets)
            
        if not log_target:
            targets_log = np.log(np.clip(targets.data, 1e-7, 1.0))
        else:
            targets_log = targets.data
            
        # KL divergence formula: KL(P||Q) = P * (log(P) - log(Q))
        loss = np.exp(targets_log) * (targets_log - predictions.data)
        loss = -loss  # Correct the sign to make it positive
        
        if reduction == 'none':
            result = loss
        elif reduction == 'sum':
            result = np.sum(loss)
        elif reduction == 'mean':
            result = np.mean(loss)
        else:
            raise ValueError(f"Invalid reduction method: {reduction}")
            
        ctx.save_for_backward(predictions, targets)
        ctx.save_arguments(reduction=reduction, log_target=log_target)
        return Tensor(result)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        predictions, targets = ctx.saved_tensors
        reduction = ctx.saved_arguments['reduction']
        log_target = ctx.saved_arguments['log_target']
        
        if not log_target:
            targets_log = np.log(np.clip(targets.data, 1e-7, 1.0))
        else:
            targets_log = targets.data
            
        grad = -np.exp(targets_log) * grad_output
        
        if reduction == 'mean':
            grad = grad / np.prod(predictions.shape)
            
        if predictions.requires_grad:
            grad_dict[id(predictions)] = grad

class CosineSimilarityLoss(Function):
    """
    Cosine Similarity Loss.
    Measures the cosine similarity between two vectors.
    
    Args:
        dim (int): Dimension along which cosine similarity is computed
        eps (float): Small value to avoid division by zero
        reduction (str): Specifies the reduction to apply to the output
    """
    
    @staticmethod
    def forward(ctx, x1, x2, dim=1, eps=1e-8, reduction='mean'):
        if not isinstance(x1, Tensor):
            x1 = Tensor(x1)
        if not isinstance(x2, Tensor):
            x2 = Tensor(x2)
            
        # Compute norms
        norm1 = np.sqrt(np.sum(x1.data * x1.data, axis=dim, keepdims=True))
        norm2 = np.sqrt(np.sum(x2.data * x2.data, axis=dim, keepdims=True))
        
        # Normalize vectors
        x1_normalized = x1.data / np.maximum(norm1, eps)
        x2_normalized = x2.data / np.maximum(norm2, eps)
        
        # Compute cosine similarity
        cos_sim = np.sum(x1_normalized * x2_normalized, axis=dim)
        
        # For orthogonal vectors, cos_sim = 0, we want loss = 1
        # For identical vectors, cos_sim = 1, we want loss = 0
        # Therefore, loss = 1 - cos_sim
        if reduction == 'none':
            result = 1 - cos_sim
        elif reduction == 'sum':
            result = float(np.sum(1 - cos_sim))
        elif reduction == 'mean':
            result = float(np.mean(1 - cos_sim))
        else:
            raise ValueError(f"Invalid reduction method: {reduction}")
            
        ctx.save_for_backward(x1, x2)
        ctx.save_arguments(dim=dim, eps=eps, reduction=reduction,
                         x1_normalized=x1_normalized,
                         x2_normalized=x2_normalized)
        return Tensor(result)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        x1, x2 = ctx.saved_tensors
        dim = ctx.saved_arguments['dim']
        eps = ctx.saved_arguments['eps']
        reduction = ctx.saved_arguments['reduction']
        x1_normalized = ctx.saved_arguments['x1_normalized']
        x2_normalized = ctx.saved_arguments['x2_normalized']
        
        if reduction == 'mean':
            grad_output = grad_output / x1.shape[0]
        
        # Gradient with respect to x1
        if x1.requires_grad:
            grad_x1 = -grad_output[..., None] * x2_normalized
            grad_dict[id(x1)] = grad_x1
            
        # Gradient with respect to x2
        if x2.requires_grad:
            grad_x2 = -grad_output[..., None] * x1_normalized
            grad_dict[id(x2)] = grad_x2

class HingeLoss(Function):
    """
    Hinge Loss (max-margin loss).
    Commonly used for SVM training.
    L = max(0, margin - y * f(x))
    
    Args:
        margin (float): Margin in the hinge loss
        reduction (str): Specifies the reduction to apply to the output
    """
    
    @staticmethod
    def forward(ctx, predictions, targets, margin=1.0, reduction='mean'):
        if not isinstance(predictions, Tensor):
            predictions = Tensor(predictions)
        if not isinstance(targets, Tensor):
            targets = Tensor(targets)
            
        # Convert targets to ±1
        signed_targets = 2.0 * targets.data - 1.0
        
        # Compute raw hinge loss
        loss = np.maximum(0, margin - signed_targets * predictions.data)
        
        if reduction == 'none':
            result = loss
        elif reduction == 'sum':
            result = np.sum(loss)
        elif reduction == 'mean':
            result = np.mean(loss)
        else:
            raise ValueError(f"Invalid reduction method: {reduction}")
            
        ctx.save_for_backward(predictions, targets)
        ctx.save_arguments(margin=margin, reduction=reduction,
                         signed_targets=signed_targets)
        return Tensor(result)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        predictions, targets = ctx.saved_tensors
        margin = ctx.saved_arguments['margin']
        reduction = ctx.saved_arguments['reduction']
        signed_targets = ctx.saved_arguments['signed_targets']
        
        # Gradient is -y when margin - y*f(x) > 0, 0 otherwise
        mask = (margin - signed_targets * predictions.data) > 0
        grad = -signed_targets * mask * grad_output
        
        if reduction == 'mean':
            grad = grad / np.prod(predictions.shape)
            
        if predictions.requires_grad:
            grad_dict[id(predictions)] = grad

class FocalLoss(Function):
    """
    Focal Loss.
    Addresses class imbalance by down-weighting easily classified examples.
    FL(p) = -alpha * (1-p)^gamma * log(p)
    
    Args:
        alpha (float): Weighting factor for rare classes
        gamma (float): Focusing parameter
        reduction (str): Specifies the reduction to apply to the output
    """
    
    @staticmethod
    def forward(ctx, predictions, targets, alpha=0.25, gamma=2.0, reduction='mean'):
        if not isinstance(predictions, Tensor):
            predictions = Tensor(predictions)
        if not isinstance(targets, Tensor):
            targets = Tensor(targets)
            
        # Clip predictions for numerical stability
        eps = 1e-7
        predictions_clipped = np.clip(predictions.data, eps, 1 - eps)
        
        # Compute pt (probability of target class)
        pt = predictions_clipped * targets.data + (1 - predictions_clipped) * (1 - targets.data)
        
        # Compute focal weight
        focal_weight = alpha * ((1 - pt) ** gamma)
        
        # Compute binary cross entropy
        bce = -(targets.data * np.log(predictions_clipped) + 
                (1 - targets.data) * np.log(1 - predictions_clipped))
        
        # Apply focal weight
        loss = focal_weight * bce
        
        if reduction == 'none':
            result = loss
        elif reduction == 'sum':
            result = np.sum(loss)
        elif reduction == 'mean':
            result = np.mean(loss)
        else:
            raise ValueError(f"Invalid reduction method: {reduction}")
            
        ctx.save_for_backward(predictions, targets)
        ctx.save_arguments(alpha=alpha, gamma=gamma, reduction=reduction,
                         pt=pt, focal_weight=focal_weight)
        return Tensor(result)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        predictions, targets = ctx.saved_tensors
        alpha = ctx.saved_arguments['alpha']
        gamma = ctx.saved_arguments['gamma']
        reduction = ctx.saved_arguments['reduction']
        pt = ctx.saved_arguments['pt']
        focal_weight = ctx.saved_arguments['focal_weight']
        
        # Compute gradient
        grad = grad_output * focal_weight * (
            gamma * pt * np.log(pt) + pt - targets.data
        )
        
        if reduction == 'mean':
            grad = grad / np.prod(predictions.shape)
            
        if predictions.requires_grad:
            grad_dict[id(predictions)] = grad

class HuberLoss(Function):
    """
    Huber Loss: L = 0.5 * (y - ŷ)² if |y - ŷ| <= delta else delta * |y - ŷ| - 0.5 * delta²
    
    This loss combines the best properties of MSE and L1 loss.
    For small errors it behaves like MSE, for large errors it behaves like L1.
    
    Args:
        delta (float): Threshold where loss transitions from squared to linear
        reduction (str): Specifies the reduction to apply to the output:
            'mean' (default) | 'sum' | 'none'
    """
    
    @staticmethod
    def forward(ctx, predictions, targets, delta=1.0, reduction='mean'):
        if not isinstance(predictions, Tensor):
            predictions = Tensor(predictions)
        if not isinstance(targets, Tensor):
            targets = Tensor(targets)
            
        if predictions.shape != targets.shape:
            raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
            
        diff = predictions.data - targets.data
        abs_diff = np.abs(diff)
        quadratic = np.minimum(abs_diff, delta)
        linear = abs_diff - quadratic
        loss = 0.5 * quadratic ** 2 + delta * linear
        
        if reduction == 'none':
            result = loss
        elif reduction == 'sum':
            result = np.sum(loss)
        elif reduction == 'mean':
            result = np.mean(loss)
        else:
            raise ValueError(f"Invalid reduction method: {reduction}")
            
        ctx.save_for_backward(predictions, targets)
        ctx.save_arguments(delta=delta, reduction=reduction)
        return Tensor(result)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        predictions, targets = ctx.saved_tensors
        delta = ctx.saved_arguments['delta']
        reduction = ctx.saved_arguments['reduction']
        
        diff = predictions.data - targets.data
        abs_diff = np.abs(diff)
        
        # Gradient is diff/|diff| * min(|diff|, delta)
        grad = np.sign(diff) * np.minimum(abs_diff, delta)
        
        if reduction == 'mean':
            grad = grad * grad_output / np.prod(diff.shape)
        else:  # 'sum' or 'none'
            grad = grad * grad_output
            
        if predictions.requires_grad:
            grad_dict[id(predictions)] = grad

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\matrix.py
// ----------------------------------------
from typing import Dict, Optional, Union, Tuple
import numpy as np
from ..core import Function, Tensor

class Transpose(Function):
    @staticmethod
    def forward(ctx, x, axes: Optional[Tuple[int, ...]] = None):
        if not isinstance(x, Tensor):
            x = Tensor(x)
            
        ctx.save_for_backward(x)
        ctx.save_arguments(axes=axes)
        
        if axes is None:
            return Tensor(np.transpose(x.data))
        return Tensor(np.transpose(x.data, axes))
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        x, = ctx.saved_tensors
        axes = ctx.saved_arguments['axes']
        
        if x.requires_grad:
            if axes is None:
                # For standard transpose, just transpose the gradient
                grad_dict[id(x)] = np.transpose(grad_output)
            else:
                # For specific axes, need to invert the permutation
                inverse_axes = np.argsort(axes)
                grad_dict[id(x)] = np.transpose(grad_output, inverse_axes)

class Compare(Function):
    """Base class for comparison operations"""
    @staticmethod
    def _compare(op, x1, x2):
        if not isinstance(x1, Tensor):
            x1 = Tensor(x1)
        if not isinstance(x2, Tensor):
            x2 = Tensor(x2)
            
        return Tensor(op(x1.data, x2.data))
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        # Comparison operations have no gradient
        pass

class Greater(Compare):
    @staticmethod
    def forward(ctx, x1, x2):
        return Compare._compare(np.greater, x1, x2)

class GreaterEqual(Compare):
    @staticmethod
    def forward(ctx, x1, x2):
        return Compare._compare(np.greater_equal, x1, x2)

class Less(Compare):
    @staticmethod
    def forward(ctx, x1, x2):
        return Compare._compare(np.less, x1, x2)

class LessEqual(Compare):
    @staticmethod
    def forward(ctx, x1, x2):
        return Compare._compare(np.less_equal, x1, x2)

class Equal(Compare):
    @staticmethod
    def forward(ctx, x1, x2):
        return Compare._compare(np.equal, x1, x2)

class NotEqual(Compare):
    @staticmethod
    def forward(ctx, x1, x2):
        return Compare._compare(np.not_equal, x1, x2)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\nn.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\power.py
// ----------------------------------------
from typing import Dict
import numpy as np
from ..core import Function, Tensor

class Power(Function):
    @staticmethod
    def forward(ctx, base, exponent):
        if not isinstance(base, Tensor):
            base = Tensor(base)
        if not isinstance(exponent, (Tensor, int, float)):
            raise TypeError("Exponent must be a Tensor, int, or float")
            
        # Convert Tensor exponent to scalar if possible
        if isinstance(exponent, Tensor):
            if exponent.data.size == 1:
                exponent = float(exponent.data)
            else:
                raise ValueError("Only scalar exponents are supported")
                
        ctx.save_for_backward(base)
        ctx.save_arguments(exponent=exponent)
        
        return Tensor(np.power(base.data, exponent))
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        base, = ctx.saved_tensors
        exponent = ctx.saved_arguments['exponent']
        
        if base.requires_grad:
            # d/dx(x^n) = nx^(n-1)
            grad = grad_output * exponent * np.power(base.data, exponent - 1)
            grad_dict[id(base)] = grad

class Divide(Function):
    @staticmethod
    def forward(ctx, numerator, denominator):
        if not isinstance(numerator, Tensor):
            numerator = Tensor(numerator)
        if not isinstance(denominator, Tensor):
            denominator = Tensor(denominator)
            
        # Check for division by zero
        if np.any(denominator.data == 0):
            raise ValueError("Division by zero encountered")
            
        ctx.save_for_backward(numerator, denominator)
        return Tensor(numerator.data / denominator.data)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        numerator, denominator = ctx.saved_tensors
        
        if numerator.requires_grad:
            # d/dx(x/y) = 1/y
            grad_dict[id(numerator)] = grad_output / denominator.data
            
        if denominator.requires_grad:
            # d/dy(x/y) = -x/y^2
            grad_dict[id(denominator)] = -grad_output * numerator.data / (denominator.data ** 2)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\reduction.py
// ----------------------------------------
from typing import Dict, Optional, Union, Tuple
import numpy as np
from ..core import Function, Tensor

class Sum(Function):
    @staticmethod
    def forward(ctx, x, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False):
        if not isinstance(x, Tensor):
            x = Tensor(x)
            
        ctx.save_for_backward(x)
        ctx.save_arguments(axis=axis, keepdims=keepdims, input_shape=x.shape)
        
        return Tensor(np.sum(x.data, axis=axis, keepdims=keepdims))
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        x, = ctx.saved_tensors
        axis = ctx.saved_arguments['axis']
        keepdims = ctx.saved_arguments['keepdims']
        input_shape = ctx.saved_arguments['input_shape']
        
        if x.requires_grad:
            # If not keeping dims, need to reshape grad_output to match broadcast
            if not keepdims and axis is not None:
                grad_output = np.expand_dims(grad_output, axis=axis)
                
            # Broadcast gradient to match input shape
            grad = np.broadcast_to(grad_output, input_shape)
            grad_dict[id(x)] = grad

class Mean(Function):
    @staticmethod
    def forward(ctx, x, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False):
        if not isinstance(x, Tensor):
            x = Tensor(x)
            
        ctx.save_for_backward(x)
        ctx.save_arguments(axis=axis, keepdims=keepdims, input_shape=x.shape)
        
        return Tensor(np.mean(x.data, axis=axis, keepdims=keepdims))
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        x, = ctx.saved_tensors
        axis = ctx.saved_arguments['axis']
        keepdims = ctx.saved_arguments['keepdims']
        input_shape = ctx.saved_arguments['input_shape']
        
        if x.requires_grad:
            # If not keeping dims, need to reshape grad_output to match broadcast
            if not keepdims and axis is not None:
                grad_output = np.expand_dims(grad_output, axis=axis)
                
            # Calculate number of elements we're taking mean over
            if axis is None:
                n = np.prod(input_shape)
            else:
                n = np.prod([input_shape[i] for i in (axis,) if i < len(input_shape)])
                
            # Broadcast gradient to match input shape and divide by n
            grad = np.broadcast_to(grad_output, input_shape) / n
            grad_dict[id(x)] = grad

class Max(Function):
    @staticmethod
    def forward(ctx, x, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False):
        if not isinstance(x, Tensor):
            x = Tensor(x)
            
        result = np.amax(x.data, axis=axis, keepdims=True)
        ctx.save_for_backward(x)
        ctx.save_arguments(axis=axis, keepdims=keepdims, max_vals=result)
        
        if not keepdims:
            result = np.squeeze(result, axis=axis)
            
        return Tensor(result)
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        x, = ctx.saved_tensors
        axis = ctx.saved_arguments['axis']
        keepdims = ctx.saved_arguments['keepdims']
        max_vals = ctx.saved_arguments['max_vals']
        
        if x.requires_grad:
            # If not keeping dims, need to reshape grad_output
            if not keepdims and axis is not None:
                grad_output = np.expand_dims(grad_output, axis=axis)
                
            # Create gradient mask (1 where x equals max, 0 elsewhere)
            mask = (x.data == max_vals)
            
            # In case of multiple maxima, distribute gradient equally
            mask = mask / np.sum(mask, axis=axis, keepdims=True)
            
            grad_dict[id(x)] = grad_output * mask

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\reshape.py
// ----------------------------------------
# DLpy/ops/reshape.py
from ..core.function import Function
from ..core.tensor import Tensor
import numpy as np
from typing import Dict

class Reshape(Function):
    @staticmethod
    def forward(ctx, tensor, shape):
        # Save both the input tensor and the target shape
        ctx.save_for_backward(tensor)
        ctx.save_arguments(target_shape=shape)
        # Create and return a new tensor with the reshaped data
        return Tensor(tensor.data.reshape(shape))
        
    @staticmethod
    def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
        # Get the original tensor and reshape the gradient back to its shape
        original_tensor, = ctx.saved_tensors
        if original_tensor.requires_grad:
            # Reshape gradient back to the original tensor's shape
            grad_dict[id(original_tensor)] = grad_output.reshape(original_tensor.shape)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\__init__.py
// ----------------------------------------
"""
Optimization algorithms for DLpy.

This module implements various optimization algorithms used in deep learning.
"""

from .optimizer import Optimizer
from .sgd import SGD
from .adam import Adam
from .rmsprop import RMSprop
from .adagrad import AdaGrad

__all__ = [
    'Optimizer',
    'SGD',
    'Adam',
    'RMSprop',
    'AdaGrad'
]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\adagrad.py
// ----------------------------------------
import numpy as np
from typing import Dict, Iterator, Optional
from .optimizer import Optimizer
from ..core import Tensor

class AdaGrad(Optimizer):
    """
    Implements AdaGrad algorithm.
    
    AdaGrad is an optimizer with parameter-specific learning rates,
    which are adapted based on historical gradient information.
    
    Args:
        params: Iterable of parameters to optimize
        lr (float): Learning rate (default: 1e-2)
        lr_decay (float): Learning rate decay (default: 0)
        weight_decay (float): Weight decay (L2 penalty) (default: 0)
        eps (float): Term added to denominator to improve numerical stability (default: 1e-10)
        initial_accumulator_value (float): Initial value for accumulator (default: 0)
    """
    
    def __init__(self, params, lr: float = 1e-2, lr_decay: float = 0,
                 weight_decay: float = 0, initial_accumulator_value: float = 0,
                 eps: float = 1e-10):
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= lr_decay:
            raise ValueError(f"Invalid lr_decay value: {lr_decay}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")
        if not 0.0 <= initial_accumulator_value:
            raise ValueError(f"Invalid initial_accumulator_value value: {initial_accumulator_value}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")
            
        defaults = dict(lr=lr, lr_decay=lr_decay, eps=eps, 
                       weight_decay=weight_decay,
                       initial_accumulator_value=initial_accumulator_value)
        super().__init__(params, defaults)

        for group in self._params:
            state = self.state[id(group)]
            state['step'] = 0
            state['sum'] = np.full_like(group.data, initial_accumulator_value, dtype=np.float64)

    def step(self) -> None:
        """
        Performs a single optimization step.
        
        For each parameter p, accumulates the square of the gradient and then
        updates the parameter using the formula:
        p = p - lr * g / (sqrt(accumulator) + eps)
        where g is the gradient.
        """
        for p in self._params:
            if p.grad is None:
                continue
                
            grad = p.grad
            state = self.state[id(p)]

            state['step'] += 1

            if self.defaults['weight_decay'] != 0:
                grad = grad + self.defaults['weight_decay'] * p.data

            # Update accumulator with squared gradient
            state['sum'] += grad * grad

            # Compute the adaptive learning rate
            std = np.sqrt(state['sum'])
            
            # Add epsilon for numerical stability before division
            denom = std + self.defaults['eps']

            # Apply learning rate decay if specified
            if self.defaults['lr_decay'] != 0:
                lr = self.defaults['lr'] / (1 + (state['step'] - 1) * self.defaults['lr_decay'])
            else:
                lr = self.defaults['lr']

            # Update parameters
            p.data -= lr * grad / denom

    def reset_state(self) -> None:
        """
        Resets the state of the optimizer.
        
        This can be useful when you want to restart training or when you want to 
        reset the accumulated gradients without creating a new optimizer instance.
        """
        initial_accumulator_value = self.defaults['initial_accumulator_value']
        
        for group in self._params:
            state = self.state[id(group)]
            state['step'] = 0
            state['sum'] = np.full_like(group.data, initial_accumulator_value, dtype=np.float64)

    def state_dict(self) -> Dict:
        """
        Returns the state of the optimizer as a Dict.
        
        The returned state dict contains two entries:
            * state - a dict holding current optimization state. Its content
                differs between optimizer classes.
            * param_groups - a dict containing all parameter groups
        """
        return {
            'state': self.state,
            'defaults': self.defaults
        }

    def load_state_dict(self, state_dict: Dict) -> None:
        """
        Loads the optimizer state.
        
        Args:
            state_dict (dict): Optimizer state. Should be an object returned
                from a call to state_dict().
        """
        self.state = state_dict['state']
        self.defaults = state_dict['defaults']

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\adam.py
// ----------------------------------------
import numpy as np
from typing import Dict, Iterator, Optional
from .optimizer import Optimizer
from ..core import Tensor

class Adam(Optimizer):
    """
    Implements Adam algorithm.
    
    Args:
        params: Iterable of parameters to optimize
        lr (float): Learning rate (default: 0.001)
        betas (tuple): Coefficients for computing running averages of gradient and its square
            (default: (0.9, 0.999))
        eps (float): Term added to denominator to improve numerical stability (default: 1e-8)
        weight_decay (float): Weight decay (L2 penalty) (default: 0)
        amsgrad (bool): Whether to use the AMSGrad variant (default: False)
    """
    
    def __init__(self, params, lr: float = 0.001, betas: tuple = (0.9, 0.999),
                 eps: float = 1e-8, weight_decay: float = 0, amsgrad: bool = False):
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")
            
        defaults = dict(lr=lr, betas=betas, eps=eps,
                       weight_decay=weight_decay, amsgrad=amsgrad)
        super().__init__(params, defaults)
        
    def step(self) -> None:
        """Performs a single optimization step."""
        for p in self._params:
            if p.grad is None:
                continue
                
            grad = p.grad
            
            # Get optimizer state
            state = self.state[id(p)]
            
            # State initialization
            if len(state) == 0:
                state['step'] = 0
                # Exponential moving average of gradient values
                state['exp_avg'] = np.zeros_like(p.data)
                # Exponential moving average of squared gradient values
                state['exp_avg_sq'] = np.zeros_like(p.data)
                if self.defaults['amsgrad']:
                    # Maintains max of all exp. moving avg. of sq. grad. values
                    state['max_exp_avg_sq'] = np.zeros_like(p.data)
                    
            exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
            if self.defaults['amsgrad']:
                max_exp_avg_sq = state['max_exp_avg_sq']
            beta1, beta2 = self.defaults['betas']
            
            state['step'] += 1
            bias_correction1 = 1 - beta1 ** state['step']
            bias_correction2 = 1 - beta2 ** state['step']
            
            if self.defaults['weight_decay'] != 0:
                grad = grad + self.defaults['weight_decay'] * p.data
                
            # Decay the first and second moment running average coefficient
            exp_avg = beta1 * exp_avg + (1 - beta1) * grad
            exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * grad * grad
            
            if self.defaults['amsgrad']:
                # Maintains the maximum of all 2nd moment running avg. till now
                max_exp_avg_sq = np.maximum(max_exp_avg_sq, exp_avg_sq)
                # Use the max. for normalizing running avg. of gradient
                denom = (np.sqrt(max_exp_avg_sq) / np.sqrt(bias_correction2)) + self.defaults['eps']
            else:
                denom = (np.sqrt(exp_avg_sq) / np.sqrt(bias_correction2)) + self.defaults['eps']
                
            step_size = self.defaults['lr'] / bias_correction1
            
            p.data -= step_size * exp_avg / denom
            
            # Save state
            state['exp_avg'] = exp_avg
            state['exp_avg_sq'] = exp_avg_sq
            if self.defaults['amsgrad']:
                state['max_exp_avg_sq'] = max_exp_avg_sq

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\optimizer.py
// ----------------------------------------
from typing import Dict, Iterator, Optional
from ..core import Tensor

class Optimizer:
    """
    Base class for all optimizers.
    
    Args:
        params: An iterable of parameters to optimize or a dict of parameter groups
        defaults: Dictionary of default hyperparameter values
    """
    
    def __init__(self, params, defaults: Dict):
        self.defaults = defaults
        self._params = list(params)  # Convert iterator to list
        self.state: Dict = {}  # State dict for optimizer states
        
        # Initialize state for each parameter
        for p in self._params:
            self.state[id(p)] = {}
            
    def zero_grad(self) -> None:
        """Clears the gradients of all optimized parameters."""
        for p in self._params:
            if p.grad is not None:
                p.grad.fill(0)
                
    def step(self) -> None:
        """Performs a single optimization step.
        
        This method should be overridden by all optimizers.
        """
        raise NotImplementedError
        
    def add_param_group(self, param_group: Dict) -> None:
        """Add a param group to the optimizer's param groups.
        
        Args:
            param_group (dict): Specifies parameters and parameter-specific options
        """
        params = param_group['params']
        if isinstance(params, Tensor):
            param_group['params'] = [params]
        elif isinstance(params, set):
            param_group['params'] = list(params)
            
        for param in param_group['params']:
            if id(param) not in self.state:
                self.state[id(param)] = {}
            self._params.append(param)
            
    def load_state_dict(self, state_dict: Dict) -> None:
        """Loads the optimizer state.
        
        Args:
            state_dict (dict): Optimizer state dict
        """
        self.state = state_dict['state']
        
    def state_dict(self) -> Dict:
        """Returns the state of the optimizer as a dict.
        
        Returns:
            dict: The state of the optimizer
        """
        return {
            'state': self.state,
        }

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\rmsprop.py
// ----------------------------------------
import numpy as np 
from typing import Dict, Iterator, Optional
from .optimizer import Optimizer
from ..core import Tensor

class RMSprop(Optimizer):
    """
    Implements RMSprop algorithm.
    
    Args:
        params: Iterable of parameters to optimize
        lr (float): Learning rate (default: 0.01)
        alpha (float): Smoothing constant (default: 0.99)
        eps (float): Term added to denominator to improve numerical stability (default: 1e-8)
        weight_decay (float): Weight decay (L2 penalty) (default: 0)
        momentum (float): Momentum factor (default: 0)
        centered (bool): If True, compute centered RMSprop, gradients normalized by their variance
    """
    
    def __init__(self, params, lr: float = 0.01, alpha: float = 0.99,
                 eps: float = 1e-8, weight_decay: float = 0,
                 momentum: float = 0, centered: bool = False):
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")
        if not 0.0 <= momentum:
            raise ValueError(f"Invalid momentum value: {momentum}")
        if not 0.0 <= alpha:
            raise ValueError(f"Invalid alpha value: {alpha}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")
            
        defaults = dict(lr=lr, alpha=alpha, eps=eps, weight_decay=weight_decay,
                       momentum=momentum, centered=centered)
        super().__init__(params, defaults)
        
    def step(self) -> None:
        """Performs a single optimization step."""
        for p in self._params:
            if p.grad is None:
                continue
                
            grad = p.grad
            state = self.state[id(p)]
            
            # State initialization
            if len(state) == 0:
                state['step'] = 0
                state['square_avg'] = np.zeros_like(p.data)
                if self.defaults['momentum'] > 0:
                    state['momentum_buffer'] = np.zeros_like(p.data)
                if self.defaults['centered']:
                    state['grad_avg'] = np.zeros_like(p.data)
                    
            square_avg = state['square_avg']
            alpha = self.defaults['alpha']
            
            state['step'] += 1
            
            if self.defaults['weight_decay'] != 0:
                grad = grad + self.defaults['weight_decay'] * p.data
                
            # Update squared average
            square_avg = alpha * square_avg + (1 - alpha) * grad * grad
            
            if self.defaults['centered']:
                grad_avg = state['grad_avg']
                grad_avg = alpha * grad_avg + (1 - alpha) * grad
                avg = square_avg - grad_avg * grad_avg
                state['grad_avg'] = grad_avg
            else:
                avg = square_avg
                
            # Apply momentum if enabled
            if self.defaults['momentum'] > 0:
                buf = state.get('momentum_buffer', np.zeros_like(grad))
                buf = self.defaults['momentum'] * buf + grad / (np.sqrt(avg) + self.defaults['eps'])
                state['momentum_buffer'] = buf
                p.data -= self.defaults['lr'] * buf
            else:
                p.data -= self.defaults['lr'] * grad / (np.sqrt(avg) + self.defaults['eps'])
                
            # Save state
            state['square_avg'] = square_avg

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\sgd.py
// ----------------------------------------
from typing import Dict, Iterator, Optional
import numpy as np
from .optimizer import Optimizer
from ..core import Tensor

class SGD(Optimizer):
    """
    Implements stochastic gradient descent with momentum.
    
    Args:
        params: Iterable of parameters to optimize
        lr (float): Learning rate (default: 0.1)
        momentum (float): Momentum factor (default: 0)
        weight_decay (float): Weight decay (L2 penalty) (default: 0)
        dampening (float): Dampening for momentum (default: 0)
        nesterov (bool): Enables Nesterov momentum (default: False)
    """
    
    def __init__(self, params, lr: float = 0.1, momentum: float = 0.0,
                 weight_decay: float = 0.0, dampening: float = 0.0,
                 nesterov: bool = False):
        if lr < 0.0:
            raise ValueError(f"Invalid learning rate: {lr}")
        if momentum < 0.0:
            raise ValueError(f"Invalid momentum value: {momentum}")
        if weight_decay < 0.0:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")
            
        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay,
                       dampening=dampening, nesterov=nesterov)
        super().__init__(params, defaults)
        
    def step(self) -> None:
        """Performs a single optimization step."""
        
        for p in self._params:
            if p.grad is None:
                continue
                
            grad = p.grad
            
            # Apply weight decay
            if self.defaults['weight_decay'] != 0:
                grad = grad + self.defaults['weight_decay'] * p.data
                
            # Get or initialize momentum buffer
            if 'momentum_buffer' not in self.state[id(p)]:
                buf = self.state[id(p)]['momentum_buffer'] = np.zeros_like(p.data)
            else:
                buf = self.state[id(p)]['momentum_buffer']
                
            # Update momentum buffer
            if self.defaults['momentum'] != 0:
                buf *= self.defaults['momentum']
                if self.defaults['dampening'] != 0:
                    grad *= 1 - self.defaults['dampening']
                buf += grad
            else:
                buf = grad
                
            # Nesterov momentum
            if self.defaults['nesterov']:
                grad += self.defaults['momentum'] * buf
            else:
                grad = buf
                
            # Update parameters
            p.data -= self.defaults['lr'] * grad
            
            # Store updated momentum buffer
            self.state[id(p)]['momentum_buffer'] = buf

// File: C:\Users\aluja\Desktop\DLpy\examples\basic_autograd.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\examples\neural_network.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\setup.py
// ----------------------------------------
from setuptools import setup, find_packages

setup(
    name="DLpy",  # Changed from DLpy to DLpy
    version="0.1.0",
    packages=find_packages(include=["DLpy", "DLpy.*"]),  # Changed from DLpy to DLpy
    install_requires=[
        "numpy>=1.20.0",
    ],
    extras_require={
        "dev": [
            "pytest>=7.0.0",
            "pytest-cov>=4.0.0",
            "pytest-xdist>=3.0.0",
            "black>=22.0.0",
            "isort>=5.0.0",
            "mypy>=1.0.0",
        ],
    },
    python_requires=">=3.8",
)

// File: C:\Users\aluja\Desktop\DLpy\tests\__init__.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\tests\test_activations.py
// ----------------------------------------
import pytest
import numpy as np
from DLpy.core import Tensor
from DLpy.nn.activations import (
    relu, leaky_relu, elu, gelu, sigmoid, tanh,
    ReLU, LeakyReLU, ELU, GELU, Sigmoid, Tanh
)

class TestActivations:
    """Tests for activation functions"""
    
    def test_relu(self):
        """Test ReLU activation"""
        x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
        y = relu(x)
        y.backward(np.ones_like(x.data))
        
        assert np.array_equal(y.data, [0.0, 0.0, 0.0, 1.0, 2.0])
        assert np.array_equal(x.grad, [0.0, 0.0, 0.0, 1.0, 1.0])
        
    def test_leaky_relu(self):
        """Test Leaky ReLU activation"""
        x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
        slope = 0.1
        y = leaky_relu(x, negative_slope=slope)
        y.backward(np.ones_like(x.data))
        
        expected_forward = [-0.2, -0.1, 0.0, 1.0, 2.0]
        expected_backward = [slope, slope, slope, 1.0, 1.0]
        
        assert np.allclose(y.data, expected_forward)
        assert np.allclose(x.grad, expected_backward)
        
    def test_elu(self):
        """Test ELU activation"""
        x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
        alpha = 1.0
        y = elu(x, alpha=alpha)
        y.backward(np.ones_like(x.data))
        
        expected_forward = [alpha * (np.exp(-2.0) - 1), alpha * (np.exp(-1.0) - 1), 0.0, 1.0, 2.0]
        expected_backward = [alpha * np.exp(-2.0), alpha * np.exp(-1.0), alpha * 1.0, 1.0, 1.0]
        
        assert np.allclose(y.data, expected_forward)
        assert np.allclose(x.grad, expected_backward)
        
    def test_gelu(self):
        """Test GELU activation"""
        x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
        y = gelu(x)
        y.backward(np.ones_like(x.data))
        
        # Values should be finite and have correct shape
        assert not np.any(np.isnan(y.data))
        assert not np.any(np.isinf(y.data))
        assert y.data.shape == x.data.shape
        assert not np.any(np.isnan(x.grad))
        assert not np.any(np.isinf(x.grad))
        
        # Test specific known values
        assert np.allclose(y.data[2], 0.0)  # GELU(0) = 0
        assert y.data[3] > 0.8  # GELU(1) ≈ 0.841
        assert y.data[1] < 0  # GELU(-1) is negative
        
    def test_sigmoid(self):
        """Test Sigmoid activation"""
        x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
        y = sigmoid(x)
        y.backward(np.ones_like(x.data))
        
        sigmoid_x = 1 / (1 + np.exp(-x.data))
        expected_backward = sigmoid_x * (1 - sigmoid_x)
        
        assert np.allclose(y.data, sigmoid_x)
        assert np.allclose(x.grad, expected_backward)
        
        # Test special values
        assert np.allclose(y.data[2], 0.5)  # sigmoid(0) = 0.5
        assert y.data[0] < 0.5  # sigmoid(-2) < 0.5
        assert y.data[4] > 0.5  # sigmoid(2) > 0.5
        
    def test_tanh(self):
        """Test Tanh activation"""
        x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
        y = tanh(x)
        y.backward(np.ones_like(x.data))
        
        expected_forward = np.tanh(x.data)
        expected_backward = 1 - np.tanh(x.data) ** 2
        
        assert np.allclose(y.data, expected_forward)
        assert np.allclose(x.grad, expected_backward)
        
        # Test special values
        assert np.allclose(y.data[2], 0.0)  # tanh(0) = 0
        assert y.data[0] < -0.9  # tanh(-2) ≈ -0.964
        assert y.data[4] > 0.9   # tanh(2) ≈ 0.964

class TestNumericalStability:
    """Tests for numerical stability of activation functions"""
    
    def test_sigmoid_stability(self):
        """Test Sigmoid with large inputs"""
        x = Tensor([1000.0, -1000.0], requires_grad=True)
        y = sigmoid(x)
        y.backward(np.ones_like(x.data))
        
        # Check that values are properly clamped
        assert np.allclose(y.data[0], 1.0)
        assert np.allclose(y.data[1], 0.0)
        assert not np.any(np.isnan(y.data))
        assert not np.any(np.isnan(x.grad))
        
    def test_elu_stability(self):
        """Test ELU with large negative inputs"""
        x = Tensor([-1000.0], requires_grad=True)
        y = elu(x)
        y.backward(np.ones_like(x.data))
        
        # Should be close to -1.0 for large negative values
        assert np.allclose(y.data[0], -1.0, rtol=1e-3)
        assert not np.any(np.isnan(y.data))
        assert not np.any(np.isnan(x.grad))
        
    def test_gelu_stability(self):
        """Test GELU with large inputs"""
        x = Tensor([1000.0, -1000.0], requires_grad=True)
        y = gelu(x)
        y.backward(np.ones_like(x.data))
        
        # Check that outputs are finite
        assert not np.any(np.isnan(y.data))
        assert not np.any(np.isinf(y.data))
        assert not np.any(np.isnan(x.grad))
        assert not np.any(np.isinf(x.grad))

class TestGradientFlow:
    """Tests for gradient flow through activation functions"""
    
    def test_relu_dead_neurons(self):
        """Test ReLU gradient flow for negative inputs"""
        x = Tensor([-1.0], requires_grad=True)
        y = relu(x)
        y.backward(np.array([1.0]))
        
        assert x.grad[0] == 0.0  # Gradient should be zero for negative input
        
    def test_leaky_relu_gradient_flow(self):
        """Test Leaky ReLU gradient flow"""
        x = Tensor([-1.0], requires_grad=True)
        slope = 0.01
        y = leaky_relu(x, negative_slope=slope)
        y.backward(np.array([1.0]))
        
        assert x.grad[0] == slope  # Should have small but non-zero gradient
        
    def test_elu_gradient_flow(self):
        """Test ELU gradient flow"""
        x = Tensor([-1.0, 1.0], requires_grad=True)
        y = elu(x)
        y.backward(np.ones_like(x.data))
        
        assert x.grad[0] > 0.0  # Should have positive gradient for negative input
        assert x.grad[1] == 1.0  # Should have gradient 1 for positive input

class TestShapes:
    """Tests for handling different input shapes"""
    
    def test_batch_input(self):
        """Test activations with batched input"""
        batch_size, features = 32, 10
        x = Tensor(np.random.randn(batch_size, features), requires_grad=True)
        
        # Test all activations with batched input
        activations = [relu, leaky_relu, elu, gelu, sigmoid, tanh]
        for activation in activations:
            y = activation(x)
            assert y.shape == x.shape
            y.backward(np.ones_like(x.data))
            assert x.grad.shape == x.shape
            
    def test_scalar_input_single(self):
        """Test scalar input handling for a single activation"""
        x = Tensor(2.0, requires_grad=True)
        y = relu(x)
        
        # Check forward pass maintains scalar nature
        assert y.data.ndim == 0
        assert isinstance(y.data, np.ndarray)
        assert y.data.shape == ()
        
        # Check backward pass (gradient should be size 1 array as in PyTorch)
        y.backward(np.array(1.0))
        assert x.grad.size == 1
        assert isinstance(x.grad, np.ndarray)

    def test_scalar_input(self):
        """Test activations with scalar input"""
        x = Tensor(2.0, requires_grad=True)
        
        # Test all activations with scalar input
        activations = [relu, leaky_relu, elu, gelu, sigmoid, tanh]
        for activation in activations:
            y = activation(x)
            assert y.data.ndim == 0  # Should preserve scalar nature
            y.backward(np.array(1.0))
            assert x.grad.size == 1  # Gradient should be size 1 array (matching PyTorch behavior)

class TestCustomGradients:
    """Tests for custom gradient computations"""
    
    def test_relu_custom_gradient(self):
        """Test ReLU with custom gradient"""
        x = Tensor([1.0, -1.0], requires_grad=True)
        y = relu(x)
        y.backward(np.array([2.0, 2.0]))  # Custom gradient values
        
        assert np.array_equal(x.grad, [2.0, 0.0])  # Should scale gradient for positive input
        
    def test_sigmoid_custom_gradient(self):
        """Test Sigmoid with custom gradient"""
        x = Tensor([0.0], requires_grad=True)
        y = sigmoid(x)
        y.backward(np.array([2.0]))  # Custom gradient value
        
        expected_grad = 2.0 * 0.25  # 2.0 * sigmoid(0) * (1 - sigmoid(0))
        assert np.allclose(x.grad, expected_grad)

// File: C:\Users\aluja\Desktop\DLpy\tests\test_autograd.py
// ----------------------------------------
import pytest
import numpy as np
from DLpy.core import (
    Tensor,
    get_autograd_engine
)
from DLpy.ops import Add, Multiply
from DLpy.core.autograd import Edge

class TestAutogradEngine:
    """Tests for the autograd engine's core functionality."""
    
    def setup_method(self):
        """Setup method run before each test."""
        self.engine = get_autograd_engine()
        self.engine.clear()

    def test_register_tensor(self):
        """Test registering a tensor with the autograd engine."""
        tensor = Tensor([1.0], requires_grad=True)
        self.engine.register_tensor(tensor)
        assert id(tensor) in self.engine._nodes

    def test_add_edge(self):
        """Test adding edges between tensors in the computational graph."""
        t1 = Tensor([1.0], requires_grad=True)
        t2 = Tensor([2.0], requires_grad=True)
        
        self.engine.register_tensor(t1)
        self.engine.register_tensor(t2)
        self.engine.add_edge(t1, t2)
        
        node1 = self.engine._nodes[id(t1)]
        node2 = self.engine._nodes[id(t2)]
        
        assert len(node1.out_edges) == 1
        assert len(node2.in_edges) == 1
        assert node1.out_edges[0].dst == node2

class TestGradientComputation:
    """Tests for gradient computation in different graph structures."""
    
    def setup_method(self):
        self.engine = get_autograd_engine()
        self.engine.clear()

    def test_linear_graph(self):
        """Test gradient computation in a linear graph."""
        # Create a simple linear computation: z = 2x + y
        x = Tensor([2.0], requires_grad=True)
        y = Tensor([3.0], requires_grad=True)
        z = Add.apply(x, y)
        self.engine.backward(z, np.array([1.0]))
        
        # Check gradients
        assert np.allclose(x.grad, [1.0])
        assert np.allclose(y.grad, [1.0])

    def test_branching_graph(self):
        """Test gradient computation in a graph with multiple paths."""

        # The test creates a computation graph shaped like:
        #     x
        #   /   \  
        #  y1   y2
        #   \   /
        #     z

        # This tests whether gradients properly flow and accumulate through 
        # multiple paths back to the same input.
        x = Tensor([2.0], requires_grad=True)
        y1 = Multiply.apply(x, Tensor([2.0]))  # y1 = 2x
        y2 = Multiply.apply(x, Tensor([3.0]))  # y2 = 3x
        z = Add.apply(y1, y2)  # z = y1 + y2 = 5x

        self.engine.backward(z, np.array([1.0]))
        # Gradient should be 5.0 (sum of both paths: 2 + 3)
        assert np.allclose(x.grad, [5.0])

    def test_diamond_graph(self):
        """Test gradient computation in a diamond-shaped graph."""
        # Create a diamond computation:
        #     x
        #    / \
        #   h1  h2
        #    \ /
        #     y
        x = Tensor([1.0], requires_grad=True)
        w1 = Tensor([2.0], requires_grad=True)
        w2 = Tensor([3.0], requires_grad=True)
        
        h1 = Multiply.apply(x, w1)
        h2 = Multiply.apply(x, w2)
        y = Add.apply(h1, h2)
        
        self.engine.backward(y, np.array([1.0]))
        
        # x's gradient should include effects from both paths
        assert np.allclose(x.grad, [5.0])  # 2 + 3
        assert np.allclose(w1.grad, [1.0])
        assert np.allclose(w2.grad, [1.0])

class TestGradientAccumulation:
    """Tests for correct gradient accumulation behavior."""

    def setup_method(self):
        self.engine = get_autograd_engine()
        self.engine.clear()

    def test_reused_variable(self):
        """Test gradient accumulation when a variable is used multiple times."""
        x = Tensor([2.0], requires_grad=True)
        
        # Use x in three separate computations
        y1 = Multiply.apply(x, Tensor([2.0]))
        y2 = Multiply.apply(x, Tensor([3.0]))
        y3 = Multiply.apply(x, Tensor([4.0]))
        
        # Backward on all three outputs
        self.engine.backward(y1, np.array([1.0]))
        self.engine.backward(y2, np.array([1.0]))
        self.engine.backward(y3, np.array([1.0]))
        
        # Gradient should accumulate: 2 + 3 + 4 = 9
        assert np.allclose(x.grad, [9.0])

    def test_shared_structure(self):
        """Test gradient computation with shared subgraphs."""
        # Create a computation where the same subgraph is used multiple times
        x = Tensor([1.0], requires_grad=True)
        y = Tensor([2.0], requires_grad=True)
        
        # Shared computation
        shared = Multiply.apply(x, y)
        
        # Use shared result multiple times
        out1 = Multiply.apply(shared, Tensor([2.0]))
        out2 = Multiply.apply(shared, Tensor([3.0]))
        
        # Final sum
        final = Add.apply(out1, out2)
        
        self.engine.backward(final, np.array([1.0]))
        
        # Verify gradients include effects from all paths
        assert x.grad is not None
        assert y.grad is not None

class TestAdvancedAutogradFeatures:
    """Tests for advanced AutogradEngine features and edge cases"""
    
    def setup_method(self):
        self.engine = get_autograd_engine()
        self.engine.clear()

    def test_validate_graph(self):
        """Test graph validation functionality"""
        # Create a disconnected subgraph
        x = Tensor([1.0], requires_grad=True)
        y = Tensor([2.0], requires_grad=True)
        _ = Add.apply(x, y)
        
        # Add an isolated node
        z = Tensor([3.0], requires_grad=True)
        self.engine.register_tensor(z)
        
        warnings = self.engine.validate_graph()
        assert len(warnings) > 0
        assert "isolated nodes" in warnings[0]  

    def test_nested_gradient_computation(self):
        """Test detection of nested gradient computations"""
        x = Tensor([1.0], requires_grad=True)
        y = Add.apply(x, Tensor([2.0]))
        
        # Simulate nested gradient computation
        self.engine._currently_computing_gradients = True
        with pytest.raises(RuntimeError, match="Nested gradient computation detected"):
            self.engine.backward(y)
        self.engine._currently_computing_gradients = False

    def test_cyclic_graph_detection(self):
        """Test detection of cycles in computational graph"""
        x = Tensor([1.0], requires_grad=True)
        y = Tensor([2.0], requires_grad=True)
        
        # Manually create a cycle in the graph
        node_x = self.engine._nodes[id(x)]
        node_y = self.engine._nodes[id(y)]
        
        edge1 = Edge(node_x, node_y)
        edge2 = Edge(node_y, node_x)
        
        node_x.out_edges.append(edge1)
        node_y.in_edges.append(edge1)
        node_y.out_edges.append(edge2)
        node_x.in_edges.append(edge2)
        
        with pytest.raises(RuntimeError, match="Cycle detected in computation graph"):
            self.engine.backward(x)

    def test_gradient_shape_mismatch(self):
        """Test detection of gradient shape mismatches"""
        x = Tensor([[1.0]], requires_grad=True)  # Shape (1, 1)
        y = Tensor([2.0], requires_grad=True)    # Shape (1,)
        
        # Create edge with obviously wrong shape
        node_x = self.engine._nodes[id(x)]
        node_y = self.engine._nodes[id(y)]
        
        edge = Edge(node_x, node_y)
        edge.grad = np.array([[1.0, 2.0]])  # Wrong shape (1, 2)
        
        # Add the edge to both nodes and the engine
        node_x.out_edges.append(edge)
        node_y.in_edges.append(edge)
        self.engine._edges.add(edge)
        
        warnings = self.engine.validate_graph()
        assert any("shape mismatch" in w for w in warnings), "Should detect shape mismatch"

// File: C:\Users\aluja\Desktop\DLpy\tests\test_cnn.py
// ----------------------------------------
import pytest
import numpy as np
from DLpy.core import Tensor
from DLpy.nn import Conv2d
from DLpy.ops.cnn import (
    Conv2dFunction, ConvMode,
    _compute_conv_output_shape,
    _unfold,
    _fold,
    _bilinear_interpolate,
    _generate_grid,
    _deform_grid
)

class TestConv2d:
    """Tests for Conv2d module."""
    
    # [Previous tests remain the same...]

    def test_conv2d_asymmetric_kernel(self):
        """Test Conv2d with asymmetric kernel."""
        batch_size = 2
        in_channels = 3
        out_channels = 16
        height = width = 32
        kernel_size = (3, 5)  # Asymmetric kernel
        
        conv = Conv2d(in_channels, out_channels, kernel_size)
        x = Tensor(np.random.randn(batch_size, in_channels, height, width))
        output = conv(x)
        
        # Check output shape
        expected_height = height - kernel_size[0] + 1
        expected_width = width - kernel_size[1] + 1
        assert output.shape == (batch_size, out_channels, expected_height, expected_width)

    def test_conv2d_asymmetric_stride(self):
        """Test Conv2d with different strides for height and width."""
        batch_size = 2
        in_channels = 3
        out_channels = 16
        height = width = 32
        kernel_size = 3
        stride = (2, 3)  # Different strides
        
        conv = Conv2d(in_channels, out_channels, kernel_size, stride=stride)
        x = Tensor(np.random.randn(batch_size, in_channels, height, width))
        output = conv(x)
        
        expected_height = (height - kernel_size) // stride[0] + 1
        expected_width = (width - kernel_size) // stride[1] + 1
        assert output.shape == (batch_size, out_channels, expected_height, expected_width)

    def test_conv2d_asymmetric_padding(self):
        """Test Conv2d with different padding for height and width."""
        batch_size = 2
        in_channels = 3
        out_channels = 16
        height = width = 32
        kernel_size = 3
        padding = (1, 2)  # Different padding
        
        conv = Conv2d(in_channels, out_channels, kernel_size, padding=padding)
        x = Tensor(np.random.randn(batch_size, in_channels, height, width))
        output = conv(x)
        
        expected_height = height + 2*padding[0] - kernel_size + 1
        expected_width = width + 2*padding[1] - kernel_size + 1
        assert output.shape == (batch_size, out_channels, expected_height, expected_width)

    def test_conv2d_dilated(self):
        """Test dilated convolution."""
        batch_size = 2
        in_channels = 3
        out_channels = 16
        height = width = 32
        kernel_size = 3
        dilation = 2
        
        conv = Conv2d(in_channels, out_channels, kernel_size, dilation=dilation)
        x = Tensor(np.random.randn(batch_size, in_channels, height, width))
        output = conv(x)
        
        effective_kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)
        expected_height = height - effective_kernel_size + 1
        expected_width = width - effective_kernel_size + 1
        assert output.shape == (batch_size, out_channels, expected_height, expected_width)

    def test_conv2d_groups(self):
        """Test grouped convolution."""
        batch_size = 2
        in_channels = 4
        out_channels = 4
        height = width = 32
        kernel_size = 3
        groups = 2
        
        conv = Conv2d(in_channels, out_channels, kernel_size, groups=groups)
        x = Tensor(np.random.randn(batch_size, in_channels, height, width))
        output = conv(x)
        
        expected_height = height - kernel_size + 1
        expected_width = width - kernel_size + 1
        assert output.shape == (batch_size, out_channels, expected_height, expected_width)

class TestConv2dFunction:
    """Tests for Conv2dFunction and related helper functions."""
    
    def test_deformable_conv_forward(self):
        """Test forward pass of deformable convolution."""
        batch_size = 2
        in_channels = 3
        out_channels = 16
        height = width = 8
        kernel_size = 3
        
        x = Tensor(np.random.randn(batch_size, in_channels, height, width))
        weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size))
        offset = Tensor(np.random.randn(batch_size, 2*kernel_size*kernel_size, height-2, width-2))
        bias = Tensor(np.random.randn(out_channels))
        
        output = Conv2dFunction.apply(x, weight, bias, (1, 1), (0, 0), (1, 1), 1,
                                    ConvMode.DEFORMABLE, offset)

    def test_modulated_deform_conv(self):
        """Test modulated deformable convolution."""
        batch_size = 2
        in_channels = 3
        out_channels = 16
        height = width = 8
        kernel_size = 3
        
        x = Tensor(np.random.randn(batch_size, in_channels, height, width))
        weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size))
        offset = Tensor(np.random.randn(batch_size, 2*kernel_size*kernel_size, height-2, width-2))
        mask = Tensor(np.random.randn(batch_size, kernel_size*kernel_size, height-2, width-2))
        bias = Tensor(np.random.randn(out_channels))
        
        output = Conv2dFunction.apply(x, weight, bias, (1, 1), (0, 0), (1, 1), 1,
                                    ConvMode.DEFORMABLE, offset, mask)

class TestHelperFunctions:
    """Tests for CNN helper functions."""
    
    def test_compute_output_shape(self):
        """Test output shape computation."""
        input_size = 32
        kernel_size = 3
        stride = 1
        padding = 0
        dilation = 1
        
        output_size = _compute_conv_output_shape(
            input_size, kernel_size, stride, padding, dilation
        )
        assert output_size == 30  # 32 - 3 + 1

    def test_unfold_operation(self):
        """Test im2col (unfold) operation."""
        batch_size = 2
        channels = 3
        height = width = 8
        kernel_size = (3, 3)
        
        input_tensor = np.random.randn(batch_size, channels, height, width)
        unfolded = _unfold(input_tensor, kernel_size, (1, 1), (0, 0), (1, 1))
        
        # Check unfolded shape
        expected_unfold_shape = (channels * kernel_size[0] * kernel_size[1],
                               batch_size * (height - kernel_size[0] + 1) * 
                               (width - kernel_size[1] + 1))
        assert unfolded.shape == expected_unfold_shape

    def test_fold_operation(self):
        """Test col2im (fold) operation."""
        batch_size = 2
        channels = 3
        height = width = 8
        kernel_size = (3, 3)
        
        # Create random input and unfold it
        input_tensor = np.random.randn(batch_size, channels, height, width)
        unfolded = _unfold(input_tensor, kernel_size, (1, 1), (0, 0), (1, 1))
        
        # Fold back
        folded = _fold(unfolded, (height, width), kernel_size, (1, 1), (0, 0), (1, 1))
        
        # Check folded shape
        assert folded.shape == input_tensor.shape

    def test_bilinear_interpolation(self):
        """Test bilinear interpolation."""
        batch_size = 2
        channels = 3
        height = width = 8
        
        input_tensor = np.random.randn(batch_size, channels, height, width)
        points = np.random.uniform(-1, 1, (batch_size, 4, 2))  # Sample 4 points
        
        interpolated = _bilinear_interpolate(input_tensor, points)
        assert interpolated.shape == (batch_size, channels, 4)

    def test_generate_grid(self):
        """Test sampling grid generation."""
        batch_size = 2
        height = 8
        width = 8
        
        grid = _generate_grid(batch_size, height, width)
        assert grid.shape == (batch_size, height, width, 2)
        assert np.all(grid >= -1) and np.all(grid <= 1)

    def test_deform_grid(self):
        """Test grid deformation."""
        batch_size = 2
        height = 8
        width = 8
        
        grid = _generate_grid(batch_size, height, width)
        offset = np.random.randn(batch_size, 2, height, width) * 0.1
        
        deformed = _deform_grid(grid, offset)
        assert deformed.shape == (batch_size, height, width, 2)
        assert np.all(deformed >= -1) and np.all(deformed <= 1)

    def test_numerical_gradient_deformable(self):
        """Test numerical gradient computation for deformable convolution."""
        batch_size = 2
        in_channels = 2
        out_channels = 3
        height = width = 5
        kernel_size = 3
        
        x = Tensor(np.random.randn(batch_size, in_channels, height, width), requires_grad=True)
        weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size), 
                       requires_grad=True)
        offset = Tensor(np.random.randn(batch_size, 2*kernel_size*kernel_size, height-2, width-2), 
                       requires_grad=True)
        setattr(weight, 'offset', offset)
        bias = Tensor(np.random.randn(out_channels), requires_grad=True)
        
        def compute_loss(x, w, b):
            return np.sum(Conv2dFunction.apply(x, w, b, (1, 1), (0, 0), (1, 1), 1, 
                                             ConvMode.DEFORMABLE).data)
        
        # Compute analytical gradients
        output = Conv2dFunction.apply(x, weight, bias, (1, 1), (0, 0), (1, 1), 1, 
                                    ConvMode.DEFORMABLE)
        output.backward(np.ones_like(output.data))
        
        # Verify offset gradients exist and have correct shape
        assert offset.grad is not None
        assert offset.grad.shape == offset.shape

// File: C:\Users\aluja\Desktop\DLpy\tests\test_context.py
// ----------------------------------------
import pytest
from DLpy.core import Context, Tensor
import numpy as np

class TestContext:
    """Tests for Context class functionality"""
    
    def test_save_and_retrieve_tensors(self):
        """Test saving and retrieving tensors"""
        ctx = Context()
        tensor1 = Tensor([1.0])
        tensor2 = Tensor([2.0])
        
        ctx.save_for_backward(tensor1, tensor2)
        saved = ctx.saved_tensors
        
        assert len(saved) == 2
        assert np.array_equal(saved[0].data, tensor1.data)
        assert np.array_equal(saved[1].data, tensor2.data)

    def test_save_and_retrieve_arguments(self):
        """Test saving and retrieving non-tensor arguments"""
        ctx = Context()
        ctx.save_arguments(arg1="test", arg2=42)
        
        args = ctx.saved_arguments
        assert args["arg1"] == "test"
        assert args["arg2"] == 42
        assert isinstance(args, dict)

    def test_intermediate_values(self):
        """Test storing and retrieving intermediate values"""
        ctx = Context()
        
        # Store various types of values
        ctx.store_intermediate("scalar", 42)
        ctx.store_intermediate("list", [1, 2, 3])
        ctx.store_intermediate("tensor", Tensor([1.0]))
        
        # Retrieve and verify values
        assert ctx.get_intermediate("scalar") == 42
        assert ctx.get_intermediate("list") == [1, 2, 3]
        assert isinstance(ctx.get_intermediate("tensor"), Tensor)
        
        # Test retrieving non-existent key
        with pytest.raises(KeyError):
            ctx.get_intermediate("nonexistent")

    def test_clear_functionality(self):
        """Test clearing all stored data"""
        ctx = Context()
        
        # Store various types of data
        ctx.save_for_backward(Tensor([1.0]))
        ctx.save_arguments(arg1="test")
        ctx.store_intermediate("key", "value")
        
        # Clear all data
        ctx.clear()
        
        # Verify everything is cleared
        assert len(ctx._saved_tensors) == 0
        assert len(ctx._non_tensor_args) == 0
        assert len(ctx._intermediate_values) == 0

// File: C:\Users\aluja\Desktop\DLpy\tests\test_function.py
// ----------------------------------------
import pytest
from DLpy.core import Function, Tensor
import numpy as np

class TestFunction:
    """Tests for Function base class and utilities"""
    
    class TestFunction(Function):
        """Simple test function implementation"""
        
        @staticmethod
        def forward(ctx, x, y=None):
            ctx.save_for_backward(x)
            ctx.save_arguments(y=y)
            return Tensor(x.data * 2)
            
        @staticmethod
        def backward(ctx, grad_output, grad_dict):
            x, = ctx.saved_tensors
            y = ctx.saved_arguments["y"]
            
            if x.requires_grad:
                grad_dict[id(x)] = grad_output * 2

    def test_function_application(self):
        """Test applying a function to inputs"""
        x = Tensor([1.0], requires_grad=True)
        result = self.TestFunction.apply(x, y=2.0)
        
        assert isinstance(result, Tensor)
        assert np.array_equal(result.data, [2.0])
        assert result.requires_grad
        assert result._backward_fn is not None

    def test_verify_backward(self):
        """Test gradient verification utility"""
        def forward_fn(x):
            return x * 2
            
        def correct_backward_fn(ctx, grad_output):
            return grad_output * 2
            
        def incorrect_backward_fn(ctx, grad_output):
            return grad_output * 3
        
        # Test with correct gradients
        x = np.array([1.0])
        assert Function.verify_backward(forward_fn, correct_backward_fn, (x,))
        
        # Test with incorrect gradients
        assert not Function.verify_backward(forward_fn, incorrect_backward_fn, (x,))

    def test_abstract_methods(self):
        """Test that abstract methods raise NotImplementedError"""
        
        class IncompleteFunction(Function):
            pass
            
        with pytest.raises(TypeError):
            IncompleteFunction()

// File: C:\Users\aluja\Desktop\DLpy\tests\test_loss.py
// ----------------------------------------
import pytest
import numpy as np
from DLpy.core import Tensor
from DLpy.ops.loss import (
    MSELoss,
    CrossEntropyLoss,
    BinaryCrossEntropyLoss,
    L1Loss,
    HuberLoss,
    KLDivLoss,
    CosineSimilarityLoss,
    HingeLoss,
    FocalLoss
)

class TestMSELoss:
    """Tests for Mean Squared Error Loss"""
    
    def test_forward(self):
        """Test forward pass of MSE loss"""
        predictions = Tensor([[1.0, 2.0], [3.0, 4.0]])
        targets = Tensor([[2.0, 1.0], [4.0, 3.0]])
        
        # Test mean reduction
        loss = MSELoss.apply(predictions, targets, 'mean')
        expected = np.mean((predictions.data - targets.data) ** 2)
        assert np.allclose(loss.data, expected)
        
        # Test sum reduction
        loss = MSELoss.apply(predictions, targets, 'sum')
        expected = np.sum((predictions.data - targets.data) ** 2)
        assert np.allclose(loss.data, expected)
        
    def test_backward(self):
        """Test backward pass of MSE loss"""
        predictions = Tensor([[1.0]], requires_grad=True)
        targets = Tensor([[2.0]])
        
        loss = MSELoss.apply(predictions, targets, 'mean')
        loss.backward()
        
        # For MSE, gradient should be 2(pred - target)/N
        expected_grad = 2 * (predictions.data - targets.data) / np.prod(predictions.shape)
        assert np.allclose(predictions.grad, expected_grad)

class TestCrossEntropyLoss:
    """Tests for Cross Entropy Loss"""
    
    def test_forward(self):
        """Test forward pass of cross entropy loss"""
        predictions = Tensor([[1.0, 0.0], [0.0, 1.0]])
        targets = Tensor([[1.0, 0.0], [0.0, 1.0]])  # One-hot encoded
        
        loss = CrossEntropyLoss.apply(predictions, targets)
        assert loss.data >= 0  # Loss should be non-negative
        
    def test_numerical_stability(self):
        """Test numerical stability with large inputs"""
        predictions = Tensor([[1000., -1000.], [-1000., 1000.]])
        targets = Tensor([[1., 0.], [0., 1.]])
        
        loss = CrossEntropyLoss.apply(predictions, targets)
        assert not np.isnan(loss.data)
        assert not np.isinf(loss.data)
        
    def test_gradient(self):
        """Test gradient computation"""
        predictions = Tensor([[1.0, 0.0]], requires_grad=True)
        targets = Tensor([[1.0, 0.0]])
        
        loss = CrossEntropyLoss.apply(predictions, targets)
        loss.backward()
        
        assert predictions.grad is not None
        assert not np.isnan(predictions.grad).any()
        assert not np.isinf(predictions.grad).any()

class TestBinaryCrossEntropyLoss:
    """Tests for Binary Cross Entropy Loss"""
    
    def test_forward(self):
        """Test forward pass of binary cross entropy loss"""
        predictions = Tensor([0.7, 0.3])
        targets = Tensor([1.0, 0.0])
        
        loss = BinaryCrossEntropyLoss.apply(predictions, targets)
        assert loss.data >= 0  # Loss should be non-negative
        
    def test_gradient(self):
        """Test gradient computation"""
        predictions = Tensor([0.7], requires_grad=True)
        targets = Tensor([1.0])
        
        loss = BinaryCrossEntropyLoss.apply(predictions, targets)
        loss.backward()
        
        assert predictions.grad is not None
        assert not np.isnan(predictions.grad).any()
        
    def test_reductions(self):
        """Test different reduction methods"""
        predictions = Tensor([[0.7, 0.3], [0.2, 0.8]])
        targets = Tensor([[1.0, 0.0], [0.0, 1.0]])
        
        loss_none = BinaryCrossEntropyLoss.apply(predictions, targets, 'none')
        loss_mean = BinaryCrossEntropyLoss.apply(predictions, targets, 'mean')
        loss_sum = BinaryCrossEntropyLoss.apply(predictions, targets, 'sum')
        
        assert loss_none.shape == predictions.shape
        # Check if scalar by ensuring it's a 0-dimensional array or float
        assert loss_mean.data.ndim == 0 or isinstance(loss_mean.data, float)
        assert loss_sum.data.ndim == 0 or isinstance(loss_sum.data, float)

class TestL1Loss:
    """Tests for L1 Loss"""
    
    def test_forward(self):
        """Test forward pass of L1 loss"""
        predictions = Tensor([[1.0, 2.0], [3.0, 4.0]])
        targets = Tensor([[2.0, 1.0], [4.0, 3.0]])
        
        loss = L1Loss.apply(predictions, targets)
        expected = np.mean(np.abs(predictions.data - targets.data))
        assert np.allclose(loss.data, expected)
        
    def test_backward(self):
        """Test backward pass of L1 loss"""
        predictions = Tensor([1.0], requires_grad=True)
        targets = Tensor([2.0])
        
        loss = L1Loss.apply(predictions, targets)
        loss.backward()
        
        # Gradient should be sign(pred - target)
        expected_grad = np.sign(predictions.data - targets.data)
        assert np.allclose(predictions.grad, expected_grad)

class TestHuberLoss:
    """Tests for Huber Loss"""
    
    def test_forward(self):
        """Test forward pass of Huber loss"""
        predictions = Tensor([1.0, 2.0])
        targets = Tensor([0.0, 4.0])
        delta = 1.0
        
        loss = HuberLoss.apply(predictions, targets, delta)
        
        # Manually calculate expected loss
        diff = predictions.data - targets.data
        expected = np.mean(np.where(np.abs(diff) <= delta,
                                  0.5 * diff ** 2,
                                  delta * np.abs(diff) - 0.5 * delta ** 2))
        
        assert np.allclose(loss.data, expected)
        
    def test_backward(self):
        """Test backward pass of Huber loss"""
        predictions = Tensor([0.0], requires_grad=True)
        targets = Tensor([2.0])
        delta = 1.0
        
        loss = HuberLoss.apply(predictions, targets, delta)
        loss.backward()
        
        assert predictions.grad is not None
        assert not np.isnan(predictions.grad).any()

class TestKLDivLoss:
    """Tests for KL Divergence Loss"""
    
    def test_forward(self):
        """Test forward pass of KL divergence loss"""
        predictions = Tensor([[0.5, 0.5]])
        targets = Tensor([[0.8, 0.2]])
        
        loss = KLDivLoss.apply(predictions, targets)
        assert loss.data >= 0  # KL divergence is always non-negative
        
    def test_numerical_stability(self):
        """Test numerical stability with small probabilities"""
        predictions = Tensor([[0.999, 0.001]])
        targets = Tensor([[0.001, 0.999]])
        
        loss = KLDivLoss.apply(predictions, targets)
        assert not np.isnan(loss.data)
        assert not np.isinf(loss.data)

class TestCosineSimilarityLoss:
    """Tests for Cosine Similarity Loss"""
    
    def test_forward(self):
        """Test forward pass of cosine similarity loss"""
        x1 = Tensor([[1.0, 0.0]])
        x2 = Tensor([[0.0, 1.0]])
        
        loss = CosineSimilarityLoss.apply(x1, x2)
        # Orthogonal vectors should have cos_sim = 0, so loss = 1 - 0 = 1
        assert np.allclose(loss.data, 1.0), f"Expected 1.0, got {loss.data}"
        
    def test_identical_vectors(self):
        """Test with identical vectors"""
        x = Tensor([[1.0, 1.0]])
        loss = CosineSimilarityLoss.apply(x, x)
        # For identical vectors, cosine similarity is 1, so loss = 1 - 1 = 0
        assert np.allclose(loss.data, 0.0, atol=1e-7)
        
    def test_numerical_stability(self):
        """Test numerical stability with zero vectors"""
        x1 = Tensor([[0.0, 0.0]])
        x2 = Tensor([[1.0, 1.0]])
        
        loss = CosineSimilarityLoss.apply(x1, x2)
        assert not np.isnan(loss.data)

class TestHingeLoss:
    """Tests for Hinge Loss"""
    
    def test_forward(self):
        """Test forward pass of hinge loss"""
        predictions = Tensor([0.5, -0.5])
        targets = Tensor([1.0, 0.0])
        
        loss = HingeLoss.apply(predictions, targets)
        assert loss.data >= 0  # Hinge loss is non-negative
        
    def test_perfect_prediction(self):
        """Test with perfect predictions"""
        predictions = Tensor([1.0])
        targets = Tensor([1.0])
        
        loss = HingeLoss.apply(predictions, targets)
        assert np.allclose(loss.data, 0.0)  # Loss should be zero
        
    def test_margin(self):
        """Test different margin values"""
        predictions = Tensor([0.5])
        targets = Tensor([1.0])
        
        loss1 = HingeLoss.apply(predictions, targets, margin=1.0)
        loss2 = HingeLoss.apply(predictions, targets, margin=2.0)
        assert loss2.data > loss1.data  # Larger margin should give larger loss

class TestFocalLoss:
    """Tests for Focal Loss"""
    
    def test_forward(self):
        """Test forward pass of focal loss"""
        predictions = Tensor([0.7, 0.3])
        targets = Tensor([1.0, 0.0])
        
        loss = FocalLoss.apply(predictions, targets)
        assert loss.data >= 0  # Focal loss is non-negative
        
    def test_gamma_effect(self):
        """Test effect of gamma parameter"""
        predictions = Tensor([0.7])
        targets = Tensor([1.0])
        
        loss1 = FocalLoss.apply(predictions, targets, gamma=0.0)  # Equivalent to BCE
        loss2 = FocalLoss.apply(predictions, targets, gamma=2.0)  # Standard focal loss
        
        # Focal loss should be smaller than BCE for easy examples
        assert loss2.data < loss1.data
        
    def test_numerical_stability(self):
        """Test numerical stability with extreme probabilities"""
        predictions = Tensor([0.999, 0.001])
        targets = Tensor([1.0, 0.0])
        
        loss = FocalLoss.apply(predictions, targets)
        assert not np.isnan(loss.data)
        assert not np.isinf(loss.data)

class TestEdgeCases:
    """Tests for edge cases and error conditions"""
    
    def test_shape_mismatch(self):
        """Test shape mismatch handling"""
        predictions = Tensor([[1.0, 2.0]])
        targets = Tensor([1.0])
        
        with pytest.raises(ValueError):
            MSELoss.apply(predictions, targets)
            
    def test_invalid_reduction(self):
        """Test invalid reduction method"""
        predictions = Tensor([1.0])
        targets = Tensor([1.0])
        
        with pytest.raises(ValueError):
            MSELoss.apply(predictions, targets, reduction='invalid')
            
    def test_negative_probabilities(self):
        """Test handling of negative probabilities"""
        predictions = Tensor([-0.1, 1.1])
        targets = Tensor([0.0, 1.0])
        
        with pytest.raises(ValueError):
            BinaryCrossEntropyLoss.apply(predictions, targets)

// File: C:\Users\aluja\Desktop\DLpy\tests\test_modules.py
// ----------------------------------------
import pytest
from DLpy.nn.modules import Module
from DLpy.core import Tensor
import numpy as np
from DLpy.nn.linear import Linear  

class TestModuleEdgeCases:
    """Tests for edge cases in Module functionality"""
    
    def test_premature_parameter_registration(self):
        """Test parameter registration before initialization"""
        with pytest.raises(TypeError):
            class BadModule(Module):
                def __init__(self):
                    self.param = Tensor([1.0])  # Before super().__init__()
            BadModule()

    def test_invalid_module_addition(self):
        """Test adding invalid modules"""
        module = Module()
        
        # Test adding None module
        module.add_module('none_module', None)
        assert module._modules['none_module'] is None
        
        # Test adding invalid type
        with pytest.raises(TypeError):
            module.add_module('invalid', "not a module")
            
        # Test adding before initialization
        with pytest.raises(TypeError):
            class BadModule(Module):
                def __init__(self):
                    self.add_module('test', Module())  # Before super().__init__()
            BadModule()

    def test_attribute_access(self):
        """Test attribute access edge cases"""
        # Test accessing non-existent attribute
        module = Module()
        with pytest.raises(AttributeError):
            _ = module.nonexistent_attr
        
        # Test accessing training attribute before initialization
        class BadModule(Module):
            def __init__(self):
                # Access training before super().__init__()
                try:
                    _ = self._parameters
                except AttributeError:
                    pass  # Expected
                    
                # Now try to get the training attribute which should fail
                _ = self.training
                super().__init__()
                
        with pytest.raises(AttributeError):
            BadModule()

    def test_module_buffer_operations(self):
        """Test buffer operations in detail"""
        class TestModule(Module):
            def __init__(self):
                super().__init__()
                self.register_buffer('running_mean', Tensor([0.0]))
                self.register_buffer('running_var', None)
                
        module = TestModule()
        assert 'running_mean' in module._buffers
        assert module._buffers['running_var'] is None
        
        # Test buffer replacement
        module.register_buffer('running_mean', Tensor([1.0]))
        assert np.array_equal(module._buffers['running_mean'].data, [1.0])

    def test_module_state_dict(self):
        """Test state dict functionality"""
        class ComplexModule(Module):
            def __init__(self):
                super().__init__()
                self.linear = Linear(2, 2)
                self.register_buffer('running_stats', Tensor([0.0]))
                
        module = ComplexModule()
        # Test parameter access
        params = dict(module.named_parameters())
        assert 'linear.weight' in params
        assert 'linear.bias' in params

    def test_nested_module_training(self):
        """Test training mode propagation in nested modules"""
        class NestedModule(Module):
            def __init__(self):
                super().__init__()
                self.sub1 = Linear(2, 2)
                self.sub2 = Linear(2, 2)
                
        module = NestedModule()
        module.train(False)
        assert not module.training
        assert not module.sub1.training
        assert not module.sub2.training

// File: C:\Users\aluja\Desktop\DLpy\tests\test_nn.py
// ----------------------------------------
import pytest
import numpy as np
from DLpy.core import Tensor
from DLpy.nn.linear import Linear
from DLpy.nn.modules import Module


class TestLinearLayer:
    """Test suite for the Linear layer implementation."""
    
    def test_linear_layer_creation(self):
        """Test that linear layers are created with correct shapes and initialization."""
        in_features, out_features = 5, 3
        layer = Linear(in_features, out_features)
        
        # Test weight dimensions
        assert layer.weight.shape == (in_features, out_features)
        assert layer.weight.requires_grad
        
        # Test bias dimensions
        assert layer.bias is not None
        assert layer.bias.shape == (out_features,)
        assert layer.bias.requires_grad
        
        # Test layer without bias
        layer_no_bias = Linear(in_features, out_features, bias=False)
        assert layer_no_bias.bias is None
        
    def test_linear_forward(self):
        """Test the forward pass of the linear layer."""
        # Create a simple linear layer with known weights for testing
        layer = Linear(2, 3)
        layer.weight.data = np.array([[1., 2., 3.], [4., 5., 6.]])
        layer.bias.data = np.array([0.1, 0.2, 0.3])
        
        # Create input tensor
        x = Tensor([[1., 2.]])  # Batch size 1, 2 features
        
        # Compute expected output manually
        expected_output = np.array([[9.1, 12.2, 15.3]])  # (1×2) @ (2×3) + bias
        
        # Get actual output
        output = layer(x)
        
        # Compare results
        assert isinstance(output, Tensor)
        assert output.shape == (1, 3)
        assert np.allclose(output.data, expected_output)
        
    def test_linear_backward(self):
        """Test the backward pass and gradient computation of the linear layer."""
        # Create a layer with specific weights for testing
        layer = Linear(2, 1)
        layer.weight.data = np.array([[1.], [2.]])
        layer.bias.data = np.array([0.])
        
        # Forward pass
        x = Tensor([[1., 2.]], requires_grad=True)
        output = layer(x)
        
        # Backward pass
        output.backward(np.array([[1.]]))
        
        # Check input gradients
        expected_input_grad = np.array([[1., 2.]])  # Gradient w.r.t input
        assert np.allclose(x.grad, expected_input_grad)
        
        # Check weight gradients
        expected_weight_grad = np.array([[1.], [2.]])  # Gradient w.r.t weights
        assert np.allclose(layer.weight.grad, expected_weight_grad)
        
        # Check bias gradients
        expected_bias_grad = np.array([1.])  # Gradient w.r.t bias
        assert np.allclose(layer.bias.grad, expected_bias_grad)
        
    def test_linear_batch_processing(self):
        """Test that the linear layer correctly handles batched inputs."""
        layer = Linear(3, 2)
        batch_size = 4
        x = Tensor(np.random.randn(batch_size, 3))
        
        output = layer(x)
        assert output.shape == (batch_size, 2)
        
    def test_weight_initialization(self):
        """Test that weights are properly initialized using He initialization."""
        in_features, out_features = 100, 100
        layer = Linear(in_features, out_features)
        
        # Check if weights follow He initialization statistics
        weights = layer.weight.data
        mean = np.mean(weights)
        std = np.std(weights)
        
        # He initialization should have mean ≈ 0 and std ≈ sqrt(2/in_features)
        expected_std = np.sqrt(2.0 / in_features)
        assert abs(mean) < 0.1  # Mean should be close to 0
        assert abs(std - expected_std) < 0.1  # Std should be close to expected


class TestModule:
    """Test suite for the base Module class."""
    
    class SimpleModule(Module):
        """A simple module for testing purposes."""
        def __init__(self):
            super().__init__()
            self.linear1 = Linear(2, 3)
            self.linear2 = Linear(3, 1)
            self.register_buffer('running_mean', Tensor(np.zeros(3)))
            
        def forward(self, x):
            x = self.linear1(x)
            return self.linear2(x)
    
    def test_module_parameter_registration(self):
        """Test that parameters are correctly registered and tracked."""
        model = self.SimpleModule()
        
        # Count parameters
        params = list(model.parameters())
        assert len(params) == 4  # 2 weights + 2 biases
        
        # Check named parameters
        named_params = dict(model.named_parameters())
        assert 'linear1.weight' in named_params
        assert 'linear1.bias' in named_params
        assert 'linear2.weight' in named_params
        assert 'linear2.bias' in named_params
        
    def test_module_buffer_registration(self):
        """Test that buffers are correctly registered."""
        model = self.SimpleModule()
        assert 'running_mean' in model._buffers
        assert isinstance(model._buffers['running_mean'], Tensor)
        
    def test_module_train_eval_modes(self):
        """Test switching between training and evaluation modes."""
        model = self.SimpleModule()
        
        # Test train mode
        model.train()
        assert model.training
        assert model.linear1.training
        assert model.linear2.training
        
        # Test eval mode
        model.eval()
        assert not model.training
        assert not model.linear1.training
        assert not model.linear2.training
        
    def test_module_repr(self):
        """Test the string representation of modules."""
        model = self.SimpleModule()
        repr_str = repr(model)
        
        # Check that repr includes important information
        assert 'SimpleModule' in repr_str
        assert 'linear1' in repr_str
        assert 'linear2' in repr_str


class TestEndToEnd:
    """End-to-end tests for neural network components."""
    
    def test_simple_network(self):
        """Test a simple network with multiple layers."""
        # Create a simple network
        class SimpleNet(Module):
            def __init__(self):
                super().__init__()
                self.linear1 = Linear(2, 3)
                self.linear2 = Linear(3, 1)
                
            def forward(self, x):
                x = self.linear1(x)
                return self.linear2(x)
        
        # Create model and input
        model = SimpleNet()
        x = Tensor([[1., 2.]], requires_grad=True)
        
        # Forward pass
        output = model(x)
        assert output.shape == (1, 1)
        
        # Backward pass
        output.backward(np.array([[1.]]))
        
        # Check that all parameters have gradients
        for param in model.parameters():
            assert param.grad is not None

// File: C:\Users\aluja\Desktop\DLpy\tests\test_ops.py
// ----------------------------------------
import pytest
import numpy as np
from DLpy.core import Tensor
from DLpy.ops import (
    Add, Multiply, Power, Divide, Log, Exp, Sum, Mean, Max, Transpose,
    Greater, GreaterEqual, Less, LessEqual, Equal, NotEqual
)

class TestBasicOps:
    """Tests for basic arithmetic operations"""
    
    def test_add_edge_cases(self):
        """Test edge cases for Add operation"""
        # Test broadcasting
        x = Tensor([[1.0]], requires_grad=True)
        y = Tensor([1.0, 2.0], requires_grad=True)
        
        with pytest.raises(ValueError):
            _ = Add.apply(x, y)
        
        # Test gradient accumulation
        x = Tensor([1.0], requires_grad=True)
        y = Tensor([2.0], requires_grad=True)
        z = Add.apply(x, y)
        z.backward()
        
        assert np.array_equal(x.grad, [1.0])
        assert np.array_equal(y.grad, [1.0])

    def test_multiply_edge_cases(self):
        """Test edge cases for Multiply operation"""
        # Test scalar multiplication
        x = Tensor([1.0], requires_grad=True)
        y = Tensor(2.0, requires_grad=True)
        z = Multiply.apply(x, y)
        z.backward()
        
        assert np.array_equal(x.grad, [2.0])
        assert np.array_equal(y.grad, [1.0])
    
    def test_add_broadcasting_complex(self):
        """Test complex broadcasting scenarios in Add operation"""
        # Test broadcasting with different dimensions
        x = Tensor([[1.0]], requires_grad=True)
        y = Tensor([1.0, 2.0, 3.0], requires_grad=True)
        with pytest.raises(ValueError):
            _ = x + y  # Incompatible shapes
            
        # Test broadcasting with scalar
        x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
        y = Tensor(2.0, requires_grad=True)
        z = x + y
        z.backward(np.ones_like(x.data))
        assert np.sum(y.grad) == np.prod(x.shape)  # Sum of gradients equals number of elements

    def test_multiply_broadcasting_complex(self):
        """Test complex broadcasting scenarios in Multiply operation"""
        # Test scalar multiplication with matrix
        x = Tensor(2.0, requires_grad=True)
        y = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
        z = x * y
        z.backward(np.ones_like(y.data))

        # Check scalar gradient - should be sum of all elements in y
        assert x.grad.shape == (1,)
        assert np.allclose(x.grad, [10.0])  # sum([1,2,3,4])

        # Check matrix gradient - should be uniformly scaled by x
        assert y.grad.shape == y.data.shape
        assert np.allclose(y.grad, np.full_like(y.data, 2.0))

        # Test broadcasting matrix with different shapes
        a = Tensor([[1.0], [2.0]], requires_grad=True)  # Shape: (2,1)
        b = Tensor([[1.0, 2.0]], requires_grad=True)    # Shape: (1,2)
        c = a * b  # Should broadcast to shape (2,2)

        assert c.data.shape == (2, 2)
        expected = np.array([[1.0, 2.0], [2.0, 4.0]])
        assert np.allclose(c.data, expected)

        # Test gradient propagation with broadcasting
        c.backward(np.ones_like(c.data))
        assert a.grad.shape == (2, 1)
        assert b.grad.shape == (1, 2)
        # Correct expected gradients
        assert np.allclose(a.grad, np.array([[3.0], [3.0]]))  # Correct sum of gradients for each row
        assert np.allclose(b.grad, np.array([[3.0, 3.0]]))    # Correct sum of gradients for each column

    def test_add_empty_tensors(self):
        x = Tensor([], requires_grad=True)
        y = Tensor([], requires_grad=True)
        z = Add.apply(x, y)
        assert z.shape == (0,)
    
    def test_multiply_empty_tensors(self):
        x = Tensor([], requires_grad=True)
        y = Tensor([], requires_grad=True)
        z = Multiply.apply(x, y)
        assert z.shape == (0,)

class TestPowerOperations:
    """Tests for power and division operations"""
    
    def test_power_scalar(self):
        """Test power operation with scalar exponent"""
        x = Tensor([2.0, 3.0], requires_grad=True)
        y = x ** 2
        y.backward(np.array([1.0, 1.0]))
        
        assert np.allclose(y.data, [4.0, 9.0])
        assert np.allclose(x.grad, [4.0, 6.0])  # d/dx(x^2) = 2x
        
    def test_power_negative(self):
        """Test power operation with negative exponent"""
        x = Tensor([2.0, 4.0], requires_grad=True)
        y = x ** (-1)
        y.backward(np.array([1.0, 1.0]))
        
        assert np.allclose(y.data, [0.5, 0.25])
        assert np.allclose(x.grad, [-0.25, -0.0625])  # d/dx(x^-1) = -x^-2
        
    def test_division(self):
        """Test division operation"""
        x = Tensor([6.0, 8.0], requires_grad=True)
        y = Tensor([2.0, 4.0], requires_grad=True)
        z = x / y
        z.backward(np.array([1.0, 1.0]))
        
        assert np.allclose(z.data, [3.0, 2.0])
        assert np.allclose(x.grad, [0.5, 0.25])  # d/dx(x/y) = 1/y
        assert np.allclose(y.grad, [-1.5, -0.5])  # d/dy(x/y) = -x/y^2
        
    def test_division_by_zero(self):
        """Test division by zero raises error"""
        x = Tensor([1.0, 2.0])
        y = Tensor([1.0, 0.0])
        with pytest.raises(ValueError):
            _ = x / y

class TestElementwiseOperations:
    """Tests for element-wise operations"""
    
    def test_log(self):
        """Test natural logarithm"""
        x = Tensor([1.0, np.e], requires_grad=True)
        y = x.log()
        y.backward(np.array([1.0, 1.0]))
        
        assert np.allclose(y.data, [0.0, 1.0])
        assert np.allclose(x.grad, [1.0, 1/np.e])  # d/dx(log(x)) = 1/x
        
    def test_exp(self):
        """Test exponential function"""
        x = Tensor([0.0, 1.0], requires_grad=True)
        y = x.exp()
        y.backward(np.array([1.0, 1.0]))
        
        assert np.allclose(y.data, [1.0, np.e])
        assert np.allclose(y.data, x.grad)  # d/dx(exp(x)) = exp(x)

class TestReductionOperations:
    """Tests for reduction operations"""
    
    def test_sum(self):
        """Test sum reduction"""
        x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
        
        # Test sum all elements
        y1 = x.sum()
        y1.backward()
        assert np.allclose(y1.data, 10.0)
        assert np.allclose(x.grad, np.ones_like(x.data))
        
        # Reset gradients
        x.grad = None
        
        # Test sum along axis
        y2 = x.sum(axis=0)
        y2.backward(np.array([1.0, 1.0]))
        assert np.allclose(y2.data, [4.0, 6.0])
        assert np.allclose(x.grad, np.ones_like(x.data))
        
    def test_mean(self):
        """Test mean reduction"""
        x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
        y = x.mean()
        y.backward()
        
        assert np.allclose(y.data, 2.5)
        assert np.allclose(x.grad, np.full_like(x.data, 0.25))  # 1/n for each element
        
    def test_max(self):
        """Test max reduction"""
        x = Tensor([[1.0, 4.0], [3.0, 2.0]], requires_grad=True)
        y = x.max()
        y.backward()
        
        assert np.allclose(y.data, 4.0)
        expected_grad = np.array([[0.0, 1.0], [0.0, 0.0]])
        assert np.allclose(x.grad, expected_grad)

class TestMatrixOperations:
    """Tests for matrix operations"""
    
    def test_transpose(self):
        """Test matrix transpose"""
        x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
        y = x.t()
        y.backward(np.ones_like(y.data))
        
        assert np.allclose(y.data, [[1.0, 3.0], [2.0, 4.0]])
        assert np.allclose(x.grad, np.ones_like(x.data))
        
    def test_transpose_3d(self):
        """Test 3D tensor transpose"""
        x = Tensor(np.arange(8).reshape(2, 2, 2), requires_grad=True)
        y = x.transpose(1, 2, 0)
        y.backward(np.ones_like(y.data))
        
        assert y.data.shape == (2, 2, 2)
        assert np.allclose(x.grad, np.ones_like(x.data))

class TestComparisonOperations:
    """Tests for comparison operations"""
    
    def test_greater(self):
        """Test greater than operation"""
        x = Tensor([1.0, 2.0, 3.0])
        y = Tensor([2.0, 2.0, 2.0])
        result = x > y
        assert np.allclose(result.data, [False, False, True])
        
    def test_less_equal(self):
        """Test less than or equal operation"""
        x = Tensor([1.0, 2.0, 3.0])
        y = Tensor([2.0, 2.0, 2.0])
        result = x <= y
        assert np.allclose(result.data, [True, True, False])
        
    def test_equal(self):
        """Test equality operation"""
        x = Tensor([1.0, 2.0, 3.0])
        y = Tensor([1.0, 2.0, 2.0])
        result = x == y
        assert np.allclose(result.data, [True, True, False])
        
    def test_not_equal(self):
        """Test inequality operation"""
        x = Tensor([1.0, 2.0, 3.0])
        y = Tensor([1.0, 2.0, 2.0])
        result = x != y
        assert np.allclose(result.data, [False, False, True])

class TestEdgeCases:
    """Tests for edge cases and error conditions"""
    
    def test_log_negative(self):
        """Test log of negative number raises error"""
        x = Tensor([-1.0])
        with pytest.raises(ValueError):
            _ = x.log()
            
    def test_power_non_scalar(self):
        """Test power with non-scalar exponent raises error"""
        x = Tensor([2.0])
        y = Tensor([1.0, 2.0])
        with pytest.raises(ValueError):
            _ = x ** y
            
    def test_reduction_keepdims(self):
        """Test reduction operations with keepdims=True"""
        x = Tensor([[1.0, 2.0], [3.0, 4.0]])
        y = x.sum(axis=0, keepdims=True)
        assert y.shape == (1, 2)
        
    def test_broadcasting_division(self):
        """Test division with broadcasting"""
        x = Tensor([[1.0, 2.0], [3.0, 4.0]])
        y = Tensor([2.0])
        z = x / y
        assert z.shape == x.shape
        assert np.allclose(z.data, [[0.5, 1.0], [1.5, 2.0]])

// File: C:\Users\aluja\Desktop\DLpy\tests\test_optim.py
// ----------------------------------------
import pytest
import numpy as np
from DLpy.core import Tensor
from DLpy.optim import SGD, Adam, RMSprop, AdaGrad

class TestOptimizers:
    """Base test class for all optimizers."""
    
    def setup_method(self):
        """Setup method run before each test."""
        # Create a simple parameter tensor
        self.param = Tensor([1.0, 2.0, 3.0], requires_grad=True)
        self.grad = np.array([0.1, 0.2, 0.3])
        
    def _test_basic_update(self, optimizer_class, **kwargs):
        """Helper method to test basic parameter updates."""
        optimizer = optimizer_class([self.param], **kwargs)
        
        # Initial parameter values
        initial_params = self.param.data.copy()
        
        # Set gradient and perform optimization step
        self.param.grad = self.grad
        optimizer.step()
        
        # Check that parameters were updated
        assert not np.array_equal(self.param.data, initial_params)
        
    def _test_zero_grad(self, optimizer_class, **kwargs):
        """Helper method to test zero_grad functionality."""
        optimizer = optimizer_class([self.param], **kwargs)
        
        # Set some gradient
        self.param.grad = self.grad
        
        # Zero out gradients
        optimizer.zero_grad()
        
        # Check that gradients are zeroed
        assert np.all(self.param.grad == 0)

class TestSGD(TestOptimizers):
    """Tests for SGD optimizer."""
    
    def test_basic_sgd(self):
        """Test basic SGD functionality."""
        self._test_basic_update(SGD, lr=0.1)
        
    def test_sgd_momentum(self):
        """Test SGD with momentum."""
        optimizer = SGD([self.param], lr=0.1, momentum=0.9)
        
        # First update
        self.param.grad = self.grad
        optimizer.step()
        first_update = self.param.data.copy()
        
        # Second update with same gradient
        optimizer.step()
        
        # With momentum, second update should be larger
        first_step = np.abs(first_update - np.array([1.0, 2.0, 3.0]))
        second_step = np.abs(self.param.data - first_update)
        assert np.all(second_step > first_step)
        
    def test_sgd_nesterov(self):
        """Test SGD with Nesterov momentum."""
        self._test_basic_update(SGD, lr=0.1, momentum=0.9, nesterov=True)
        
    def test_sgd_weight_decay(self):
        """Test SGD with weight decay."""
        optimizer = SGD([self.param], lr=0.1, weight_decay=0.1)
        self.param.grad = self.grad
        optimizer.step()
        
        # Parameters should decrease more with weight decay
        no_decay_param = Tensor([1.0, 2.0, 3.0], requires_grad=True)
        no_decay_optimizer = SGD([no_decay_param], lr=0.1)
        no_decay_param.grad = self.grad
        no_decay_optimizer.step()
        
        assert np.all(np.abs(self.param.data) < np.abs(no_decay_param.data))

class TestAdam(TestOptimizers):
    """Tests for Adam optimizer."""
    
    def test_basic_adam(self):
        """Test basic Adam functionality."""
        self._test_basic_update(Adam, lr=0.001)
        
    def test_adam_bias_correction(self):
        """Test Adam bias correction."""
        optimizer = Adam([self.param], lr=0.001)
        
        initial_param = self.param.data.copy()
        updates = []
        
        # Perform several updates and track parameter changes
        for _ in range(6):
            self.param.grad = self.grad  # Keep constant gradient for testing
            optimizer.step()
            updates.append(np.linalg.norm(self.param.data - initial_param))
        
        # Test that:
        # 1. Parameters are actually being updated
        assert not np.array_equal(self.param.data, initial_param)
        
        # 2. Updates are being affected by bias correction
        # (not necessarily smaller, but should be different)
        assert len(set(updates)) > 1  # Updates should not all be identical
        
        # 3. State contains expected bias correction terms
        assert 'step' in optimizer.state[id(self.param)]
        assert 'exp_avg' in optimizer.state[id(self.param)]
        assert 'exp_avg_sq' in optimizer.state[id(self.param)]
        
    def test_adam_amsgrad(self):
        """Test Adam with AMSGrad."""
        self._test_basic_update(Adam, lr=0.001, amsgrad=True)

class TestRMSprop(TestOptimizers):
    """Tests for RMSprop optimizer."""
    
    def test_basic_rmsprop(self):
        """Test basic RMSprop functionality."""
        self._test_basic_update(RMSprop, lr=0.01)
        
    def test_rmsprop_momentum(self):
        """Test RMSprop with momentum."""
        self._test_basic_update(RMSprop, lr=0.01, momentum=0.9)
        
    def test_rmsprop_centered(self):
        """Test centered RMSprop."""
        self._test_basic_update(RMSprop, lr=0.01, centered=True)

class TestAdaGrad(TestOptimizers):
    """Tests for AdaGrad optimizer."""
    
    def test_basic_adagrad(self):
        """Test basic AdaGrad functionality."""
        self._test_basic_update(AdaGrad, lr=0.01)
        
    def test_adagrad_lr_decay(self):
        """Test AdaGrad learning rate decay."""
        optimizer = AdaGrad([self.param], lr=0.01, lr_decay=0.1)
        
        initial_param = self.param.data.copy()
        effective_lrs = []
        
        # Perform several updates and track effective learning rates
        for _ in range(6):
            self.param.grad = self.grad  # Keep constant gradient for testing
            prev_param = self.param.data.copy()
            optimizer.step()
            
            # Calculate effective learning rate from parameter update
            param_update = np.linalg.norm(self.param.data - prev_param)
            grad_norm = np.linalg.norm(self.grad)
            effective_lrs.append(param_update / grad_norm if grad_norm != 0 else 0)
        
        # Test that:
        # 1. Parameters are actually being updated
        assert not np.array_equal(self.param.data, initial_param)
        
        # 2. Accumulated sum in state is increasing
        assert np.all(optimizer.state[id(self.param)]['sum'] > 0)
        
        # 3. Effective learning rates should show some variation
        assert len(set(map(lambda x: round(x, 6), effective_lrs))) > 1
        
    def test_adagrad_reset(self):
        """Test AdaGrad state reset."""
        optimizer = AdaGrad([self.param], lr=0.01)
        
        # Perform some updates
        self.param.grad = self.grad
        optimizer.step()
        
        # Reset state
        optimizer.reset_state()
        
        # Check that state was reset
        for state in optimizer.state.values():
            assert state['step'] == 0
            assert np.all(state['sum'] == 0)

class TestOptimizerEdgeCases:
    """Tests for optimizer edge cases and error conditions."""
    
    def test_invalid_learning_rates(self):
        """Test that invalid learning rates raise errors."""
        param = Tensor([1.0], requires_grad=True)
        
        with pytest.raises(ValueError):
            SGD([param], lr=-0.1)
        with pytest.raises(ValueError):
            Adam([param], lr=-0.1)
        with pytest.raises(ValueError):
            RMSprop([param], lr=-0.1)
        with pytest.raises(ValueError):
            AdaGrad([param], lr=-0.1)
            
    def test_no_gradients(self):
        """Test optimizer behavior with no gradients."""
        param = Tensor([1.0], requires_grad=True)
        optimizers = [
            SGD([param], lr=0.1),
            Adam([param], lr=0.001),
            RMSprop([param], lr=0.01),
            AdaGrad([param], lr=0.01)
        ]
        
        # Parameter should not change if there's no gradient
        for optimizer in optimizers:
            initial_param = param.data.copy()
            optimizer.step()
            assert np.array_equal(param.data, initial_param)
            
    def test_param_groups(self):
        """Test adding parameter groups."""
        param1 = Tensor([1.0], requires_grad=True)
        param2 = Tensor([2.0], requires_grad=True)
        
        optimizer = SGD([param1], lr=0.1)
        optimizer.add_param_group({'params': [param2]})
        
        # Both parameters should be updated
        param1.grad = np.array([0.1])
        param2.grad = np.array([0.2])
        optimizer.step()
        
        assert not np.array_equal(param1.data, [1.0])
        assert not np.array_equal(param2.data, [2.0])

// File: C:\Users\aluja\Desktop\DLpy\tests\test_tensor.py
// ----------------------------------------
import pytest
import numpy as np
from DLpy.core import Tensor

class TestReshapeOp:
    """Tests for Reshape operation"""

    def test_reshape_edge_cases(self):
        """Test edge cases for Reshape operation"""
        x = Tensor([1.0, 2.0], requires_grad=True)
        
        # Test invalid shape
        with pytest.raises(ValueError):
            _ = x.reshape(3)  # Invalid shape
        
        # Test gradients with different shapes
        y = x.reshape(2, 1)
        y.backward(np.array([[1.0], [1.0]]))
        assert np.array_equal(x.grad, [1.0, 1.0])

class TestAdvancedOperations:
    """Additional tests for basic operations"""
    
    def test_broadcasting_edge_cases(self):
        """Test broadcasting with different dimensions"""
        # Test broadcasting scalar to matrix
        x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
        y = Tensor(2.0, requires_grad=True)
        z = x * y
        z.backward(np.ones_like(x.data))
        assert y.grad.shape == (1,)
        assert np.array_equal(x.grad, [[2.0, 2.0], [2.0, 2.0]])
        
        # Test broadcasting vector to matrix
        x = Tensor([[1.0], [2.0]], requires_grad=True)
        y = Tensor([1.0, 2.0], requires_grad=True)
        z = x + y  # Should broadcast to [[1,2], [2,3]]
        z.backward(np.ones((2, 2)))
        assert x.grad.shape == (2, 1)
        assert y.grad.shape == (2,)
        
    def test_zero_gradient_handling(self):
        """Test operations with zero gradients"""
        x = Tensor([1.0, 2.0], requires_grad=True)
        y = Tensor([3.0, 4.0], requires_grad=True)
        z = x * y
        z.backward(np.zeros_like(z.data))
        assert np.all(x.grad == 0)
        assert np.all(y.grad == 0)
        
    def test_non_differentiable_inputs(self):
        """Test operations with non-differentiable inputs"""
        x = Tensor([1.0, 2.0], requires_grad=False)
        y = Tensor([3.0, 4.0], requires_grad=True)
        z = x * y
        z.backward(np.ones_like(z.data))
        assert x.grad is None  # Non-differentiable input should have no gradient
        assert np.array_equal(y.grad, [1.0, 2.0])

    def test_tensor_creation_edge_cases(self):
        """Test edge cases in tensor creation"""
        # Test with different dtypes
        t1 = Tensor([1, 2, 3], dtype=np.int32)
        assert t1.dtype == np.int32
        
        # Test with nested lists
        t2 = Tensor([[1, 2], [3, 4]])
        assert t2.shape == (2, 2)
        
        # Test with another tensor
        t3 = Tensor(t2)
        assert np.array_equal(t3.data, t2.data)

    def test_backward_edge_cases(self):
        """Test edge cases in backward pass"""
        # Test backward with scalar tensor
        x = Tensor(2.0, requires_grad=True)
        y = x * 2
        y.backward(np.array(3.0))
        assert x.grad is not None
        
        # Test backward with non-scalar tensor without gradient
        x = Tensor([1.0, 2.0, 3.0], requires_grad=True)
        y = x * 2
        with pytest.raises(RuntimeError):
            y.backward()  # Should raise error for non-scalar

    def test_repr_and_str(self):
        """Test string representations"""
        t = Tensor([1.0, 2.0], requires_grad=True)
        assert 'Tensor' in repr(t)
        assert 'requires_grad=True' in repr(t)

// ----------------------------------------
// Total Python files found: 43
