// Python Files Concatenated on 01/22/2025 16:56:03
// ----------------------------------------



// File: C:\Users\aluja\Desktop\DLpy\DLpy\__init__.py
// ----------------------------------------
0001: """
0002: DLpy: A Deep Learning Library with DAG-based Autograd
0003: 
0004: This library provides a PyTorch-like interface for building and training neural networks,
0005: with a focus on clear implementation and educational value.
0006: """
0007: 
0008: from .core import Tensor, Function, Context
0009: from .ops import Add, Multiply, Reshape
0010: 
0011: __version__ = "0.1.0"
0012: 
0013: __all__ = [
0014:     'Tensor',
0015:     'Function',
0016:     'Context',
0017:     'Add',
0018:     'Multiply',
0019:     'Reshape',
0020: ]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\__init__.py
// ----------------------------------------
0001: """
0002: Core functionality for DLpy.
0003: 
0004: This module contains the fundamental building blocks of the deep learning library.
0005: """
0006: 
0007: from .tensor import Tensor
0008: from .function import Function
0009: from .context import Context
0010: from .module import Module
0011: from .autograd import AutogradEngine, get_autograd_engine
0012: 
0013: __all__ = ['Tensor', 'Function', 'Context', 'Module', 'AutogradEngine', 'get_autograd_engine']

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\autograd.py
// ----------------------------------------
0001: from typing import Dict, Set, List, Optional, Tuple, Union
0002: import numpy as np
0003: from collections import defaultdict
0004: import warnings
0005: 
0006: class Edge:
0007:     """
0008:     Represents a directed edge in the computational graph.
0009:     
0010:     Each edge connects a source node (input tensor) to a destination node
0011:     (output tensor) and stores gradient information for that connection.
0012:     """
0013:     
0014:     def __init__(self, src: 'Node', dst: 'Node'):
0015:         self.src = src
0016:         self.dst = dst
0017:         self.grad: Optional[np.ndarray] = None
0018:         
0019: class Node:
0020:     """
0021:     Represents a node in the computational graph.
0022:     
0023:     Each node corresponds to an operation in the computation and maintains
0024:     connections to its inputs and outputs through edges.
0025:     """
0026:     
0027:     def __init__(self, tensor: 'Tensor'):
0028:         self.tensor = tensor
0029:         self.in_edges: List[Edge] = []
0030:         self.out_edges: List[Edge] = []
0031:         self._backward_fn = tensor._backward_fn
0032: 
0033: class AutogradEngine:
0034:     """
0035:     Engine for managing automatic differentiation computations.
0036:     
0037:     This class handles the creation and execution of the computational graph,
0038:     manages gradient computation and accumulation, and provides utilities for
0039:     graph manipulation and visualization.
0040:     """
0041:     
0042:     def __init__(self):
0043:         self._nodes: Dict[int, Node] = {}
0044:         self._edges: Set[Edge] = set()
0045:         self._currently_computing_gradients = False
0046:         
0047:     def register_tensor(self, tensor: 'Tensor') -> None:
0048:         """
0049:         Registers a tensor with the autograd engine.
0050:         
0051:         Args:
0052:             tensor: Tensor to register
0053:         """
0054:         if id(tensor) not in self._nodes:
0055:             self._nodes[id(tensor)] = Node(tensor)
0056:             
0057:     def add_edge(self, src: 'Tensor', dst: 'Tensor') -> None:
0058:         """
0059:         Adds a directed edge between two tensors in the computational graph.
0060:         
0061:         Args:
0062:             src: Source tensor
0063:             dst: Destination tensor
0064:         """
0065:         src_node = self._nodes[id(src)]
0066:         dst_node = self._nodes[id(dst)]
0067:         
0068:         edge = Edge(src_node, dst_node)
0069:         src_node.out_edges.append(edge)
0070:         dst_node.in_edges.append(edge)
0071:         self._edges.add(edge)
0072:         
0073:     def backward(self, tensor: 'Tensor', gradient: Optional[np.ndarray] = None) -> None:
0074:         """Executes backward pass starting from the given tensor."""
0075:         if self._currently_computing_gradients:
0076:             raise RuntimeError("Nested gradient computation detected")
0077:             
0078:         self._currently_computing_gradients = True
0079:         try:
0080:             # Initialize grad_dict as a regular dictionary
0081:             grad_dict: Dict[int, np.ndarray] = {}
0082:             
0083:             # If no gradient is provided, assume it's 1 (for scalar outputs)
0084:             if gradient is None:
0085:                 if tensor.data.shape == ():
0086:                     grad_dict[id(tensor)] = np.array(1.0)
0087:                 else:
0088:                     grad_dict[id(tensor)] = np.ones_like(tensor.data)
0089:             else:
0090:                 grad_dict[id(tensor)] = gradient
0091:             
0092:             # Perform topological sort
0093:             sorted_nodes = self._topological_sort(tensor)
0094:             
0095:             # Traverse nodes in reverse topological order
0096:             for node in reversed(sorted_nodes):
0097:                 node_id = id(node.tensor)
0098:                 if node_id not in grad_dict or not node.tensor.requires_grad:
0099:                     continue  # No gradient to propagate
0100:                 
0101:                 current_grad = grad_dict[node_id]
0102:                 
0103:                 if node.tensor._backward_fn is not None:
0104:                     node.tensor._backward_fn(current_grad, grad_dict)
0105:                 
0106:                 # Accumulate gradients for leaf nodes
0107:                 if len(node.in_edges) == 0 and node.tensor.requires_grad:
0108:                     if node.tensor.grad is None:
0109:                         node.tensor.grad = current_grad
0110:                     else:
0111:                         try:
0112:                             node.tensor.grad += current_grad
0113:                         except ValueError:
0114:                             # If shapes don't match, reshape current_grad
0115:                             node.tensor.grad += current_grad.reshape(node.tensor.grad.shape)
0116:         finally:
0117:             self._currently_computing_gradients = False
0118: 
0119:     def _topological_sort(self, start_tensor: 'Tensor') -> List[Node]:
0120:         """
0121:         Performs topological sort on the computation graph.
0122:         
0123:         Args:
0124:             start_tensor: Tensor to start the sort from
0125:             
0126:         Returns:
0127:             List of nodes in topological order
0128:             
0129:         Raises:
0130:             RuntimeError: If graph contains cycles
0131:         """
0132:         result: List[Node] = []
0133:         visited: Set[Node] = set()
0134:         temp_visited: Set[Node] = set()
0135:         
0136:         def visit(node: Node) -> None:
0137:             if node in temp_visited:
0138:                 raise RuntimeError("Cycle detected in computation graph")
0139:                 
0140:             if node not in visited:
0141:                 temp_visited.add(node)
0142:                 for edge in node.in_edges:
0143:                     visit(edge.src)
0144:                 temp_visited.remove(node)
0145:                 visited.add(node)
0146:                 result.append(node)
0147:                 
0148:         visit(self._nodes[id(start_tensor)])
0149:         return result
0150:             
0151:     def clear(self) -> None:
0152:         """Clears the computational graph."""
0153:         self._nodes.clear()
0154:         self._edges.clear()
0155:     
0156:     def validate_graph(self) -> List[str]:
0157:         """
0158:         Validates the computational graph structure.
0159:         """
0160:         warnings: List[str] = []
0161:         
0162:         # If no nodes in graph
0163:         if not self._nodes:
0164:             return warnings
0165: 
0166:         # Step 1: Find all nodes that are part of computations
0167:         active_nodes = set()
0168:         output_nodes = []
0169:         for node in self._nodes.values():
0170:             if not node.out_edges:  # Output node
0171:                 output_nodes.append(node)
0172:             if node.in_edges or node.out_edges:  # Node is part of a computation
0173:                 active_nodes.add(node)
0174: 
0175:         # Step 2: Find all connected nodes starting from outputs
0176:         connected_nodes = set()
0177:         for output_node in output_nodes:
0178:             stack = [output_node]
0179:             while stack:
0180:                 curr = stack.pop()
0181:                 connected_nodes.add(curr)
0182:                 for edge in curr.in_edges:
0183:                     if edge.src not in connected_nodes:
0184:                         stack.append(edge.src)
0185:                         
0186:         # Step 3: Find nodes not connected to outputs
0187:         all_nodes = set(self._nodes.values())
0188:         unconnected_nodes = all_nodes - connected_nodes
0189: 
0190:         # Step 4: Find completely isolated nodes
0191:         isolated_nodes = all_nodes - active_nodes
0192: 
0193:         # Add appropriate warnings
0194:         if unconnected_nodes:
0195:             warnings.append(f"Found {len(unconnected_nodes)} nodes not connected to any output")
0196:             
0197:         if isolated_nodes:
0198:             warnings.append(f"Found {len(isolated_nodes)} isolated nodes")
0199:             
0200:         # Check gradient shapes
0201:         for edge in self._edges:
0202:             if edge.grad is not None:
0203:                 src_shape = edge.src.tensor.shape
0204:                 grad_shape = edge.grad.shape
0205:                 if src_shape != grad_shape:
0206:                     warnings.append(
0207:                         f"Gradient shape mismatch: grad shape {grad_shape} vs tensor shape {src_shape}"
0208:                     )
0209:                     
0210:         return warnings
0211: 
0212: 
0213: # Global autograd engine instance
0214: _autograd_engine = AutogradEngine()
0215: 
0216: def get_autograd_engine() -> AutogradEngine:
0217:     """Returns the global autograd engine instance."""
0218:     return _autograd_engine

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\context.py
// ----------------------------------------
0001: from typing import Any, Dict, List, Tuple
0002: from dataclasses import dataclass, field
0003: 
0004: @dataclass
0005: class Context:
0006:     """
0007:     Context class for storing information needed during the backward pass.
0008:     
0009:     The Context class serves as a storage mechanism for tensors and metadata that are 
0010:     needed during backpropagation. It's passed to both forward and backward functions
0011:     to maintain state between the two passes.
0012:     
0013:     Attributes:
0014:         _saved_tensors: List of tensors saved during forward pass for use in backward pass
0015:         _non_tensor_args: Dictionary of additional arguments needed for backward pass
0016:         _intermediate_values: Dictionary storing intermediate computations
0017:     """
0018:     
0019:     _saved_tensors: List[Any] = field(default_factory=list)
0020:     _non_tensor_args: Dict[str, Any] = field(default_factory=dict)
0021:     _intermediate_values: Dict[str, Any] = field(default_factory=dict)
0022: 
0023:     def save_for_backward(self, *args: Any) -> None:
0024:         """
0025:         Saves tensors that will be needed for the backward pass.
0026:         
0027:         Args:
0028:             *args: Variable number of tensors to save
0029:         """
0030:         self._saved_tensors = list(args)
0031: 
0032:     def save_arguments(self, **kwargs: Any) -> None:
0033:         """
0034:         Saves additional arguments that will be needed for the backward pass.
0035:         
0036:         Args:
0037:             **kwargs: Keyword arguments to save
0038:         """
0039:         self._non_tensor_args.update(kwargs)
0040:         
0041:     def store_intermediate(self, name: str, value: Any) -> None:
0042:         """
0043:         Stores intermediate values computed during forward pass that may be
0044:         useful during backward pass or for debugging.
0045:         
0046:         Args:
0047:             name: Identifier for the intermediate value
0048:             value: The value to store
0049:         """
0050:         self._intermediate_values[name] = value
0051: 
0052:     @property
0053:     def saved_tensors(self) -> Tuple[Any, ...]:
0054:         """Returns the saved tensors as a tuple."""
0055:         return tuple(self._saved_tensors)
0056: 
0057:     @property
0058:     def saved_arguments(self) -> Dict[str, Any]:
0059:         """Returns the saved non-tensor arguments."""
0060:         return self._non_tensor_args.copy()
0061:         
0062:     def get_intermediate(self, name: str) -> Any:
0063:         """
0064:         Retrieves a stored intermediate value.
0065:         
0066:         Args:
0067:             name: Identifier for the intermediate value
0068:             
0069:         Returns:
0070:             The stored value
0071:             
0072:         Raises:
0073:             KeyError: If no value exists for the given name
0074:         """
0075:         return self._intermediate_values[name]
0076: 
0077:     def clear(self) -> None:
0078:         """Clears all saved data from the context."""
0079:         self._saved_tensors.clear()
0080:         self._non_tensor_args.clear()
0081:         self._intermediate_values.clear()

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\function.py
// ----------------------------------------
0001: from abc import ABC, abstractmethod
0002: from typing import Any, Tuple, Optional, Dict
0003: import numpy as np
0004: 
0005: from .context import Context
0006: from .tensor import Tensor  # This will be implemented next
0007: 
0008: class Function(ABC):
0009:     """
0010:     Base class for all autograd operations.
0011:     
0012:     This class defines the interface for creating differentiable operations.
0013:     Each operation should implement both a forward pass (computing the result)
0014:     and a backward pass (computing gradients).
0015:     
0016:     The Function class follows a similar design pattern to PyTorch's autograd.Function,
0017:     but with some simplifications and additional features for clarity and debugging.
0018:     """
0019:     
0020:     requires_grad: bool = True
0021:     
0022:     @staticmethod
0023:     @abstractmethod
0024:     def forward(ctx: Context, *args: Any, **kwargs: Any) -> Tensor:
0025:         """
0026:         Performs the forward computation.
0027:         
0028:         Args:
0029:             ctx: Context object for saving information needed in backward pass
0030:             *args: Input tensors and other arguments
0031:             **kwargs: Additional keyword arguments for the operation
0032:             
0033:         Returns:
0034:             Result of the computation as a Tensor
0035:         """
0036:         raise NotImplementedError
0037:         
0038:     @staticmethod
0039:     @abstractmethod
0040:     def backward(ctx: Context, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0041:         """
0042:         Computes gradients of the operation with respect to its inputs.
0043:         
0044:         Args:
0045:             ctx: Context object containing saved tensors from forward pass
0046:             grad_output: Gradient of the loss with respect to the output
0047:             grad_dict: Dictionary mapping tensor IDs to their gradients
0048:         """
0049:         raise NotImplementedError
0050:         
0051:     @classmethod
0052:     def apply(cls, *args: Any, **kwargs: Any) -> Tensor:
0053:         """
0054:         Applies the function to the given inputs.
0055:         
0056:         This method:
0057:         1. Creates a Context object for storing intermediate values
0058:         2. Runs the forward pass
0059:         3. Sets up the computational graph for gradient computation
0060:         4. Returns the result
0061:         """
0062:         ctx = Context()
0063:         result = cls.forward(ctx, *args, **kwargs)
0064:         
0065:         # Check if we need to compute gradients
0066:         needs_grad = cls.requires_grad and any(
0067:             isinstance(arg, Tensor) and arg.requires_grad 
0068:             for arg in args
0069:         )
0070:         
0071:         if needs_grad:
0072:             def backward_fn(grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0073:                 cls.backward(ctx, grad_output, grad_dict)
0074:             
0075:             result._backward_fn = backward_fn
0076:             result.requires_grad_(True)
0077:             
0078:             # Get autograd engine and register edges
0079:             from .autograd import get_autograd_engine
0080:             engine = get_autograd_engine()
0081:             for arg in args:
0082:                 if isinstance(arg, Tensor):
0083:                     engine.add_edge(arg, result)
0084:         
0085:         return result  # Return result in all cases
0086:         
0087:         
0088:     @staticmethod
0089:     def verify_backward(
0090:         forward_fn: Any,
0091:         backward_fn: Any,
0092:         inputs: Tuple[np.ndarray, ...],
0093:         epsilon: float = 1e-6
0094:     ) -> bool:
0095:         """
0096:         Verifies backward pass implementation using numerical gradients.
0097:         
0098:         This helper method compares analytically computed gradients with
0099:         numerically computed gradients to check for correctness.
0100:         
0101:         Args:
0102:             forward_fn: The forward pass function
0103:             backward_fn: The backward pass function
0104:             inputs: Tuple of input arrays
0105:             epsilon: Small value for numerical gradient computation
0106:             
0107:         Returns:
0108:             True if gradients match within tolerance, False otherwise
0109:         """
0110:         def compute_numerical_gradient(idx: int, inp: np.ndarray) -> np.ndarray:
0111:             grad = np.zeros_like(inp)
0112:             it = np.nditer(inp, flags=['multi_index'])
0113:             
0114:             while not it.finished:
0115:                 ix = it.multi_index
0116:                 old_value = inp[ix]
0117:                 
0118:                 # Compute f(x + epsilon)
0119:                 inp[ix] = old_value + epsilon
0120:                 pos_inputs = list(inputs)
0121:                 pos_inputs[idx] = inp.copy()
0122:                 pos_output = forward_fn(*pos_inputs)
0123:                 
0124:                 # Compute f(x - epsilon)
0125:                 inp[ix] = old_value - epsilon
0126:                 neg_inputs = list(inputs)
0127:                 neg_inputs[idx] = inp.copy()
0128:                 neg_output = forward_fn(*neg_inputs)
0129:                 
0130:                 # Restore original value
0131:                 inp[ix] = old_value
0132:                 
0133:                 # Compute numerical gradient
0134:                 grad[ix] = np.sum(pos_output - neg_output) / (2 * epsilon)
0135:                 it.iternext()
0136:                 
0137:             return grad
0138:             
0139:         # Compute analytical gradients
0140:         ctx = Context()
0141:         output = forward_fn(*inputs)
0142:         grad_output = np.ones_like(output)
0143:         analytical_grads = backward_fn(ctx, grad_output)
0144:         
0145:         # Compute numerical gradients
0146:         numerical_grads = tuple(
0147:             compute_numerical_gradient(i, inp.copy()) 
0148:             for i, inp in enumerate(inputs)
0149:         )
0150:         
0151:         # Compare gradients
0152:         for analytical, numerical in zip(analytical_grads, numerical_grads):
0153:             if analytical is not None:
0154:                 rel_error = np.max(
0155:                     np.abs(analytical - numerical) /
0156:                     (np.maximum(np.abs(analytical), np.abs(numerical)) + epsilon)
0157:                 )
0158:                 if rel_error > 1e-5:
0159:                     return False
0160:                     
0161:         return True

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\module.py
// ----------------------------------------
0001: from typing import Iterator, Dict, Any, Optional
0002: import numpy as np
0003: from collections import OrderedDict
0004: from ..core import Tensor
0005: 
0006: class Module:
0007:     """
0008:     Base class for all neural network modules.
0009:     
0010:     Your models should also subclass this class.
0011:     Modules can also contain other Modules, allowing to nest them in
0012:     a tree structure.
0013:     """
0014:     
0015:     def __init__(self):
0016:         """Initialize the module."""
0017:         # First set these directly to avoid triggering __setattr__
0018:         object.__setattr__(self, 'training', True)
0019:         object.__setattr__(self, '_parameters', OrderedDict())
0020:         object.__setattr__(self, '_buffers', OrderedDict())
0021:         object.__setattr__(self, '_modules', OrderedDict())
0022:         
0023:     def register_parameter(self, name: str, param: Optional[Tensor]) -> None:
0024:         """Add a parameter to the module.
0025:         
0026:         Args:
0027:             name: Name of the parameter
0028:             param: The parameter tensor to register
0029:         """
0030:         if '_parameters' not in self.__dict__:
0031:             raise TypeError(
0032:                 "cannot assign parameter before Module.__init__() call"
0033:             )
0034:             
0035:         if param is not None and not isinstance(param, Tensor):
0036:             raise TypeError(f"Parameter {name} must be a Tensor, not {type(param)}")
0037:             
0038:         self._parameters[name] = param
0039:         
0040:     def register_buffer(self, name: str, tensor: Optional[Tensor]) -> None:
0041:         """Add a persistent buffer to the module.
0042:         
0043:         Buffers are typically used for running statistics in modules like BatchNorm.
0044:         
0045:         Args:
0046:             name: Name of the buffer
0047:             tensor: The tensor to register as a buffer
0048:         """
0049:         if '_buffers' not in self.__dict__:
0050:             raise TypeError(
0051:                 "cannot assign buffer before Module.__init__() call"
0052:             )
0053:             
0054:         if tensor is not None and not isinstance(tensor, Tensor):
0055:             raise TypeError(f"Buffer {name} must be a Tensor, not {type(tensor)}")
0056:             
0057:         self._buffers[name] = tensor
0058:         
0059:     def add_module(self, name: str, module: Optional['Module']) -> None:
0060:         """Add a child module to the current module.
0061:         
0062:         Args:
0063:             name: Name of the child module
0064:             module: The module to add
0065:         """
0066:         if not isinstance(module, (Module, type(None))):
0067:             raise TypeError(f"{name} is not a Module subclass")
0068:             
0069:         if '_modules' not in self.__dict__:
0070:             raise TypeError(
0071:                 "cannot assign module before Module.__init__() call"
0072:             )
0073:             
0074:         self._modules[name] = module
0075:         
0076:     def __getattr__(self, name: str) -> Any:
0077:         """Custom getattr that looks through parameters, buffers, and modules."""
0078:         if '_parameters' in self.__dict__:
0079:             _parameters = self.__dict__['_parameters']
0080:             if name in _parameters:
0081:                 return _parameters[name]
0082:                 
0083:         if '_buffers' in self.__dict__:
0084:             _buffers = self.__dict__['_buffers']
0085:             if name in _buffers:
0086:                 return _buffers[name]
0087:                 
0088:         if '_modules' in self.__dict__:
0089:             modules = self.__dict__['_modules']
0090:             if name in modules:
0091:                 return modules[name]
0092:                 
0093:         raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
0094:         
0095:     def __setattr__(self, name: str, value: Any) -> None:
0096:         """Custom setattr that handles parameter registration."""
0097:         # Handle special module attributes first
0098:         if name in ['training']:
0099:             object.__setattr__(self, name, value)
0100:             return
0101:             
0102:         if isinstance(value, Tensor):
0103:             if not hasattr(self, '_parameters'):
0104:                 raise TypeError(
0105:                     "cannot assign parameters before Module.__init__() call"
0106:                 )
0107:             self.register_parameter(name, value)
0108:         elif isinstance(value, Module):
0109:             if not hasattr(self, '_modules'):
0110:                 raise TypeError(
0111:                     "cannot assign module before Module.__init__() call"
0112:                 )
0113:             self.add_module(name, value)
0114:         else:
0115:             object.__setattr__(self, name, value)
0116:             
0117:     def parameters(self) -> Iterator[Tensor]:
0118:         """Returns an iterator over module parameters."""
0119:         for param in self._parameters.values():
0120:             if param is not None:
0121:                 yield param
0122:         for module in self._modules.values():
0123:             if module is not None:
0124:                 yield from module.parameters()
0125:                 
0126:     def named_parameters(self) -> Iterator[tuple[str, Tensor]]:
0127:         """Returns an iterator over module parameters, yielding both the
0128:         name of the parameter as well as the parameter itself."""
0129:         for name, param in self._parameters.items():
0130:             if param is not None:
0131:                 yield name, param
0132:         for mname, module in self._modules.items():
0133:             if module is not None:
0134:                 for name, param in module.named_parameters():
0135:                     yield f"{mname}.{name}", param
0136:                     
0137:     def train(self, mode: bool = True) -> 'Module':
0138:         """Sets the module in training mode."""
0139:         self.training = mode
0140:         for module in self._modules.values():
0141:             if module is not None:
0142:                 module.train(mode)
0143:         return self
0144:         
0145:     def eval(self) -> 'Module':
0146:         """Sets the module in evaluation mode."""
0147:         return self.train(False)
0148:         
0149:     def __call__(self, *args, **kwargs):
0150:         return self.forward(*args, **kwargs)
0151:         
0152:     def forward(self, *args, **kwargs):
0153:         """Define the computation performed at every call."""
0154:         raise NotImplementedError
0155:         
0156:     def __repr__(self):
0157:         """Returns a string representation of the module."""
0158:         extra_lines = []
0159:         extra_repr = self.extra_repr()
0160:         if extra_repr:
0161:             extra_lines = extra_repr.split('\n')
0162:             
0163:         child_lines = []
0164:         for key, module in self._modules.items():
0165:             mod_str = repr(module)
0166:             mod_str = _addindent(mod_str, 2)
0167:             child_lines.append('(' + key + '): ' + mod_str)
0168:             
0169:         lines = extra_lines + child_lines
0170:         
0171:         main_str = self.__class__.__name__ + '('
0172:         if lines:
0173:             main_str += '\n  ' + '\n  '.join(lines) + '\n'
0174:         main_str += ')'
0175:         return main_str
0176:         
0177:     def extra_repr(self) -> str:
0178:         """Set the extra representation of the module."""
0179:         return ''
0180: 
0181:     def state_dict(self) -> Dict[str, Any]:
0182:         """Returns a dictionary containing module's state."""
0183:         state = {}
0184:         for name, param in self.named_parameters():
0185:             if param is not None:
0186:                 state[name] = param.data
0187:         for name, buf in self._buffers.items():
0188:             if buf is not None:
0189:                 state[name] = buf.data
0190:         return state
0191: 
0192:     def load_state_dict(self, state_dict: Dict[str, np.ndarray]) -> None:
0193:         """
0194:         Loads module state from state_dict.
0195:         
0196:         Args:
0197:             state_dict: A dictionary containing parameters and buffers
0198:         """
0199:         # Load parameters
0200:         for name, param in self.named_parameters():
0201:             if name in state_dict:
0202:                 if param.data.shape != state_dict[name].shape:
0203:                     raise ValueError(
0204:                         f"Parameter {name} shape mismatch: expected {param.data.shape}, "
0205:                         f"got {state_dict[name].shape}"
0206:                     )
0207:                 param.data = state_dict[name]
0208:                 
0209:         # Load buffers
0210:         for name, buf in self._buffers.items():
0211:             if name in state_dict:
0212:                 if buf.data.shape != state_dict[name].shape:
0213:                     raise ValueError(
0214:                         f"Buffer {name} shape mismatch: expected {buf.data.shape}, "
0215:                         f"got {state_dict[name].shape}"
0216:                     )
0217:                 buf.data = state_dict[name]
0218: 
0219: def _addindent(s_: str, numSpaces: int) -> str:
0220:     """Helper for indenting multiline strings."""
0221:     s = s_.split('\n')
0222:     if len(s) == 1:
0223:         return s_
0224:     first = s.pop(0)
0225:     s = [(numSpaces * ' ') + line for line in s]
0226:     return '\n'.join([first] + s)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\serialization.py
// ----------------------------------------
0001: # DLpy/core/serialization.py
0002: 
0003: import pickle
0004: import numpy as np
0005: from typing import Dict, Any, Union, BinaryIO
0006: from pathlib import Path
0007: import warnings
0008: import json
0009: from ..core import Module, Tensor
0010: 
0011: class ModelSaver:
0012:     """
0013:     Handles saving and loading of models and their state dictionaries.
0014:     
0015:     Supports:
0016:     - Complete model saving/loading (architecture + parameters)
0017:     - State dict only saving/loading (just parameters)
0018:     - Checkpointing (model state + optimizer state + training state)
0019:     """
0020:     
0021:     @staticmethod
0022:     def save_model(model: Module, path: Union[str, Path], optimize: bool = True) -> None:
0023:         """
0024:         Save complete model (architecture + parameters).
0025:         
0026:         Args:
0027:             model: The model to save
0028:             path: Path where to save the model
0029:             optimize: If True, optimize the saved file size
0030:         """
0031:         path = Path(path)
0032:         state = {
0033:             'model_class': model.__class__.__name__,
0034:             'model_dict': model.__dict__,
0035:             'state_dict': ModelSaver.get_state_dict(model)
0036:         }
0037:         
0038:         # Save module hierarchy for reconstruction
0039:         if hasattr(model, '_modules'):
0040:             state['modules'] = {
0041:                 name: module.__class__.__name__ 
0042:                 for name, module in model._modules.items()
0043:             }
0044:             
0045:         with open(path, 'wb') as f:
0046:             if optimize:
0047:                 pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)
0048:             else:
0049:                 pickle.dump(state, f)
0050:                 
0051:     @staticmethod
0052:     def load_model(path: Union[str, Path], custom_classes: Dict[str, type] = None) -> Module:
0053:         """
0054:         Load complete model (architecture + parameters).
0055:         """
0056:         path = Path(path)
0057:         with open(path, 'rb') as f:
0058:             state = pickle.load(f)
0059:             
0060:         # Get model class - first try custom classes if provided
0061:         model_class = None
0062:         if custom_classes and state['model_class'] in custom_classes:
0063:             model_class = custom_classes[state['model_class']]
0064:         
0065:         if model_class is None:
0066:             # Try modules in this order: nn module, main module, test module
0067:             import DLpy.nn as nn
0068:             model_class = getattr(nn, state['model_class'], None)
0069:             
0070:             if model_class is None:
0071:                 # Try test module
0072:                 import sys
0073:                 test_module_names = ['__main__', 'tests.test_serialization']
0074:                 for module_name in test_module_names:
0075:                     if module_name in sys.modules:
0076:                         model_class = getattr(sys.modules[module_name], state['model_class'], None)
0077:                         if model_class is not None:
0078:                             break
0079:                             
0080:         if model_class is None:
0081:             raise ValueError(f"Unknown model class: {state['model_class']}")
0082:             
0083:         # Create model instance
0084:         model = model_class()  # Call __init__ to ensure proper initialization
0085:         
0086:         # Restore model state
0087:         for key, value in state['model_dict'].items():
0088:             if key in model.__dict__:
0089:                 model.__dict__[key] = value
0090:                 
0091:         # Load state dict
0092:         model.load_state_dict(state['state_dict'])
0093:         
0094:         return model
0095:     
0096:     @staticmethod
0097:     def save_state_dict(model: Module, path: Union[str, Path]) -> None:
0098:         """
0099:         Save only the model's state dictionary (parameters).
0100:         
0101:         Args:
0102:             model: The model whose parameters to save
0103:             path: Path where to save the state dict
0104:         """
0105:         path = Path(path)
0106:         state_dict = ModelSaver.get_state_dict(model)
0107:         np.savez(path, **state_dict)
0108:         
0109:     @staticmethod
0110:     def load_state_dict(model: Module, path: Union[str, Path]) -> None:
0111:         """
0112:         Load parameters into an existing model.
0113:         
0114:         Args:
0115:             model: The model to load parameters into
0116:             path: Path to the saved state dict
0117:         """
0118:         path = Path(path)
0119:         if path.suffix != '.npz':
0120:             path = path.with_suffix('.npz')
0121:             
0122:         state_dict = dict(np.load(path))
0123:         model.load_state_dict(state_dict)
0124:         
0125:     @staticmethod
0126:     def save_checkpoint(path: Union[str, Path],
0127:                        model: Module,
0128:                        optimizer: Any = None,
0129:                        epoch: int = None,
0130:                        loss: float = None,
0131:                        additional_data: Dict[str, Any] = None) -> None:
0132:         """
0133:         Save a training checkpoint including model, optimizer and training state.
0134:         
0135:         Args:
0136:             path: Path where to save the checkpoint
0137:             model: The model to checkpoint
0138:             optimizer: The optimizer to checkpoint (optional)
0139:             epoch: Current epoch number (optional)
0140:             loss: Current loss value (optional)
0141:             additional_data: Additional data to save (optional)
0142:         """
0143:         path = Path(path)
0144:         checkpoint = {
0145:             'model_state_dict': ModelSaver.get_state_dict(model),
0146:             'optimizer_state_dict': optimizer.state_dict() if optimizer else None,
0147:             'epoch': epoch,
0148:             'loss': loss,
0149:             'additional_data': additional_data or {}
0150:         }
0151:         
0152:         with open(path, 'wb') as f:
0153:             pickle.dump(checkpoint, f)
0154:             
0155:     @staticmethod
0156:     def load_checkpoint(path: Union[str, Path],
0157:                        model: Module,
0158:                        optimizer: Any = None) -> Dict[str, Any]:
0159:         """
0160:         Load a training checkpoint.
0161:         
0162:         Args:
0163:             path: Path to the checkpoint
0164:             model: The model to load state into
0165:             optimizer: The optimizer to load state into (optional)
0166:             
0167:         Returns:
0168:             Dictionary containing the non-model/optimizer checkpoint data
0169:         """
0170:         path = Path(path)
0171:         with open(path, 'rb') as f:
0172:             checkpoint = pickle.load(f)
0173:             
0174:         model.load_state_dict(checkpoint['model_state_dict'])
0175:         if optimizer and checkpoint['optimizer_state_dict']:
0176:             optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
0177:             
0178:         return {
0179:             'epoch': checkpoint['epoch'],
0180:             'loss': checkpoint['loss'],
0181:             'additional_data': checkpoint['additional_data']
0182:         }
0183:         
0184:     @staticmethod
0185:     def get_state_dict(model: Module) -> Dict[str, np.ndarray]:
0186:         """Gets the state dictionary from a model."""
0187:         state_dict = {}
0188:         for name, param in model.named_parameters():
0189:             if param is not None:
0190:                 state_dict[name] = param.data
0191:         for name, buffer in model._buffers.items():
0192:             if buffer is not None:
0193:                 state_dict[name] = buffer.data
0194:         return state_dict

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\tensor.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Optional, Union, List, Tuple, Callable, Dict, Set
0003: from numbers import Number
0004: 
0005: class Tensor:
0006:     """
0007:     A multidimensional array with autograd capabilities.
0008:     
0009:     The Tensor class wraps numpy arrays and adds automatic differentiation
0010:     capabilities. It tracks the computational graph and enables gradient
0011:     computation through backpropagation.
0012:     
0013:     Attributes:
0014:         data: The underlying numpy array holding the tensor's values
0015:         grad: Gradient of the loss with respect to this tensor
0016:         requires_grad: Whether to compute gradients for this tensor
0017:         _prev: Set of immediate predecessor nodes in computational graph
0018:         _backward_fn: Function to compute gradients during backpropagation
0019:         _is_leaf: Whether this tensor is a leaf node (created by user)
0020:     """
0021:     
0022:     def __init__(
0023:         self,
0024:         data: Union[np.ndarray, List, Number],
0025:         requires_grad: bool = False,
0026:         dtype: Optional[np.dtype] = None
0027:     ):
0028:         # Convert scalars to scalar arrays with shape ()
0029:         if isinstance(data, (int, float)):
0030:             self.data = np.array(data, dtype=dtype or np.float64)  # Will have shape ()
0031:         elif isinstance(data, Tensor):
0032:             self.data = data.data
0033:         elif isinstance(data, list):
0034:             self.data = np.array(data, dtype=dtype)
0035:         else:
0036:             self.data = data.astype(dtype) if dtype else data
0037:             
0038:         self.grad: Optional[np.ndarray] = None
0039:         self._requires_grad = requires_grad
0040:         self._backward_fn: Optional[Callable] = None
0041:         self._prev: Set['Tensor'] = set()
0042:         self._is_leaf = True
0043: 
0044:         # Register with autograd engine
0045:         from .autograd import get_autograd_engine
0046:         engine = get_autograd_engine()
0047:         engine.register_tensor(self)
0048:         
0049:         if requires_grad:
0050:             self.zero_grad()
0051: 
0052:     @property
0053:     def shape(self) -> Tuple[int, ...]:
0054:         """Returns the shape of the tensor."""
0055:         return self.data.shape
0056:         
0057:     @property
0058:     def dtype(self) -> np.dtype:
0059:         """Returns the data type of the tensor."""
0060:         return self.data.dtype
0061:         
0062:     @property
0063:     def requires_grad(self) -> bool:
0064:         """Returns whether the tensor requires gradient computation."""
0065:         return self._requires_grad
0066:     
0067:     def __getitem__(self, index) -> 'Tensor':
0068:         """Enable indexing for tensors."""
0069:         return Tensor(self.data[index], requires_grad=self.requires_grad)
0070:     
0071:     def __len__(self) -> int:
0072:         """Return length of first dimension."""
0073:         return self.data.shape[0] if self.data.shape else 1
0074:         
0075:     def requires_grad_(self, requires_grad: bool = True) -> 'Tensor':
0076:         """Sets gradient computation requirement and returns self."""
0077:         self._requires_grad = requires_grad
0078:         if requires_grad and self.grad is None:
0079:             self.zero_grad()
0080:         return self
0081: 
0082:     def zero_grad(self) -> None:
0083:         """Zeros out the gradient."""
0084:         if self.data.shape == ():  # For scalar tensors
0085:             self.grad = np.zeros(1, dtype=np.float64)  # Force 1D array
0086:         else:
0087:             self.grad = np.zeros_like(self.data, dtype=np.float64)
0088:         
0089:     def backward(self, gradient: Optional[np.ndarray] = None) -> None:
0090:         """
0091:         Computes gradients of the loss with respect to this tensor.
0092:         """
0093:         if not self.requires_grad:
0094:             return
0095: 
0096:         # Handle default gradient for scalar tensors
0097:         if gradient is None:
0098:             if np.prod(self.shape) == 1:
0099:                 if self.shape == ():  # scalar tensor
0100:                     gradient = np.array(1.0)
0101:                 else:
0102:                     gradient = np.ones(self.shape)
0103:             else:
0104:                 raise RuntimeError("grad can be implicitly created only for scalar outputs")
0105: 
0106:         # Ensure gradient is numpy array
0107:         if isinstance(gradient, (int, float)):
0108:             gradient = np.array(gradient)
0109:             
0110:         # Ensure matching shapes for scalar case
0111:         if self.shape == () and gradient.shape != ():
0112:             gradient = gradient.sum()
0113:         elif self.shape != () and gradient.shape == ():
0114:             gradient = np.full(self.shape, gradient)
0115: 
0116:         # Get autograd engine and execute backward pass
0117:         from .autograd import get_autograd_engine
0118:         engine = get_autograd_engine()
0119:         engine.backward(self, gradient)
0120: 
0121: 
0122:     def __repr__(self) -> str:
0123:         return f"Tensor({self.data}, requires_grad={self.requires_grad})"
0124: 
0125:     # Basic arithmetic operations that will be connected to Function implementations
0126:     def __add__(self, other: Union['Tensor', Number]) -> 'Tensor':
0127:         from ..ops.basic import Add
0128:         return Add.apply(self, other)
0129:         
0130:     def __mul__(self, other: Union['Tensor', Number]) -> 'Tensor':
0131:         from ..ops.basic import Multiply
0132:         return Multiply.apply(self, other)
0133:         
0134:     def __matmul__(self, other: 'Tensor') -> 'Tensor':
0135:         from ..ops.basic import MatMul
0136:         return MatMul.apply(self, other)
0137:         
0138:     def __neg__(self) -> 'Tensor':
0139:         return self * (-1)
0140:         
0141:     def __sub__(self, other: Union['Tensor', Number]) -> 'Tensor':
0142:         return self + (-other)
0143: 
0144:     def reshape(self, *shape: int) -> 'Tensor':
0145:         from ..ops.reshape import Reshape
0146:         return Reshape.apply(self, shape)
0147: 
0148:     # Helper methods for numpy compatibility
0149:     def numpy(self) -> np.ndarray:
0150:         """Returns the underlying numpy array."""
0151:         return self.data
0152:         
0153:     @classmethod
0154:     def from_numpy(cls, array: np.ndarray, requires_grad: bool = False) -> 'Tensor':
0155:         """Creates a Tensor from a numpy array."""
0156:         return cls(array.copy(), requires_grad=requires_grad)
0157: 
0158:     # Shape manipulation methods
0159:     def reshape(self, *shape: int) -> 'Tensor':
0160:         """Returns a tensor with the same data and new shape."""
0161:         from ..ops import Reshape
0162:         return Reshape.apply(self, shape)
0163:     
0164:     def permute(self, *dims: int) -> 'Tensor':
0165:         """Permutes the dimensions of the tensor."""
0166:         from ..ops import Transpose
0167:         return Transpose.apply(self, dims)
0168: 
0169:     def pow(self, exponent: Union['Tensor', float]) -> 'Tensor':
0170:         """Returns tensor raised to the power of exponent."""
0171:         from ..ops import Power
0172:         return Power.apply(self, exponent)
0173: 
0174:     def div(self, other: Union['Tensor', float]) -> 'Tensor':
0175:         """Returns self divided by other."""
0176:         from ..ops import Divide
0177:         return Divide.apply(self, other)
0178: 
0179:     def log(self) -> 'Tensor':
0180:         """Returns the natural logarithm of the tensor."""
0181:         from ..ops import Log
0182:         return Log.apply(self)
0183: 
0184:     def exp(self) -> 'Tensor':
0185:         """Returns e raised to the power of each element in the tensor."""
0186:         from ..ops import Exp
0187:         return Exp.apply(self)
0188:     
0189:     def softmax(self, dim: int = -1) -> 'Tensor':
0190:         """Applies the softmax function along the specified dimension."""
0191:         from ..ops.basic import Softmax
0192:         return Softmax.apply(self, dim)
0193: 
0194:     def sigmoid(self) -> 'Tensor':
0195:         """Returns the sigmoid of the tensor."""
0196:         from ..ops import Sigmoid
0197:         return Sigmoid.apply(self)
0198: 
0199:     def tanh(self) -> 'Tensor':
0200:         """Returns the hyperbolic tangent of the tensor."""
0201:         from ..ops import Tanh
0202:         return Tanh.apply(self)
0203:     
0204:     def clip(self, min_val: Union[float, int], max_val: Union[float, int]) -> 'Tensor':
0205:         """Clips tensor values between minimum and maximum."""
0206:         from ..ops import Clip
0207:         return Clip.apply(self, min_val, max_val)
0208: 
0209:     def sum(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':
0210:         """Returns the sum of all elements in the tensor."""
0211:         from ..ops import Sum
0212:         return Sum.apply(self, axis, keepdims)
0213: 
0214:     def mean(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':
0215:         """Returns the mean of all elements in the tensor."""
0216:         from ..ops import Mean
0217:         return Mean.apply(self, axis, keepdims)
0218: 
0219:     def max(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':
0220:         """Returns the maximum value of all elements in the tensor."""
0221:         from ..ops import Max
0222:         return Max.apply(self, axis, keepdims)
0223: 
0224:     def t(self) -> 'Tensor':
0225:         """Returns the transpose of the tensor."""
0226:         from ..ops import Transpose
0227:         return Transpose.apply(self)
0228: 
0229:     def transpose(self, *axes: int) -> 'Tensor':
0230:         """Returns the transposed tensor."""
0231:         from ..ops import Transpose
0232:         return Transpose.apply(self, axes if axes else None)
0233: 
0234:     # Comparison operations
0235:     def __gt__(self, other: Union['Tensor', float]) -> 'Tensor':
0236:         from ..ops import Greater
0237:         return Greater.apply(self, other)
0238: 
0239:     def __ge__(self, other: Union['Tensor', float]) -> 'Tensor':
0240:         from ..ops import GreaterEqual
0241:         return GreaterEqual.apply(self, other)
0242: 
0243:     def __lt__(self, other: Union['Tensor', float]) -> 'Tensor':
0244:         from ..ops import Less
0245:         return Less.apply(self, other)
0246: 
0247:     def __le__(self, other: Union['Tensor', float]) -> 'Tensor':
0248:         from ..ops import LessEqual
0249:         return LessEqual.apply(self, other)
0250: 
0251:     def __eq__(self, other: Union['Tensor', float]) -> 'Tensor':
0252:         from ..ops import Equal
0253:         return Equal.apply(self, other)
0254: 
0255:     def __ne__(self, other: Union['Tensor', float]) -> 'Tensor':
0256:         from ..ops import NotEqual
0257:         return NotEqual.apply(self, other)
0258: 
0259:     def __truediv__(self, other: Union['Tensor', float]) -> 'Tensor':
0260:         """Implements division using the / operator."""
0261:         from ..ops import Divide
0262:         return Divide.apply(self, other)
0263: 
0264:     def __pow__(self, exponent: Union['Tensor', float]) -> 'Tensor':
0265:         """Implements power using the ** operator."""
0266:         from ..ops import Power
0267:         return Power.apply(self, exponent)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\data\__init__.py
// ----------------------------------------
0001: """
0002: DLpy.data
0003: """
0004: 
0005: from .dataset import Dataset, TensorDataset
0006: from .dataloader import DataLoader
0007: from .samplers import Sampler, SequentialSampler, RandomSampler
0008: 
0009: __all__ = [
0010:     'Dataset',
0011:     'TensorDataset',
0012:     'DataLoader',
0013:     'Sampler',
0014:     'SequentialSampler',
0015:     'RandomSampler'
0016: ]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\data\dataloader.py
// ----------------------------------------
0001: # dataloader.py
0002: from typing import Iterator, Optional, Sequence, Any, Callable
0003: import numpy as np
0004: from .samplers import SequentialSampler, RandomSampler, Sampler
0005: from .dataset import Dataset
0006: from DLpy.core.tensor import Tensor
0007: 
0008: class DataLoader:
0009:     """
0010:     Data loader. Combines a dataset and a sampler, and provides an iterable over
0011:     the given dataset.
0012:     
0013:     Args:
0014:         dataset: Dataset from which to load the data
0015:         batch_size: How many samples per batch to load
0016:         shuffle: Set to True to have the data reshuffled at every epoch
0017:         sampler: Defines the strategy to draw samples from the dataset
0018:         drop_last: If True, drop the last incomplete batch
0019:         collate_fn: Merges a list of samples to form a mini-batch
0020:     """
0021:     
0022:     def __init__(
0023:         self,
0024:         dataset: Dataset,
0025:         batch_size: Optional[int] = 1,
0026:         shuffle: bool = False,
0027:         sampler: Optional[Sampler] = None,
0028:         drop_last: bool = False,
0029:         collate_fn: Optional[Callable] = None
0030:     ):
0031:         self.dataset = dataset
0032:         self.batch_size = batch_size
0033:         self.drop_last = drop_last
0034:         self.collate_fn = collate_fn if collate_fn is not None else self._default_collate
0035:         
0036:         if sampler is not None:
0037:             if shuffle:
0038:                 raise ValueError("Cannot specify both shuffle and sampler")
0039:             self.sampler = sampler
0040:         else:
0041:             if shuffle:
0042:                 self.sampler = RandomSampler(dataset)
0043:             else:
0044:                 self.sampler = SequentialSampler(dataset)
0045:                 
0046:     def _default_collate(self, batch: Sequence[Any]) -> Any:
0047:         """Default collate function for batching samples."""
0048:         elem = batch[0]
0049:         
0050:         if isinstance(elem, Tensor):
0051:             return Tensor(np.stack([b.data for b in batch]))
0052:         elif isinstance(elem, (int, float)):
0053:             # Convert scalar values to a flat tensor
0054:             return Tensor(np.array(batch))
0055:         elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple
0056:             return type(elem)(*(self._default_collate(samples) for samples in zip(*batch)))
0057:         elif isinstance(elem, (list, tuple)):
0058:             return [self._default_collate(samples) for samples in zip(*batch)]
0059:         else:
0060:             try:
0061:                 return Tensor(np.array(batch))
0062:             except:
0063:                 return batch
0064:             
0065:     def __iter__(self) -> Iterator[Any]:
0066:         batch = []
0067:         for idx in self.sampler:
0068:             batch.append(self.dataset[idx])
0069:             if len(batch) == self.batch_size:
0070:                 yield self.collate_fn(batch)
0071:                 batch = []
0072:         if len(batch) > 0 and not self.drop_last:
0073:             yield self.collate_fn(batch)
0074:             
0075:     def __len__(self) -> int:
0076:         if self.drop_last:
0077:             return len(self.dataset) // self.batch_size
0078:         else:
0079:             return (len(self.dataset) + self.batch_size - 1) // self.batch_size

// File: C:\Users\aluja\Desktop\DLpy\DLpy\data\dataset.py
// ----------------------------------------
0001: # dataset.py
0002: from typing import Any, Sized, TypeVar, Tuple
0003: from DLpy.core.tensor import Tensor
0004: 
0005: T_co = TypeVar('T_co', covariant=True)
0006: 
0007: class Dataset(Sized):
0008:     """
0009:     Abstract base class for all datasets.
0010:     
0011:     All datasets that represent a map from keys to data samples should subclass it.
0012:     All subclasses must implement __getitem__() and __len__().
0013:     """
0014:     
0015:     def __getitem__(self, index: int) -> Any:
0016:         raise NotImplementedError
0017:         
0018:     def __len__(self) -> int:
0019:         raise NotImplementedError
0020:         
0021: class TensorDataset(Dataset):
0022:     """
0023:     Dataset wrapping tensors.
0024:     
0025:     Each sample will be retrieved by indexing tensors along the first dimension.
0026:     
0027:     Args:
0028:         *tensors: Tensors that have the same size of the first dimension.
0029:     """
0030:     
0031:     def __init__(self, *tensors: Tensor):
0032:         assert all(tensors[0].shape[0] == tensor.shape[0] for tensor in tensors), \
0033:             "Size mismatch between tensors"
0034:         self.tensors = tensors
0035:         
0036:     def __getitem__(self, index: int) -> Tuple[Tensor, ...]:
0037:         return tuple(tensor[index] for tensor in self.tensors)
0038:         
0039:     def __len__(self) -> int:
0040:         return self.tensors[0].shape[0]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\data\samplers.py
// ----------------------------------------
0001: # samplers.py
0002: from typing import Iterator, Optional, Sized
0003: import numpy as np
0004: 
0005: class Sampler:
0006:     """Base class for all Samplers."""
0007:     
0008:     def __init__(self, data_source: Optional[Sized]):
0009:         self.data_source = data_source
0010:         
0011:     def __iter__(self) -> Iterator[int]:
0012:         raise NotImplementedError
0013:         
0014:     def __len__(self) -> int:
0015:         raise NotImplementedError
0016: 
0017: class SequentialSampler(Sampler):
0018:     """Samples elements sequentially."""
0019:     
0020:     def __iter__(self) -> Iterator[int]:
0021:         return iter(range(len(self.data_source)))
0022:         
0023:     def __len__(self) -> int:
0024:         return len(self.data_source)
0025: 
0026: class RandomSampler(Sampler):
0027:     """Samples elements randomly without replacement."""
0028:     
0029:     def __iter__(self) -> Iterator[int]:
0030:         indices = np.random.permutation(len(self.data_source))
0031:         return iter(indices.tolist())
0032:         
0033:     def __len__(self) -> int:
0034:         return len(self.data_source)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\__init__.py
// ----------------------------------------
0001: """
0002: Neural network module for DLpy.
0003: 
0004: This module contains all components needed for building neural networks.
0005: """
0006: 
0007: from .linear import Linear
0008: from .batch_norm import BatchNorm1d, BatchNorm2d
0009: from .layer_norm import LayerNorm
0010: from .dropout import Dropout, Dropout2d
0011: from .sequential import Sequential
0012: from .activations import (
0013:     relu, leaky_relu, elu, gelu, sigmoid, tanh,
0014:     ReLU, LeakyReLU, ELU, GELU, Sigmoid, Tanh,
0015:     ReLUFunction, LeakyReLUFunction, ELUFunction, GELUFunction, SigmoidFunction, TanhFunction
0016: )
0017: from .conv2d import Conv2d 
0018: from .pooling import MaxPool2d, AvgPool2d
0019: from .normalization import GroupNorm, InstanceNorm2d
0020: from .rnn import LSTM, GRU, LSTMCell, GRUCell
0021: from .transformer import (
0022:     MultiHeadAttention,
0023:     TransformerEncoderLayer,
0024:     TransformerEncoder,
0025:     PositionalEncoding
0026: )
0027: 
0028: __all__ = [
0029:     # Layers
0030:     'Linear',
0031:     'BatchNorm1d',
0032:     'BatchNorm2d',
0033:     'LayerNorm',
0034:     'Dropout',
0035:     'Dropout2d',
0036:     'Sequential',
0037:     
0038:     # Activation functions
0039:     'relu',
0040:     'leaky_relu',
0041:     'elu',
0042:     'gelu',
0043:     'sigmoid',
0044:     'tanh',
0045:     'ReLU',
0046:     'LeakyReLU',
0047:     'ELU',
0048:     'GELU',
0049:     'Sigmoid',
0050:     'Tanh',
0051:     'ReLUFunction',
0052:     'LeakyReLUFunction',
0053:     'ELUFunction',
0054:     'GELUFunction',
0055:     'SigmoidFunction',
0056:     'TanhFunction',
0057: 
0058:     # Convolutional layers
0059:     'Conv2d',
0060: 
0061:     # Pooling layers
0062:     'MaxPool2d',
0063:     'AvgPool2d',
0064: 
0065:     # Normalization layers
0066:     'GroupNorm',
0067:     'InstanceNorm2d',
0068:     
0069:     # RNN layers
0070:     'LSTM',
0071:     'GRU',
0072:     'LSTMCell',
0073:     'GRUCell',
0074: 
0075:     # Transformer layers
0076:     'MultiHeadAttention',
0077:     'TransformerEncoderLayer',
0078:     'TransformerEncoder',
0079:     'PositionalEncoding',
0080: ]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\activations.py
// ----------------------------------------
0001: """
0002: Activation functions module for DLpy.
0003: 
0004: This module contains both Function and Module implementations of standard activation functions.
0005: Functions can be used directly (relu(x)), while Modules can be used in Sequential layers (ReLU()).
0006: Each activation function is implemented with full autograd support.
0007: """
0008: 
0009: from typing import Dict, Optional
0010: import numpy as np
0011: from ..core import Function, Module, Tensor
0012: 
0013: # Function implementations (for functional usage)
0014: 
0015: class ReLUFunction(Function):
0016:     """
0017:     Rectified Linear Unit activation function.
0018:     
0019:     Forward: f(x) = max(0, x)
0020:     Backward: f'(x) = 1 if x > 0 else 0
0021:     """
0022:     
0023:     @staticmethod
0024:     def forward(ctx, x):
0025:         if not isinstance(x, Tensor):
0026:             x = Tensor(x)
0027:             
0028:         ctx.save_for_backward(x)
0029:         return Tensor(np.maximum(0, x.data))
0030:         
0031:     @staticmethod
0032:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0033:         x, = ctx.saved_tensors
0034:         if x.requires_grad:
0035:             grad = grad_output * (x.data > 0)
0036:             grad_dict[id(x)] = grad
0037: 
0038: class LeakyReLUFunction(Function):
0039:     """
0040:     Leaky Rectified Linear Unit activation function.
0041:     
0042:     Forward: f(x) = x if x > 0 else negative_slope * x
0043:     Backward: f'(x) = 1 if x > 0 else negative_slope
0044:     
0045:     Args:
0046:         negative_slope: Controls slope for negative values. Default: 0.01
0047:     """
0048:     
0049:     @staticmethod
0050:     def forward(ctx, x, negative_slope: float = 0.01):
0051:         if not isinstance(x, Tensor):
0052:             x = Tensor(x)
0053:             
0054:         ctx.save_for_backward(x)
0055:         ctx.save_arguments(negative_slope=negative_slope)
0056:         
0057:         return Tensor(np.where(x.data > 0, x.data, negative_slope * x.data))
0058:         
0059:     @staticmethod
0060:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0061:         x, = ctx.saved_tensors
0062:         negative_slope = ctx.saved_arguments['negative_slope']
0063:         
0064:         if x.requires_grad:
0065:             grad = grad_output * np.where(x.data > 0, 1.0, negative_slope)
0066:             grad_dict[id(x)] = grad
0067: 
0068: class ELUFunction(Function):
0069:     """
0070:     Exponential Linear Unit activation function.
0071:     
0072:     Forward: f(x) = x if x > 0 else alpha * (exp(x) - 1)
0073:     Backward: f'(x) = 1 if x > 0 else alpha * exp(x)
0074:     
0075:     Args:
0076:         alpha: Controls the value to which an ELU saturates for negative inputs. Default: 1.0
0077:     """
0078:     
0079:     @staticmethod
0080:     def forward(ctx, x, alpha: float = 1.0):
0081:         if not isinstance(x, Tensor):
0082:             x = Tensor(x)
0083:             
0084:         ctx.save_for_backward(x)
0085:         ctx.save_arguments(alpha=alpha)
0086:         
0087:         return Tensor(np.where(x.data > 0, x.data, alpha * (np.exp(x.data) - 1)))
0088:         
0089:     @staticmethod
0090:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0091:         x, = ctx.saved_tensors
0092:         alpha = ctx.saved_arguments['alpha']
0093:         
0094:         if x.requires_grad:
0095:             grad = grad_output * np.where(x.data > 0, 1.0, alpha * np.exp(x.data))
0096:             grad_dict[id(x)] = grad
0097: 
0098: class GELUFunction(Function):
0099:     """
0100:     Gaussian Error Linear Unit activation function.
0101:     
0102:     Forward: f(x) = x * Φ(x)
0103:     where Φ(x) is the Gaussian cumulative distribution function.
0104:     
0105:     This implementation uses the approximation:
0106:     f(x) ≈ 0.5x * (1 + tanh(√(2/π) * (x + 0.044715x³)))
0107:     """
0108:     
0109:     @staticmethod
0110:     def forward(ctx, x):
0111:         if not isinstance(x, Tensor):
0112:             x = Tensor(x)
0113:             
0114:         # Constants for the approximation
0115:         sqrt_2_over_pi = np.sqrt(2 / np.pi)
0116:         coeff = 0.044715
0117:         
0118:         # Compute intermediate values
0119:         x_cubed = x.data ** 3
0120:         inner = sqrt_2_over_pi * (x.data + coeff * x_cubed)
0121:         tanh_inner = np.tanh(inner)
0122:         
0123:         # Compute output
0124:         result = 0.5 * x.data * (1 + tanh_inner)
0125:         
0126:         # Save for backward pass
0127:         ctx.save_for_backward(x)
0128:         ctx.save_arguments(tanh_inner=tanh_inner)
0129:         
0130:         return Tensor(result)
0131:         
0132:     @staticmethod
0133:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0134:         x, = ctx.saved_tensors
0135:         tanh_inner = ctx.saved_arguments['tanh_inner']
0136:         
0137:         if x.requires_grad:
0138:             sqrt_2_over_pi = np.sqrt(2 / np.pi)
0139:             coeff = 0.044715
0140:             
0141:             # Compute derivative
0142:             x_cubed = x.data ** 3
0143:             inner = sqrt_2_over_pi * (x.data + coeff * x_cubed)
0144:             
0145:             # d/dx[GELU(x)] = 0.5 * (1 + tanh(inner)) + 
0146:             #                 0.5x * (1 - tanh²(inner)) * sqrt(2/π) * (1 + 3 * 0.044715x²)
0147:             grad = 0.5 * (1 + tanh_inner)
0148:             grad += 0.5 * x.data * (1 - tanh_inner ** 2) * sqrt_2_over_pi * (1 + 3 * coeff * x.data ** 2)
0149:             
0150:             grad_dict[id(x)] = grad_output * grad
0151: 
0152: class SigmoidFunction(Function):
0153:     """
0154:     Sigmoid activation function.
0155:     
0156:     Forward: f(x) = 1 / (1 + exp(-x))
0157:     Backward: f'(x) = f(x) * (1 - f(x))
0158:     """
0159:     
0160:     @staticmethod
0161:     def forward(ctx, x):
0162:         if not isinstance(x, Tensor):
0163:             x = Tensor(x)
0164:             
0165:         # Compute sigmoid with numerical stability
0166:         x_data = x.data
0167:         exp_neg_x = np.exp(-np.abs(x_data))
0168:         sigmoid_x = np.where(x_data >= 0, 
0169:                            1 / (1 + exp_neg_x),
0170:                            exp_neg_x / (1 + exp_neg_x))
0171:         
0172:         ctx.save_for_backward(x)
0173:         ctx.save_arguments(sigmoid_x=sigmoid_x)
0174:         return Tensor(sigmoid_x)
0175:         
0176:     @staticmethod
0177:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0178:         x, = ctx.saved_tensors
0179:         sigmoid_x = ctx.saved_arguments['sigmoid_x']
0180:         
0181:         if x.requires_grad:
0182:             grad = grad_output * sigmoid_x * (1 - sigmoid_x)
0183:             grad_dict[id(x)] = grad
0184: 
0185: class TanhFunction(Function):
0186:     """
0187:     Hyperbolic tangent activation function.
0188:     
0189:     Forward: f(x) = tanh(x)
0190:     Backward: f'(x) = 1 - tanh²(x)
0191:     """
0192:     
0193:     @staticmethod
0194:     def forward(ctx, x):
0195:         if not isinstance(x, Tensor):
0196:             x = Tensor(x)
0197:             
0198:         tanh_x = np.tanh(x.data)
0199:         ctx.save_for_backward(x)
0200:         ctx.save_arguments(tanh_x=tanh_x)
0201:         return Tensor(tanh_x)
0202:         
0203:     @staticmethod
0204:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0205:         x, = ctx.saved_tensors
0206:         tanh_x = ctx.saved_arguments['tanh_x']
0207:         
0208:         if x.requires_grad:
0209:             grad = grad_output * (1 - tanh_x ** 2)
0210:             grad_dict[id(x)] = grad
0211: 
0212: # Module implementations (for use in Sequential and other Module-based architectures)
0213: 
0214: class ReLU(Module):
0215:     """Applies the rectified linear unit function element-wise."""
0216:     def forward(self, x: Tensor) -> Tensor:
0217:         return ReLUFunction.apply(x)
0218: 
0219: class LeakyReLU(Module):
0220:     """Applies leaky ReLU function element-wise."""
0221:     def __init__(self, negative_slope: float = 0.01):
0222:         super().__init__()
0223:         self.negative_slope = negative_slope
0224:         
0225:     def forward(self, x: Tensor) -> Tensor:
0226:         return LeakyReLUFunction.apply(x, self.negative_slope)
0227: 
0228: class ELU(Module):
0229:     """Applies the exponential linear unit function element-wise."""
0230:     def __init__(self, alpha: float = 1.0):
0231:         super().__init__()
0232:         self.alpha = alpha
0233:         
0234:     def forward(self, x: Tensor) -> Tensor:
0235:         return ELUFunction.apply(x, self.alpha)
0236: 
0237: class GELU(Module):
0238:     """Applies the Gaussian Error Linear Units function."""
0239:     def forward(self, x: Tensor) -> Tensor:
0240:         return GELUFunction.apply(x)
0241: 
0242: class Sigmoid(Module):
0243:     """Applies the sigmoid function element-wise."""
0244:     def forward(self, x: Tensor) -> Tensor:
0245:         return SigmoidFunction.apply(x)
0246: 
0247: class Tanh(Module):
0248:     """Applies the hyperbolic tangent function element-wise."""
0249:     def forward(self, x: Tensor) -> Tensor:
0250:         return TanhFunction.apply(x)
0251: 
0252: # Functional interface (for direct usage)
0253: 
0254: def relu(x: Tensor) -> Tensor:
0255:     """Applies ReLU activation function."""
0256:     return ReLUFunction.apply(x)
0257: 
0258: def leaky_relu(x: Tensor, negative_slope: float = 0.01) -> Tensor:
0259:     """Applies Leaky ReLU activation function."""
0260:     return LeakyReLUFunction.apply(x, negative_slope)
0261: 
0262: def elu(x: Tensor, alpha: float = 1.0) -> Tensor:
0263:     """Applies ELU activation function."""
0264:     return ELUFunction.apply(x, alpha)
0265: 
0266: def gelu(x: Tensor) -> Tensor:
0267:     """Applies GELU activation function."""
0268:     return GELUFunction.apply(x)
0269: 
0270: def sigmoid(x: Tensor) -> Tensor:
0271:     """Applies Sigmoid activation function."""
0272:     return SigmoidFunction.apply(x)
0273: 
0274: def tanh(x: Tensor) -> Tensor:
0275:     """Applies Tanh activation function."""
0276:     return TanhFunction.apply(x)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\batch_norm.py
// ----------------------------------------
0001: # batch_norm.py
0002: import numpy as np
0003: from typing import Optional, Tuple
0004: from ..core import Tensor, Module
0005: 
0006: class BatchNorm1d(Module):
0007:     """
0008:     Applies Batch Normalization over a 2D input (batch, features)
0009: 
0010:     Args:
0011:         num_features: Number of features or channels
0012:         eps: Small constant for numerical stability
0013:         momentum: Value for running_mean and running_var computation
0014:         affine: If True, use learnable affine parameters
0015:         track_running_stats: If True, track running mean and variance
0016:     """
0017:     def __init__(self, 
0018:                  num_features: int,
0019:                  eps: float = 1e-5,
0020:                  momentum: float = 0.1,
0021:                  affine: bool = True,
0022:                  track_running_stats: bool = True):
0023:         super().__init__()
0024:         self.num_features = num_features
0025:         self.eps = eps
0026:         self.momentum = momentum
0027:         self.affine = affine
0028:         self.track_running_stats = track_running_stats
0029: 
0030:         if self.affine:
0031:             self.weight = Tensor(np.ones(num_features), requires_grad=True)
0032:             self.bias = Tensor(np.zeros(num_features), requires_grad=True)
0033:         else:
0034:             self.register_parameter('weight', None)
0035:             self.register_parameter('bias', None)
0036: 
0037:         if self.track_running_stats:
0038:             self.register_buffer('running_mean', Tensor(np.zeros(num_features)))
0039:             self.register_buffer('running_var', Tensor(np.ones(num_features)))
0040:             self.register_buffer('num_batches_tracked', Tensor(np.array([0])))
0041:         else:
0042:             self.register_buffer('running_mean', None)
0043:             self.register_buffer('running_var', None)
0044:             self.register_buffer('num_batches_tracked', None)
0045: 
0046:     def forward(self, x: Tensor) -> Tensor:
0047:         if self.momentum is None:
0048:             exponential_average_factor = 0.0
0049:         else:
0050:             exponential_average_factor = self.momentum
0051: 
0052:         if self.training and self.track_running_stats:
0053:             if self.num_batches_tracked is not None:
0054:                 self.num_batches_tracked += 1
0055:                 if self.momentum is None:  # Use cumulative moving average
0056:                     exponential_average_factor = 1.0 / float(self.num_batches_tracked)
0057:                 else:  # Use exponential moving average
0058:                     exponential_average_factor = self.momentum
0059: 
0060:         # Calculate mean and variance for the current batch
0061:         if self.training or not self.track_running_stats:
0062:             mean = x.data.mean(axis=0)
0063:             var = x.data.var(axis=0, ddof=0)
0064:             n = x.data.shape[0]
0065:         else:
0066:             mean = self.running_mean.data
0067:             var = self.running_var.data
0068:             n = None
0069: 
0070:         # Update running stats if tracking
0071:         if self.training and self.track_running_stats:
0072:             running_mean = self.running_mean.data
0073:             running_var = self.running_var.data
0074:             running_mean = running_mean * (1 - exponential_average_factor) + mean * exponential_average_factor
0075:             running_var = running_var * (1 - exponential_average_factor) + var * exponential_average_factor
0076:             self.running_mean.data = running_mean
0077:             self.running_var.data = running_var
0078: 
0079:         # Normalize with fixed std calculation
0080:         x_norm = (x.data - mean) / np.sqrt(var)
0081:         
0082:         # Apply affine transform if specified
0083:         if self.affine:
0084:             # Explicit broadcasting by adding batch dimension
0085:             weight = self.weight.data
0086:             bias = self.bias.data
0087:             x_norm = x_norm * weight + bias
0088: 
0089:         return Tensor(x_norm, requires_grad=x.requires_grad)
0090: 
0091: class BatchNorm2d(BatchNorm1d):
0092:     """
0093:     Applies Batch Normalization over a 4D input (batch, channels, height, width)
0094:     
0095:     Args:
0096:         num_features: Number of channels (C from input of size (N,C,H,W))
0097:         eps: Small constant for numerical stability
0098:         momentum: Value for running_mean and running_var computation
0099:         affine: If True, use learnable affine parameters
0100:         track_running_stats: If True, track running mean and variance
0101:     """
0102:     def forward(self, x: Tensor) -> Tensor:
0103:         assert len(x.shape) == 4, f'expected 4D input, got {len(x.shape)}D input'
0104:         
0105:         # Reshape input: (N,C,H,W) -> (N*H*W,C)
0106:         N, C, H, W = x.shape
0107:         x_reshaped = x.data.transpose(0, 2, 3, 1).reshape(-1, C)
0108:         x_normed = super().forward(Tensor(x_reshaped, requires_grad=x.requires_grad))
0109:         
0110:         # Reshape back: (N*H*W,C) -> (N,C,H,W)
0111:         return Tensor(x_normed.data.reshape(N, H, W, C).transpose(0, 3, 1, 2),
0112:                      requires_grad=x.requires_grad)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\conv2d.py
// ----------------------------------------
0001: from typing import Tuple, Optional, Union
0002: import numpy as np
0003: from ..core import Tensor, Module
0004: from ..ops.cnn import Conv2dFunction
0005: 
0006: def _pair(x: Union[int, Tuple[int, int]]) -> Tuple[int, int]:
0007:     """Convert input to a pair of values."""
0008:     if isinstance(x, tuple):
0009:         return x
0010:     return (x, x)
0011: 
0012: class Conv2d(Module):
0013:     """
0014:     Applies a 2D convolution over an input signal composed of several input planes.
0015:     
0016:     Args:
0017:         in_channels (int): Number of channels in the input image
0018:         out_channels (int): Number of channels produced by the convolution
0019:         kernel_size (int or tuple): Size of the convolving kernel
0020:         stride (int or tuple, optional): Stride of the convolution. Default: 1
0021:         padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0
0022:         dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
0023:         groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 
0024:         bias (bool, optional): If True, adds a learnable bias to the output. Default: True
0025:         
0026:     Shape:
0027:         - Input: (N, C_in, H, W)
0028:         - Output: (N, C_out, H_out, W_out)
0029:           where
0030:           H_out = (H + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1
0031:           W_out = (W + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1
0032:     """
0033:     
0034:     def __init__(
0035:         self,
0036:         in_channels: int,
0037:         out_channels: int,
0038:         kernel_size: Union[int, Tuple[int, int]],
0039:         stride: Union[int, Tuple[int, int]] = 1,
0040:         padding: Union[int, Tuple[int, int]] = 0,
0041:         dilation: Union[int, Tuple[int, int]] = 1,
0042:         groups: int = 1,
0043:         bias: bool = True
0044:     ):
0045:         super().__init__()
0046:         
0047:         if in_channels % groups != 0:
0048:             raise ValueError('in_channels must be divisible by groups')
0049:         if out_channels % groups != 0:
0050:             raise ValueError('out_channels must be divisible by groups')
0051:             
0052:         self.in_channels = in_channels
0053:         self.out_channels = out_channels
0054:         self.kernel_size = _pair(kernel_size)
0055:         self.stride = _pair(stride)
0056:         self.padding = _pair(padding)
0057:         self.dilation = _pair(dilation)
0058:         self.groups = groups
0059:         
0060:         # Initialize weights using He initialization
0061:         # Adjust fan_in to account for groups
0062:         fan_in = in_channels // groups * self.kernel_size[0] * self.kernel_size[1]
0063:         bound = np.sqrt(2.0 / fan_in)
0064:         weight_shape = (out_channels, in_channels // groups, *self.kernel_size)
0065:         weight = Tensor(
0066:             np.random.uniform(-bound, bound, weight_shape),
0067:             requires_grad=True
0068:         )
0069:         self.register_parameter('weight', weight)
0070:         
0071:         if bias:
0072:             # Initialize bias to zero
0073:             bias_data = np.zeros(out_channels)
0074:             self.register_parameter('bias', Tensor(bias_data, requires_grad=True))
0075:         else:
0076:             self.register_parameter('bias', None)
0077:             
0078:     def forward(self, x: Tensor) -> Tensor:
0079:         """
0080:         Forward pass of the convolution layer.
0081:         
0082:         Args:
0083:             x: Input tensor of shape (N, C_in, H, W)
0084:             
0085:         Returns:
0086:             Output tensor of shape (N, C_out, H_out, W_out)
0087:         """
0088:         return Conv2dFunction.apply(
0089:             x, self.weight, self.bias,
0090:             self.stride, self.padding,
0091:             self.dilation, self.groups
0092:         )
0093:         
0094:     def extra_repr(self) -> str:
0095:         """Returns a string with extra representation information."""
0096:         s = (f'{self.in_channels}, {self.out_channels}, '
0097:              f'kernel_size={self.kernel_size}')
0098:         
0099:         if self.stride != (1, 1):
0100:             s += f', stride={self.stride}'
0101:         if self.padding != (0, 0):
0102:             s += f', padding={self.padding}'
0103:         if self.dilation != (1, 1):
0104:             s += f', dilation={self.dilation}'
0105:         if self.groups != 1:
0106:             s += f', groups={self.groups}'
0107:         if self.bias is None:
0108:             s += ', bias=False'
0109:         return s
0110: 
0111:     @staticmethod
0112:     def calc_output_shape(
0113:         input_shape: Tuple[int, ...],
0114:         out_channels: int,
0115:         kernel_size: Tuple[int, int],
0116:         stride: Tuple[int, int],
0117:         padding: Tuple[int, int],
0118:         dilation: Tuple[int, int]
0119:     ) -> Tuple[int, int, int, int]:
0120:         """
0121:         Calculate the output shape of the convolution.
0122:         
0123:         Args:
0124:             input_shape: Input shape (N, C_in, H, W)
0125:             out_channels: Number of output channels
0126:             kernel_size: Size of the kernel
0127:             stride: Stride of the convolution
0128:             padding: Zero-padding added to both sides
0129:             dilation: Spacing between kernel elements
0130:             
0131:         Returns:
0132:             Output shape (N, C_out, H_out, W_out)
0133:         """
0134:         N, _, H, W = input_shape
0135:         
0136:         H_out = ((H + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) 
0137:                 // stride[0] + 1)
0138:         W_out = ((W + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) 
0139:                 // stride[1] + 1)
0140:         
0141:         return (N, out_channels, H_out, W_out)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\dropout.py
// ----------------------------------------
0001: # dropout.py
0002: import numpy as np
0003: from typing import Optional
0004: from ..core import Tensor, Module
0005: 
0006: class Dropout(Module):
0007:     """
0008:     Randomly zeroes some of the elements of the input tensor with probability p using samples 
0009:     from a Bernoulli distribution.
0010:     
0011:     Args:
0012:         p: Probability of an element to be zeroed. Default: 0.5
0013:         inplace: If set to True, will do operation in-place. Default: False
0014:     """
0015:     def __init__(self, p: float = 0.5, inplace: bool = False):
0016:         super().__init__()
0017:         if p < 0 or p > 1:
0018:             raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
0019:         self.p = p
0020:         self.inplace = inplace
0021:         self.mask: Optional[np.ndarray] = None
0022: 
0023:     def forward(self, x: Tensor) -> Tensor:
0024:         if self.training:
0025:             # Generate mask
0026:             self.mask = (np.random.rand(*x.shape) > self.p).astype(np.float64)
0027:             # Scale up by 1/(1-p) to maintain expected value
0028:             scale = 1.0 / (1.0 - self.p) if self.p != 1.0 else 0.0
0029:             
0030:             if self.inplace:
0031:                 x.data *= self.mask * scale
0032:                 return x
0033:             else:
0034:                 return Tensor(x.data * self.mask * scale, requires_grad=x.requires_grad)
0035:         else:
0036:             return x
0037: 
0038: class Dropout2d(Module):
0039:     """
0040:     Randomly zero out entire channels (a channel is a 2D feature map,
0041:     e.g., the j-th channel of the i-th sample in the batch input) of the input tensor.
0042:     Each channel will be zeroed out independently on every forward call with probability p.
0043:     
0044:     Args:
0045:         p: Probability of a channel to be zeroed. Default: 0.5
0046:         inplace: If set to True, will do operation in-place. Default: False
0047:     """
0048:     def __init__(self, p: float = 0.5, inplace: bool = False):
0049:         super().__init__()
0050:         if p < 0 or p > 1:
0051:             raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
0052:         self.p = p
0053:         self.inplace = inplace
0054:         self.mask: Optional[np.ndarray] = None
0055: 
0056:     def forward(self, x: Tensor) -> Tensor:
0057:         if self.training:
0058:             assert len(x.shape) == 4, f'expected 4D input, got {len(x.shape)}D input'
0059:             
0060:             # Generate mask for entire channels
0061:             mask = (np.random.rand(x.shape[0], x.shape[1], 1, 1) > self.p).astype(np.float64)
0062:             self.mask = np.broadcast_to(mask, x.shape)
0063:             
0064:             # Scale up by 1/(1-p) to maintain expected value
0065:             scale = 1.0 / (1.0 - self.p) if self.p != 1.0 else 0.0
0066:             
0067:             if self.inplace:
0068:                 x.data *= self.mask * scale
0069:                 return x
0070:             else:
0071:                 return Tensor(x.data * self.mask * scale, requires_grad=x.requires_grad)
0072:         else:
0073:             return x

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\layer_norm.py
// ----------------------------------------
0001: # layer_norm.py
0002: import numpy as np
0003: from typing import List, Optional, Tuple
0004: from ..core import Tensor, Module
0005: 
0006: class LayerNorm(Module):
0007:     """
0008:     Applies Layer Normalization over a mini-batch of inputs
0009: 
0010:     Args:
0011:         normalized_shape: Input shape from an expected input of size
0012:         eps: Small constant for numerical stability
0013:         elementwise_affine: If True, use learnable affine parameters
0014:     """
0015:     def __init__(self,
0016:                  normalized_shape: List[int],
0017:                  eps: float = 1e-5,
0018:                  elementwise_affine: bool = True):
0019:         super().__init__()
0020:         if isinstance(normalized_shape, int):
0021:             normalized_shape = (normalized_shape,)
0022:         self.normalized_shape = tuple(normalized_shape)
0023:         self.eps = eps
0024:         self.elementwise_affine = elementwise_affine
0025: 
0026:         if self.elementwise_affine:
0027:             self.weight = Tensor(np.ones(normalized_shape), requires_grad=True)
0028:             self.bias = Tensor(np.zeros(normalized_shape), requires_grad=True)
0029:         else:
0030:             self.register_parameter('weight', None)
0031:             self.register_parameter('bias', None)
0032: 
0033:     def forward(self, x: Tensor) -> Tensor:
0034:         # Check input dimensions
0035:         input_shape = x.shape
0036:         ndim = len(input_shape)
0037:         normalized_ndim = len(self.normalized_shape)
0038:         
0039:         if ndim < normalized_ndim:
0040:             raise ValueError(f'Expected {normalized_ndim}D or higher input, got {ndim}D input')
0041:         
0042:         for i, s in enumerate(reversed(self.normalized_shape)):
0043:             if input_shape[ndim - i - 1] != s:
0044:                 raise ValueError(
0045:                     f'Expected normalized_shape={self.normalized_shape}, '
0046:                     f'got input shape={input_shape}'
0047:                 )
0048: 
0049:         # Calculate statistics over the normalized dimensions
0050:         stats_shape = input_shape[:-normalized_ndim] + (1,) * normalized_ndim
0051:         reduction_axes = tuple(range(ndim - normalized_ndim, ndim))
0052:         
0053:         mean = np.mean(x.data, axis=reduction_axes, keepdims=True)
0054:         # Use ddof=0 for layer normalization
0055:         var = np.var(x.data, axis=reduction_axes, keepdims=True, ddof=0)
0056:         
0057:         # Normalize
0058:         x_norm = (x.data - mean) / np.sqrt(var + self.eps)
0059: 
0060:         # Apply affine transform if specified
0061:         if self.elementwise_affine:
0062:             # Reshape weight and bias to broadcast correctly
0063:             weight_shape = (1,) * (ndim - normalized_ndim) + self.normalized_shape
0064:             bias_shape = (1,) * (ndim - normalized_ndim) + self.normalized_shape
0065:             
0066:             # Handle broadcasting for weight and bias
0067:             weight = np.broadcast_to(
0068:                 self.weight.data.reshape(weight_shape), 
0069:                 x_norm.shape
0070:             )
0071:             bias = np.broadcast_to(
0072:                 self.bias.data.reshape(bias_shape),
0073:                 x_norm.shape
0074:             )
0075:             
0076:             x_norm = x_norm * weight + bias
0077: 
0078:         return Tensor(x_norm, requires_grad=x.requires_grad)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\linear.py
// ----------------------------------------
0001: from typing import Optional, Dict
0002: import numpy as np
0003: from ..core import Tensor, Function, Module
0004: 
0005: class Linear(Module):
0006:     """
0007:     Applies a linear transformation to the incoming data: y = xW^T + b
0008:     
0009:     Args:
0010:         in_features: size of each input sample
0011:         out_features: size of each output sample
0012:         bias: If set to False, the layer will not learn an additive bias
0013:     """
0014:     
0015:     def __init__(self, in_features: int, out_features: int, bias: bool = True):
0016:         super().__init__()
0017:         
0018:         self.in_features = in_features
0019:         self.out_features = out_features
0020:         
0021:         # Initialize weights using He initialization
0022:         bound = np.sqrt(2.0 / in_features)
0023:         weight = Tensor(
0024:             np.random.uniform(-bound, bound, (out_features, in_features)),
0025:             requires_grad=True
0026:         )
0027:         self.register_parameter('weight', weight)
0028:         
0029:         if bias:
0030:             bias = Tensor(np.zeros(out_features), requires_grad=True)
0031:             self.register_parameter('bias', bias)
0032:         else:
0033:             self.register_parameter('bias', None)
0034:             
0035:     def forward(self, input: Tensor) -> Tensor:
0036:         """Forward pass of the linear layer."""
0037:         return LinearFunction.apply(input, self.weight, self.bias)
0038: 
0039: class LinearFunction(Function):
0040:     @staticmethod
0041:     def forward(ctx, input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
0042:         # Save tensors needed for backward pass
0043:         ctx.save_for_backward(input, weight, bias)
0044:         
0045:         # Compute output: y = xW^T + b
0046:         output = input.data @ weight.data.T
0047:         if bias is not None:
0048:             output = output + bias.data
0049:             
0050:         return Tensor(output)
0051:     
0052:     @staticmethod
0053:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0054:         input, weight, bias = ctx.saved_tensors
0055: 
0056:         if input.requires_grad:
0057:             # For input gradient: (batch_size, out_features) @ (out_features, in_features)
0058:             grad_dict[id(input)] = grad_output @ weight.data
0059: 
0060:         if weight.requires_grad:
0061:             # For weight gradient: (in_features, batch_size) @ (batch_size, out_features)
0062:             # Reshape grad_output to (batch_size, out_features) if needed
0063:             if len(grad_output.shape) == 3:
0064:                 batch_size, seq_len, out_features = grad_output.shape
0065:                 grad_output = grad_output.reshape(-1, out_features)
0066:                 input_data = input.data.reshape(-1, input.data.shape[-1])
0067:             else:
0068:                 input_data = input.data
0069:                 
0070:             grad_dict[id(weight)] = input_data.T @ grad_output
0071: 
0072:         if bias is not None and bias.requires_grad:
0073:             # Sum across batch dimension
0074:             grad_dict[id(bias)] = grad_output.sum(axis=0)
0075:             if len(grad_output.shape) == 3:
0076:                 grad_dict[id(bias)] = grad_dict[id(bias)].sum(axis=0)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\normalization.py
// ----------------------------------------
0001: # normalization.py
0002: from typing import List, Optional, Tuple, Union
0003: import numpy as np
0004: from ..core import Module, Tensor
0005: 
0006: class InstanceNorm2d(Module):
0007:     """Applies Instance Normalization over a 4D input"""
0008:     
0009:     def __init__(self, num_features: int, eps: float = 1e-5, momentum: float = 0.1,
0010:                 affine: bool = True, track_running_stats: bool = False):
0011:         super().__init__()
0012:         self.num_features = num_features
0013:         self.eps = eps
0014:         self.momentum = momentum
0015:         self.affine = affine
0016:         self.track_running_stats = track_running_stats
0017:         
0018:         if self.affine:
0019:             self.weight = Tensor(np.ones(num_features), requires_grad=True)
0020:             self.bias = Tensor(np.zeros(num_features), requires_grad=True)
0021:         else:
0022:             self.register_parameter('weight', None)
0023:             self.register_parameter('bias', None)
0024:             
0025:         if self.track_running_stats:
0026:             self.register_buffer('running_mean', Tensor(np.zeros(num_features)))
0027:             self.register_buffer('running_var', Tensor(np.ones(num_features)))
0028:         else:
0029:             self.register_buffer('running_mean', None)
0030:             self.register_buffer('running_var', None)
0031: 
0032:     def forward(self, x: Tensor) -> Tensor:
0033:         if self.training or not self.track_running_stats:
0034:             # Calculate per-instance statistics
0035:             batch_size, num_channels = x.shape[:2]
0036:             x_reshaped = x.data.reshape(batch_size, num_channels, -1)
0037:             
0038:             mean = x_reshaped.mean(axis=2, keepdims=True)  # (N, C, 1)
0039:             var = x_reshaped.var(axis=2, keepdims=True, ddof=0)  # (N, C, 1)
0040: 
0041:             # Reshape for broadcasting (N, C, 1, 1)
0042:             mean = mean.reshape(batch_size, num_channels, 1, 1)
0043:             var = var.reshape(batch_size, num_channels, 1, 1)
0044: 
0045:             if self.track_running_stats and self.training:
0046:                 # Update running stats with proper dimension handling
0047:                 self.running_mean.data = (
0048:                     (1 - self.momentum) * self.running_mean.data +
0049:                     self.momentum * mean.mean(axis=0).squeeze()  # Remove extra dimensions
0050:                 )
0051:                 self.running_var.data = (
0052:                     (1 - self.momentum) * self.running_var.data +
0053:                     self.momentum * var.mean(axis=0).squeeze()
0054:                 )
0055:         else:
0056:             # Use running stats with correct broadcasting shape
0057:             mean = self.running_mean.data.reshape(1, -1, 1, 1)
0058:             var = self.running_var.data.reshape(1, -1, 1, 1)
0059: 
0060:         # Normalization
0061:         x_norm = (x.data - mean) / np.sqrt(var + self.eps)
0062: 
0063:         # Apply affine transform if enabled
0064:         if self.affine:
0065:             x_norm = x_norm * self.weight.data.reshape(1, -1, 1, 1) + \
0066:                     self.bias.data.reshape(1, -1, 1, 1)
0067: 
0068:         return Tensor(x_norm, requires_grad=x.requires_grad)
0069:         
0070:     def extra_repr(self) -> str:
0071:         return (f'num_features={self.num_features}, eps={self.eps}, momentum={self.momentum}, '
0072:                 f'affine={self.affine}, track_running_stats={self.track_running_stats}')
0073: 
0074: class GroupNorm(Module):
0075:     """
0076:     Applies Group Normalization over a mini-batch of inputs.
0077:     
0078:     Group Normalization divides channels into groups and computes within each group
0079:     the mean and variance for normalization.
0080:     
0081:     Args:
0082:         num_groups: Number of groups to separate the channels into
0083:         num_channels: Number of channels expected in input
0084:         eps: Small value for numerical stability
0085:         affine: If True, use learnable affine parameters
0086:     """
0087:     
0088:     def __init__(self, num_groups: int, num_channels: int, eps: float = 1e-5, affine: bool = True):
0089:         super().__init__()
0090:         
0091:         if num_channels % num_groups != 0:
0092:             raise ValueError('num_channels must be divisible by num_groups')
0093:             
0094:         self.num_groups = num_groups
0095:         self.num_channels = num_channels
0096:         self.eps = eps
0097:         self.affine = affine
0098:         
0099:         if self.affine:
0100:             self.weight = Tensor(np.ones(num_channels), requires_grad=True)
0101:             self.bias = Tensor(np.zeros(num_channels), requires_grad=True)
0102:         else:
0103:             self.register_parameter('weight', None)
0104:             self.register_parameter('bias', None)
0105: 
0106:     def forward(self, x: Tensor) -> Tensor:
0107:         # Check input dimensions
0108:         if len(x.shape) != 4:
0109:             raise ValueError(f'expected 4D input, got {len(x.shape)}D input')
0110:             
0111:         N, C, H, W = x.shape
0112:         if C != self.num_channels:
0113:             raise ValueError(f'expected {self.num_channels} channels, got {C} channels')
0114:             
0115:         # Reshape input to (N, G, C/G, H, W)
0116:         x_reshaped = x.data.reshape(N, self.num_groups, -1, H, W)
0117:         
0118:         # Calculate mean and variance over (C/G, H, W) for each group
0119:         mean = x_reshaped.mean(axis=(2, 3, 4), keepdims=True)
0120:         var = x_reshaped.var(axis=(2, 3, 4), keepdims=True, ddof=0)
0121:         
0122:         # Normalize
0123:         x_norm = (x_reshaped - mean) / np.sqrt(var + self.eps)
0124:         
0125:         # Reshape back to (N, C, H, W)
0126:         x_norm = x_norm.reshape(N, C, H, W)
0127:         
0128:         # Apply affine transform if specified
0129:         if self.affine:
0130:             x_norm = x_norm * self.weight.data.reshape(1, -1, 1, 1) + \
0131:                     self.bias.data.reshape(1, -1, 1, 1)
0132:             
0133:         return Tensor(x_norm, requires_grad=x.requires_grad)
0134:         
0135:     def extra_repr(self) -> str:
0136:         return (f'num_groups={self.num_groups}, num_channels={self.num_channels}, '
0137:                 f'eps={self.eps}, affine={self.affine}')

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\pooling.py
// ----------------------------------------
0001: # pooling.py
0002: from typing import Tuple, Union, Optional
0003: from ..core import Module, Tensor
0004: from ..ops.pooling import MaxPool2dFunction, AvgPool2dFunction
0005: 
0006: class MaxPool2d(Module):
0007:     """2D max pooling layer."""
0008:     
0009:     def __init__(self, kernel_size: Union[int, Tuple[int, int]],
0010:                 stride: Optional[Union[int, Tuple[int, int]]] = None,
0011:                 padding: Union[int, Tuple[int, int]] = 0):
0012:         super().__init__()
0013:         
0014:         # Convert scalar parameters to tuples
0015:         self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size
0016:         self.stride = self.kernel_size if stride is None else \
0017:                     (stride, stride) if isinstance(stride, int) else stride
0018:         self.padding = (padding, padding) if isinstance(padding, int) else padding
0019:         
0020:     def forward(self, x: Tensor) -> Tensor:
0021:         return MaxPool2dFunction.apply(x, self.kernel_size, self.stride, self.padding)
0022:         
0023:     def extra_repr(self) -> str:
0024:         return f'kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding}'
0025: 
0026: class AvgPool2d(Module):
0027:     """2D average pooling layer."""
0028:     
0029:     def __init__(self, kernel_size: Union[int, Tuple[int, int]],
0030:                 stride: Optional[Union[int, Tuple[int, int]]] = None,
0031:                 padding: Union[int, Tuple[int, int]] = 0):
0032:         super().__init__()
0033:         
0034:         # Convert scalar parameters to tuples
0035:         self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size
0036:         self.stride = self.kernel_size if stride is None else \
0037:                     (stride, stride) if isinstance(stride, int) else stride
0038:         self.padding = (padding, padding) if isinstance(padding, int) else padding
0039:         
0040:     def forward(self, x: Tensor) -> Tensor:
0041:         return AvgPool2dFunction.apply(x, self.kernel_size, self.stride, self.padding)
0042:         
0043:     def extra_repr(self) -> str:
0044:         return f'kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding}'

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\rnn.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Optional, Tuple
0003: from ..core import Module, Tensor
0004: 
0005: class LSTM(Module):
0006:     """
0007:     Long Short-Term Memory (LSTM) layer.
0008:     
0009:     Applies a multi-layer LSTM to an input sequence.
0010:     
0011:     Args:
0012:         input_size: Number of features in input
0013:         hidden_size: Number of features in hidden state
0014:         num_layers: Number of recurrent layers
0015:         bias: If False, doesn't learn bias weights
0016:         batch_first: If True, input shape is (batch, seq, feature)
0017:         dropout: Dropout probability between layers (0 means no dropout)
0018:         bidirectional: If True, becomes bidirectional LSTM
0019:         
0020:     Inputs: input, (h_0, c_0)
0021:         input: tensor of shape (seq_len, batch, input_size) 
0022:             or (batch, seq_len, input_size) if batch_first=True
0023:         h_0: initial hidden state (num_layers * num_directions, batch, hidden_size)
0024:         c_0: initial cell state (num_layers * num_directions, batch, hidden_size)
0025:         
0026:     Outputs: output, (h_n, c_n)
0027:         output: tensor of shape (seq_len, batch, num_directions * hidden_size)
0028:         h_n: final hidden state
0029:         c_n: final cell state
0030:     """
0031:     def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1,
0032:                  bias: bool = True, batch_first: bool = False, dropout: float = 0.,
0033:                  bidirectional: bool = False):
0034:         super().__init__()
0035:         if dropout < 0 or dropout > 1:
0036:             raise ValueError(f"Dropout probability has to be between 0 and 1, but got {dropout}")
0037:         self.input_size = input_size
0038:         self.hidden_size = hidden_size
0039:         self.num_layers = num_layers
0040:         self.bias = bias
0041:         self.batch_first = batch_first
0042:         self.dropout = dropout
0043:         self.bidirectional = bidirectional
0044:         num_directions = 2 if bidirectional else 1
0045:         
0046:         # Create parameter tensors for each gate (input, forget, cell, output)
0047:         # for each layer and direction
0048:         self.cell_list = []
0049:         for layer in range(num_layers):
0050:             for direction in range(num_directions):
0051:                 layer_input_size = input_size if layer == 0 else hidden_size * num_directions
0052:                 cell = LSTMCell(layer_input_size, hidden_size, bias)
0053:                 name = f'cell_{layer}_{direction}'
0054:                 setattr(self, name, cell)
0055:                 self.cell_list.append(cell)
0056:                 
0057:     def forward(self, x: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = None) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
0058:         """Forward pass of LSTM."""
0059:         # Handle batch_first
0060:         if self.batch_first:
0061:             x = Tensor(np.swapaxes(x.data, 0, 1))  # Changed to use swapaxes
0062:             
0063:         seq_len, batch_size, _ = x.shape
0064:         num_directions = 2 if self.bidirectional else 1
0065:             
0066:         if hx is None:
0067:             h_0 = Tensor(np.zeros((self.num_layers * num_directions, batch_size, self.hidden_size)))
0068:             c_0 = Tensor(np.zeros((self.num_layers * num_directions, batch_size, self.hidden_size)))
0069:             hx = (h_0, c_0)
0070:         else:
0071:             # Validate hidden state dimensions
0072:             h_0, c_0 = hx
0073:             expected_shape = (self.num_layers * num_directions, batch_size, self.hidden_size)
0074:             if h_0.shape != expected_shape or c_0.shape != expected_shape:
0075:                 raise ValueError(f"Expected hidden size {expected_shape}, got {h_0.shape} and {c_0.shape}")
0076: 
0077:         h_n, c_n = hx
0078:         layer_output = x
0079:         new_h = []
0080:         new_c = []
0081: 
0082:         # Process each layer
0083:         for layer in range(self.num_layers):
0084:             layer_h_list = []
0085:             h_forward = h_n[layer * num_directions]
0086:             c_forward = c_n[layer * num_directions]
0087:             
0088:             # Forward direction
0089:             for t in range(seq_len):
0090:                 h_forward, c_forward = self.cell_list[layer * num_directions](
0091:                     layer_output[t], (h_forward, c_forward))
0092:                 layer_h_list.append(h_forward)
0093:                 
0094:             if self.bidirectional:
0095:                 # Backward direction
0096:                 h_backward = h_n[layer * num_directions + 1]
0097:                 c_backward = c_n[layer * num_directions + 1]
0098:                 layer_h_back = []
0099:                 
0100:                 for t in range(seq_len - 1, -1, -1):
0101:                     h_backward, c_backward = self.cell_list[layer * num_directions + 1](
0102:                         layer_output[t], (h_backward, c_backward))
0103:                     layer_h_back.append(h_backward)
0104:                     
0105:                 # Combine forward and backward outputs
0106:                 layer_h_back.reverse()
0107:                 layer_forward = Tensor(np.stack([h.data for h in layer_h_list]))
0108:                 layer_backward = Tensor(np.stack([h.data for h in layer_h_back]))
0109:                 layer_output = Tensor(np.concatenate([layer_forward.data, layer_backward.data], axis=-1))
0110:             else:
0111:                 layer_output = Tensor(np.stack([h.data for h in layer_h_list]))
0112:                 
0113:             # Apply dropout except for last layer
0114:             if layer < self.num_layers - 1 and self.training and self.dropout > 0:
0115:                 mask = (np.random.rand(*layer_output.shape) > self.dropout).astype(np.float64)
0116:                 layer_output = Tensor(layer_output.data * mask / (1 - self.dropout))
0117:                 
0118:             new_h.append(h_forward.data)
0119:             if self.bidirectional:
0120:                 new_h.append(h_backward.data)
0121:             new_c.append(c_forward.data)
0122:             if self.bidirectional:
0123:                 new_c.append(c_backward.data)
0124:                 
0125:         # Stack hidden states and cell states
0126:         h_n = Tensor(np.stack(new_h))
0127:         c_n = Tensor(np.stack(new_c))
0128:         
0129:         # Restore batch_first if needed
0130:         if self.batch_first:
0131:             layer_output = Tensor(np.swapaxes(layer_output.data, 0, 1))
0132:             
0133:         return layer_output, (h_n, c_n)
0134: 
0135: class GRU(Module):
0136:     """
0137:     Gated Recurrent Unit (GRU) layer.
0138:     
0139:     Applies a multi-layer GRU to an input sequence.
0140:     
0141:     Args:
0142:         input_size: Number of features in input
0143:         hidden_size: Number of features in hidden state
0144:         num_layers: Number of recurrent layers
0145:         bias: If False, doesn't learn bias weights
0146:         batch_first: If True, input shape is (batch, seq, feature)
0147:         dropout: Dropout probability between layers (0 means no dropout)
0148:         bidirectional: If True, becomes bidirectional GRU
0149:         
0150:     Inputs: input, h_0
0151:         input: tensor of shape (seq_len, batch, input_size)
0152:             or (batch, seq_len, input_size) if batch_first=True
0153:         h_0: initial hidden state (num_layers * num_directions, batch, hidden_size)
0154:         
0155:     Outputs: output, h_n
0156:         output: tensor of shape (seq_len, batch, num_directions * hidden_size)
0157:         h_n: final hidden state
0158:     """
0159:     def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1,
0160:                  bias: bool = True, batch_first: bool = False, dropout: float = 0.,
0161:                  bidirectional: bool = False):
0162:         super().__init__()
0163:         if dropout < 0 or dropout > 1:
0164:             raise ValueError(f"Dropout probability has to be between 0 and 1, but got {dropout}")
0165:         self.input_size = input_size
0166:         self.hidden_size = hidden_size
0167:         self.num_layers = num_layers
0168:         self.bias = bias
0169:         self.batch_first = batch_first
0170:         self.dropout = dropout
0171:         self.bidirectional = bidirectional
0172:         num_directions = 2 if bidirectional else 1
0173:         
0174:         # Create parameter tensors for each gate (reset, update, new)
0175:         # for each layer and direction
0176:         self.cell_list = []
0177:         for layer in range(num_layers):
0178:             for direction in range(num_directions):
0179:                 layer_input_size = input_size if layer == 0 else hidden_size * num_directions
0180:                 cell = GRUCell(layer_input_size, hidden_size, bias)
0181:                 name = f'cell_{layer}_{direction}'
0182:                 setattr(self, name, cell)
0183:                 self.cell_list.append(cell)
0184:                 
0185:     def forward(self, x: Tensor, hx: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
0186:         """Forward pass of GRU."""
0187:         # Handle batch_first
0188:         if self.batch_first:
0189:             x = Tensor(np.swapaxes(x.data, 0, 1))
0190:             
0191:         seq_len, batch_size, _ = x.shape
0192:         num_directions = 2 if self.bidirectional else 1
0193:             
0194:         if hx is None:
0195:             hx = Tensor(np.zeros((self.num_layers * num_directions, batch_size, self.hidden_size)))
0196:         else:
0197:             # Validate hidden state dimensions
0198:             expected_shape = (self.num_layers * num_directions, batch_size, self.hidden_size)
0199:             if hx.shape != expected_shape:
0200:                 raise ValueError(f"Expected hidden size {expected_shape}, got {hx.shape}")
0201: 
0202:         layer_output = x
0203:         new_h = []
0204:         
0205:         # Process each layer
0206:         for layer in range(self.num_layers):
0207:             layer_h_list = []
0208:             h_forward = hx[layer * num_directions]
0209:             
0210:             # Forward direction
0211:             for t in range(seq_len):
0212:                 h_forward = self.cell_list[layer * num_directions](layer_output[t], h_forward)
0213:                 layer_h_list.append(h_forward)
0214:                 
0215:             if self.bidirectional:
0216:                 # Backward direction
0217:                 h_backward = hx[layer * num_directions + 1]
0218:                 layer_h_back = []
0219:                 
0220:                 for t in range(seq_len - 1, -1, -1):
0221:                     h_backward = self.cell_list[layer * num_directions + 1](
0222:                         layer_output[t], h_backward)
0223:                     layer_h_back.append(h_backward)
0224:                     
0225:                 # Combine forward and backward outputs
0226:                 layer_h_back.reverse()
0227:                 layer_forward = Tensor(np.stack([h.data for h in layer_h_list]))
0228:                 layer_backward = Tensor(np.stack([h.data for h in layer_h_back]))
0229:                 layer_output = Tensor(np.concatenate([layer_forward.data, layer_backward.data], axis=-1))
0230:             else:
0231:                 layer_output = Tensor(np.stack([h.data for h in layer_h_list]))
0232:                 
0233:             # Apply dropout except for last layer
0234:             if layer < self.num_layers - 1 and self.training and self.dropout > 0:
0235:                 mask = (np.random.rand(*layer_output.shape) > self.dropout).astype(np.float64)
0236:                 layer_output = Tensor(layer_output.data * mask / (1 - self.dropout))
0237:                 
0238:             new_h.append(h_forward.data)
0239:             if self.bidirectional:
0240:                 new_h.append(h_backward.data)
0241:                 
0242:         # Stack hidden states
0243:         h_n = Tensor(np.stack(new_h))
0244:         
0245:         # Restore batch_first if needed
0246:         if self.batch_first:
0247:             layer_output = Tensor(np.swapaxes(layer_output.data, 0, 1))
0248:             
0249:         return layer_output, h_n
0250: 
0251: class LSTMCell(Module):
0252:     """
0253:     A single LSTM cell.
0254:     
0255:     Args:
0256:         input_size: Number of features in input
0257:         hidden_size: Number of features in hidden state
0258:         bias: If False, doesn't learn bias weights
0259:     """
0260:     def __init__(self, input_size: int, hidden_size: int, bias: bool = True):
0261:         super().__init__()
0262:         self.input_size = input_size
0263:         self.hidden_size = hidden_size
0264:         
0265:         # Create weight matrices for the four gates
0266:         self.weight_ih = Tensor(
0267:             np.random.randn(4 * hidden_size, input_size) / np.sqrt(input_size),
0268:             requires_grad=True
0269:         )
0270:         self.weight_hh = Tensor(
0271:             np.random.randn(4 * hidden_size, hidden_size) / np.sqrt(hidden_size),
0272:             requires_grad=True
0273:         )
0274:         
0275:         if bias:
0276:             self.bias_ih = Tensor(np.zeros(4 * hidden_size), requires_grad=True)
0277:             self.bias_hh = Tensor(np.zeros(4 * hidden_size), requires_grad=True)
0278:         else:
0279:             self.register_parameter('bias_ih', None)
0280:             self.register_parameter('bias_hh', None)
0281:             
0282:     def forward(self, x: Tensor, 
0283:                 hx: Optional[Tuple[Tensor, Tensor]] = None) -> Tuple[Tensor, Tensor]:
0284:         """Forward pass of LSTM cell."""
0285:         if hx is None:
0286:             hx = (Tensor(np.zeros((x.shape[0], self.hidden_size))),
0287:                   Tensor(np.zeros((x.shape[0], self.hidden_size))))
0288:             
0289:         h, c = hx
0290:         
0291:         # Calculate gates
0292:         gates = (x @ self.weight_ih.t())
0293:         if self.bias_ih is not None:
0294:             gates = gates + self.bias_ih
0295:             
0296:         gates = gates + (h @ self.weight_hh.t())
0297:         if self.bias_hh is not None:
0298:             gates = gates + self.bias_hh
0299:             
0300:         # Split into individual gates
0301:         chunk_size = gates.shape[-1] // 4
0302:         i, f, g, o = np.split(gates.data, 4, axis=-1)
0303:         
0304:         # Apply gate activations
0305:         i = 1 / (1 + np.exp(-i))  # input gate
0306:         f = 1 / (1 + np.exp(-f))  # forget gate
0307:         g = np.tanh(g)           # cell gate
0308:         o = 1 / (1 + np.exp(-o))  # output gate
0309:         
0310:         # Update cell state and hidden state
0311:         c_next = f * c.data + i * g
0312:         h_next = o * np.tanh(c_next)
0313:         
0314:         return Tensor(h_next), Tensor(c_next)
0315: 
0316: class GRUCell(Module):
0317:     """
0318:     A single GRU cell.
0319:     
0320:     Args:
0321:         input_size: Number of features in input
0322:         hidden_size: Number of features in hidden state
0323:         bias: If False, doesn't learn bias weights
0324:     """
0325:     def __init__(self, input_size: int, hidden_size: int, bias: bool = True):
0326:         super().__init__()
0327:         self.input_size = input_size
0328:         self.hidden_size = hidden_size
0329:         
0330:         # Create weight matrices for the three gates (reset, update, new)
0331:         self.weight_ih = Tensor(
0332:             np.random.randn(3 * hidden_size, input_size) / np.sqrt(input_size),
0333:             requires_grad=True
0334:         )
0335:         self.weight_hh = Tensor(
0336:             np.random.randn(3 * hidden_size, hidden_size) / np.sqrt(hidden_size),
0337:             requires_grad=True
0338:         )
0339:         
0340:         if bias:
0341:             self.bias_ih = Tensor(np.zeros(3 * hidden_size), requires_grad=True)
0342:             self.bias_hh = Tensor(np.zeros(3 * hidden_size), requires_grad=True)
0343:         else:
0344:             self.register_parameter('bias_ih', None)
0345:             self.register_parameter('bias_hh', None)
0346:             
0347:     def forward(self, x: Tensor, hx: Optional[Tensor] = None) -> Tensor:
0348:         """
0349:         Forward pass of GRU cell.
0350:         
0351:         Args:
0352:             x: Input tensor of shape (batch, input_size)
0353:             hx: Hidden state tensor of shape (batch, hidden_size)
0354:             
0355:         Returns:
0356:             New hidden state tensor of shape (batch, hidden_size)
0357:         """
0358:         if hx is None:
0359:             hx = Tensor(np.zeros((x.shape[0], self.hidden_size)))
0360:             
0361:         # Calculate gates
0362:         gates_x = (x @ self.weight_ih.t())
0363:         if self.bias_ih is not None:
0364:             gates_x = gates_x + self.bias_ih
0365:             
0366:         gates_h = (hx @ self.weight_hh.t())
0367:         if self.bias_hh is not None:
0368:             gates_h = gates_h + self.bias_hh
0369:             
0370:         # Split into individual gates
0371:         chunk_size = gates_x.shape[-1] // 3
0372:         r_x, z_x, n_x = np.split(gates_x.data, 3, axis=-1)  # reset, update, new
0373:         r_h, z_h, n_h = np.split(gates_h.data, 3, axis=-1)
0374:         
0375:         # Compute gate values
0376:         r = 1 / (1 + np.exp(-(r_x + r_h)))      # reset gate
0377:         z = 1 / (1 + np.exp(-(z_x + z_h)))      # update gate
0378:         n = np.tanh(n_x + r * n_h)              # new gate
0379:         
0380:         # Compute new hidden state
0381:         h_next = (1 - z) * hx.data + z * n
0382:         
0383:         return Tensor(h_next)
0384:         
0385:     def extra_repr(self) -> str:
0386:         """Returns a string with extra representation information."""
0387:         return f'input_size={self.input_size}, hidden_size={self.hidden_size}'

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\sequential.py
// ----------------------------------------
0001: # DLpy/nn/sequential.py
0002: 
0003: from typing import List, Union, Iterator
0004: from ..core import Module, Tensor
0005: from collections import OrderedDict
0006: 
0007: class Sequential(Module):
0008:     """
0009:     A sequential container.
0010:     Modules will be added to it in the order they are passed in the constructor.
0011:     Alternatively, an ordered dict of modules can be passed in.
0012:     
0013:     Example usage:
0014:     >>> net = Sequential(
0015:     >>>     Linear(10, 5),
0016:     >>>     ReLU(),
0017:     >>>     Linear(5, 1)
0018:     >>> )
0019:     """
0020:     
0021:     def __init__(self, *args):
0022:         super().__init__()
0023:         if len(args) == 1 and isinstance(args[0], OrderedDict):
0024:             for key, module in args[0].items():
0025:                 self.add_module(key, module)
0026:         else:
0027:             for idx, module in enumerate(args):
0028:                 self.add_module(str(idx), module)
0029:                 
0030:     def forward(self, x: Tensor) -> Tensor:
0031:         """Forward pass through all modules in sequence."""
0032:         for module in self._modules.values():
0033:             x = module(x)
0034:         return x
0035:         
0036:     def __getitem__(self, idx: Union[slice, int]) -> Union['Sequential', Module]:
0037:         """Get a module or slice of modules."""
0038:         if isinstance(idx, slice):
0039:             return Sequential(OrderedDict(list(self._modules.items())[idx]))
0040:         else:
0041:             return list(self._modules.values())[idx]
0042:             
0043:     def __len__(self) -> int:
0044:         """Return number of modules."""
0045:         return len(self._modules)
0046:         
0047:     def __iter__(self) -> Iterator[Module]:
0048:         """Iterate over modules."""
0049:         return iter(self._modules.values())
0050:         
0051:     def append(self, module: Module) -> None:
0052:         """Add a module to the end."""
0053:         self.add_module(str(len(self)), module)
0054:         
0055:     def insert(self, index: int, module: Module) -> None:
0056:         """Insert a module at specified index."""
0057:         # Shift existing modules
0058:         for i in range(len(self) - 1, index - 1, -1):
0059:             if str(i) in self._modules:
0060:                 self._modules[str(i + 1)] = self._modules[str(i)]
0061:                 del self._modules[str(i)]
0062:                 
0063:         # Insert new module
0064:         self.add_module(str(index), module)
0065:         
0066:     def extend(self, modules: List[Module]) -> None:
0067:         """Add multiple modules to the end."""
0068:         for module in modules:
0069:             self.append(module)
0070:             
0071:     def pop(self, index: int = -1) -> Module:
0072:         """Remove and return module at index."""
0073:         if index < 0:
0074:             index = len(self) + index
0075:             
0076:         if not 0 <= index < len(self):
0077:             raise IndexError("Index out of range")
0078:             
0079:         module = self[index]
0080:         del self._modules[str(index)]
0081:         
0082:         # Shift remaining modules
0083:         for i in range(index + 1, len(self) + 1):
0084:             if str(i) in self._modules:
0085:                 self._modules[str(i - 1)] = self._modules[str(i)]
0086:                 del self._modules[str(i)]
0087:                 
0088:         return module

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\transformer.py
// ----------------------------------------
0001: from typing import Optional, Tuple
0002: import numpy as np
0003: from ..core import Module, Tensor
0004: from ..nn.linear import Linear
0005: from ..nn.layer_norm import LayerNorm
0006: from ..nn.dropout import Dropout
0007: from ..nn.sequential import Sequential
0008: from ..nn.activations import ReLU
0009: 
0010: class MultiHeadAttention(Module):
0011:     """
0012:     Multi-head attention mechanism.
0013:     
0014:     This module splits the input into multiple heads, applies scaled dot-product
0015:     attention independently on each head, and then concatenates the results.
0016:     
0017:     Args:
0018:         embed_dim (int): Total dimension of the model
0019:         num_heads (int): Number of parallel attention heads
0020:         dropout (float): Dropout probability (optional)
0021:         bias (bool): If True, use bias in linear layers
0022:     """
0023:     def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.0,
0024:                  bias: bool = True):
0025:         super().__init__()
0026:         
0027:         if embed_dim % num_heads != 0:
0028:             raise ValueError(
0029:                 f"Embedding dimension {embed_dim} not divisible by num_heads {num_heads}"
0030:             )
0031:             
0032:         self.embed_dim = embed_dim
0033:         self.num_heads = num_heads
0034:         self.dropout = dropout
0035:         self.head_dim = embed_dim // num_heads
0036:         self.scaling = self.head_dim ** -0.5
0037:         
0038:         self.w_q = Linear(embed_dim, embed_dim, bias=bias)
0039:         self.w_k = Linear(embed_dim, embed_dim, bias=bias)
0040:         self.w_v = Linear(embed_dim, embed_dim, bias=bias)
0041:         self.w_o = Linear(embed_dim, embed_dim, bias=bias)
0042:         
0043:     def _reshape_for_heads(self, x: Tensor) -> Tensor:
0044:         """Reshapes input for parallel head processing."""
0045:         batch_size, seq_len, _ = x.shape
0046:         # First reshape to separate head dimensions
0047:         x = x.reshape(batch_size, seq_len, self.num_heads, self.head_dim)
0048:         # Then transpose dimensions to (batch_size, num_heads, seq_len, head_dim)
0049:         x = x.transpose(0, 2, 1, 3)  # Note: passing axes as a tuple
0050:         return x
0051:         
0052:     def forward(self, query: Tensor, key: Tensor, value: Tensor,
0053:                 attention_mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
0054:         """
0055:         Forward pass of multi-head attention.
0056:         
0057:         Args:
0058:             query: Query tensor of shape (batch_size, query_len, embed_dim)
0059:             key: Key tensor of shape (batch_size, key_len, embed_dim)
0060:             value: Value tensor of shape (batch_size, key_len, embed_dim)
0061:             attention_mask: Optional mask tensor of shape (batch_size, num_heads, query_len, key_len)
0062:             
0063:         Returns:
0064:             Tuple of:
0065:                 - Output tensor of shape (batch_size, query_len, embed_dim)
0066:                 - Attention weights of shape (batch_size, num_heads, query_len, key_len)
0067:         """
0068:         batch_size, query_len, _ = query.shape
0069:         _, key_len, _ = key.shape
0070: 
0071:         # Linear projections
0072:         q = self.w_q(query)  # (batch_size, query_len, embed_dim)
0073:         k = self.w_k(key)    # (batch_size, key_len, embed_dim)
0074:         v = self.w_v(value)  # (batch_size, key_len, embed_dim)
0075: 
0076:         # Reshape for multi-head attention
0077:         q = self._reshape_for_heads(q)  # (batch_size, num_heads, query_len, head_dim)
0078:         k = self._reshape_for_heads(k)
0079:         v = self._reshape_for_heads(v)
0080: 
0081:         # Scale query
0082:         q = q * self.scaling
0083: 
0084:         # Compute attention scores
0085:         k_t = k.transpose(0, 1, 3, 2)  # (batch_size, num_heads, head_dim, key_len)
0086:         attention_scores = q @ k_t      # (batch_size, num_heads, query_len, key_len)
0087: 
0088:         if attention_mask is not None:
0089:             # Apply the mask first
0090:             attention_scores = attention_scores + attention_mask
0091: 
0092:             # For numerical stability, subtract max after applying the mask
0093:             attention_scores = attention_scores - attention_scores.max(axis=-1, keepdims=True)
0094: 
0095:             # Clip to avoid overflow (though unlikely after subtraction)
0096:             attention_scores = attention_scores.clip(-1e30, 1e30)
0097: 
0098:         # Apply softmax to get attention weights
0099:         attention_weights = attention_scores.softmax(dim=-1)
0100: 
0101:         if self.training and self.dropout > 0:
0102:             attention_weights = Dropout(self.dropout)(attention_weights)
0103: 
0104:         # Apply attention to values
0105:         output = attention_weights @ v  # (batch_size, num_heads, query_len, head_dim)
0106: 
0107:         # Reshape back to original dimensions
0108:         output = output.transpose(0, 2, 1, 3)  # (batch_size, query_len, num_heads, head_dim)
0109:         output = output.reshape(batch_size, query_len, self.embed_dim)
0110: 
0111:         # Final linear projection
0112:         output = self.w_o(output)
0113: 
0114:         return output, attention_weights
0115: 
0116: class TransformerEncoderLayer(Module):
0117:     """
0118:     Transformer Encoder Layer.
0119:     
0120:     Implements a single layer of the transformer encoder, consisting of:
0121:     1. Multi-head self-attention
0122:     2. Add & Norm
0123:     3. Feed-forward network
0124:     4. Add & Norm
0125:     
0126:     Args:
0127:         d_model (int): The dimension of the model
0128:         nhead (int): Number of attention heads
0129:         dim_feedforward (int): Dimension of feedforward network
0130:         dropout (float): Dropout probability
0131:         activation (str): Activation function to use
0132:         layer_norm_eps (float): eps value in layer normalizations
0133:     """
0134:     def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048,
0135:                  dropout: float = 0.1, activation: str = "relu",
0136:                  layer_norm_eps: float = 1e-5):
0137:         super().__init__()
0138:         
0139:         self.self_attn = MultiHeadAttention(d_model, nhead, dropout)
0140:         
0141:         # Create ReLU activation as a Module instance
0142:         self.activation = ReLU()
0143:         
0144:         # Fix Sequential layer construction
0145:         self.ff = Sequential(
0146:             Linear(d_model, dim_feedforward),
0147:             self.activation,  # Use the Module instance
0148:             Dropout(dropout),
0149:             Linear(dim_feedforward, d_model)
0150:         )
0151:         
0152:         self.norm1 = LayerNorm([d_model], eps=layer_norm_eps)
0153:         self.norm2 = LayerNorm([d_model], eps=layer_norm_eps)
0154:         self.dropout = Dropout(dropout)
0155:         
0156:     def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:
0157:         attn_output, _ = self.self_attn(x, x, x, mask)
0158:         x = x + self.dropout(attn_output)
0159:         x = self.norm1(x)
0160:         
0161:         ff_output = self.ff(x)
0162:         x = x + self.dropout(ff_output)
0163:         x = self.norm2(x)
0164:         
0165:         return x
0166: 
0167: class PositionalEncoding(Module):
0168:     """
0169:     Positional Encoding module.
0170:     
0171:     Adds positional information to the input embeddings using sine and cosine
0172:     functions of different frequencies.
0173:     
0174:     Args:
0175:         d_model (int): Dimension of the model
0176:         max_len (int): Maximum sequence length
0177:         dropout (float): Dropout probability
0178:     """
0179:     def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):
0180:         super().__init__()
0181:         self.dropout = Dropout(dropout)
0182:         
0183:         # Create positional encoding matrix
0184:         pe = np.zeros((max_len, d_model))
0185:         position = np.arange(max_len)[:, np.newaxis]
0186:         div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))
0187:         
0188:         pe[:, 0::2] = np.sin(position * div_term)
0189:         pe[:, 1::2] = np.cos(position * div_term)
0190:         
0191:         # Register buffer (not a parameter)
0192:         self.register_buffer('pe', Tensor(pe[np.newaxis, :, :]))
0193:         
0194:     def forward(self, x: Tensor) -> Tensor:
0195:         """
0196:         Args:
0197:             x: Input tensor of shape (batch_size, seq_len, d_model)
0198:             
0199:         Returns:
0200:             Output tensor of shape (batch_size, seq_len, d_model)
0201:         """
0202:         x = x + self.pe[:, :x.shape[1]]
0203:         return self.dropout(x)
0204: 
0205: class TransformerEncoder(Module):
0206:     """
0207:     Transformer Encoder.
0208:     
0209:     A stack of N encoder layers.
0210:     
0211:     Args:
0212:         encoder_layer: An instance of TransformerEncoderLayer
0213:         num_layers (int): Number of encoder layers in the stack
0214:         norm (Module, optional): Layer normalization component
0215:     """
0216:     def __init__(self, encoder_layer: TransformerEncoderLayer, num_layers: int,
0217:                  norm: Optional[Module] = None):
0218:         super().__init__()
0219:         self.layers = Sequential(
0220:             *[encoder_layer for _ in range(num_layers)]
0221:         )
0222:         self.norm = norm
0223:         
0224:     def forward(self, src: Tensor, mask: Optional[Tensor] = None) -> Tensor:
0225:         """
0226:         Forward pass of transformer encoder.
0227:         
0228:         Args:
0229:             src: Source sequence of shape (batch_size, seq_len, d_model)
0230:             mask: Optional attention mask
0231:             
0232:         Returns:
0233:             Output tensor of shape (batch_size, seq_len, d_model)
0234:         """
0235:         output = src
0236:         
0237:         for layer in self.layers:
0238:             output = layer(output, mask)
0239:             
0240:         if self.norm is not None:
0241:             output = self.norm(output)
0242:             
0243:         return output

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\__init__.py
// ----------------------------------------
0001: """
0002: Operations module for DLpy.
0003: 
0004: This module contains all the mathematical operations that can be performed on tensors.
0005: """
0006: 
0007: from .basic import Add, Multiply, MatMul, Softmax, Clip
0008: from .reshape import Reshape
0009: from .power import Power, Divide
0010: from .elementwise import Log, Exp
0011: from .reduction import Sum, Mean, Max
0012: from .matrix import (
0013:     Transpose,
0014:     Greater,
0015:     GreaterEqual,
0016:     Less,
0017:     LessEqual,
0018:     Equal,
0019:     NotEqual
0020: )
0021: from .loss import (
0022:     MSELoss,
0023:     CrossEntropyLoss,
0024:     BinaryCrossEntropyLoss,
0025:     L1Loss,
0026:     HuberLoss,
0027:     KLDivLoss,
0028:     CosineSimilarityLoss,
0029:     HingeLoss,
0030:     FocalLoss
0031: )
0032: 
0033: from .cnn import Conv2dFunction
0034: from .pooling import MaxPool2dFunction, AvgPool2dFunction
0035: 
0036: __all__ = [
0037:     # Basic operations
0038:     'Add',
0039:     'Multiply',
0040:     'MatMul',
0041:     'Reshape',
0042:     'Softmax',
0043:     'Clip',
0044:     
0045:     # Power operations
0046:     'Power',
0047:     'Divide',
0048:     
0049:     # Element-wise operations
0050:     'Log',
0051:     'Exp',
0052:     
0053:     # Reduction operations
0054:     'Sum',
0055:     'Mean',
0056:     'Max',
0057:     
0058:     # Matrix operations
0059:     'Transpose',
0060:     
0061:     # Comparison operations
0062:     'Greater',
0063:     'GreaterEqual',
0064:     'Less',
0065:     'LessEqual',
0066:     'Equal',
0067:     'NotEqual',
0068: 
0069:     # Loss functions
0070:     'MSELoss',
0071:     'CrossEntropyLoss',
0072:     'BinaryCrossEntropyLoss',
0073:     'L1Loss',
0074:     'HuberLoss',
0075:     'KLDivLoss',
0076:     'CosineSimilarityLoss',
0077:     'HingeLoss',
0078:     'FocalLoss',
0079: 
0080:     # CNN operations
0081:     'Conv2dFunction',
0082: 
0083:     # Pooling operations
0084:     'MaxPool2dFunction',
0085:     'AvgPool2dFunction'
0086: ]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\basic.py
// ----------------------------------------
0001: from typing import Dict
0002: from ..core.function import Function
0003: from ..core.tensor import Tensor
0004: import numpy as np
0005: 
0006: class Add(Function):
0007:     @staticmethod
0008:     def forward(ctx, a, b):
0009:         if not isinstance(a, Tensor):
0010:             a = Tensor(a)
0011:         if not isinstance(b, Tensor):
0012:             b = Tensor(b)
0013:             
0014:         shape_a = a.data.shape
0015:         shape_b = b.data.shape
0016: 
0017:         # Check valid broadcasting manually
0018:         if len(shape_a) == 2 and shape_a[0] == 1 and len(shape_b) == 1:
0019:             # Special case: (1,N) matrix with (M,) vector requires N==M
0020:             if shape_a[1] != shape_b[0]:
0021:                 raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
0022:         elif len(shape_a) == 1 and len(shape_b) == 2 and shape_b[0] == 1:
0023:             # Special case: (N,) vector with (1,M) matrix requires N==M
0024:             if shape_a[0] != shape_b[1]:
0025:                 raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
0026:                 
0027:         # Save tensors for backward pass
0028:         ctx.save_for_backward(a, b)
0029:         
0030:         # If we get here, try the operation
0031:         try:
0032:             result = a.data + b.data
0033:             return Tensor(result)
0034:         except ValueError:
0035:             raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
0036:         
0037:     @staticmethod
0038:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0039:         a, b = ctx.saved_tensors
0040: 
0041:         if a.requires_grad:
0042:             grad_a = grad_output
0043:             grad_a = Add._reduce_grad(grad_a, a.data.shape)
0044:             if id(a) not in grad_dict or grad_dict[id(a)] is None:
0045:                 grad_dict[id(a)] = grad_a
0046:             else:
0047:                 grad_dict[id(a)] += grad_a  # Accumulate gradients
0048: 
0049:         if b.requires_grad:
0050:             grad_b = grad_output
0051:             grad_b = Add._reduce_grad(grad_b, b.data.shape)
0052:             if id(b) not in grad_dict or grad_dict[id(b)] is None:
0053:                 grad_dict[id(b)] = grad_b
0054:             else:
0055:                 grad_dict[id(b)] += grad_b  # Accumulate gradients
0056: 
0057:     @staticmethod
0058:     def _reduce_grad(grad, target_shape):
0059:         """
0060:         Reduces the gradient to match the target shape by summing over broadcasted dimensions.
0061:         """
0062:         # Convert target_shape to a tuple if it's not
0063:         if not isinstance(target_shape, tuple):
0064:             target_shape = tuple(target_shape)
0065:         
0066:         # Align the dimensions by prepending 1s if necessary
0067:         grad_shape = grad.shape
0068:         target_shape = (1,) * (len(grad_shape) - len(target_shape)) + target_shape
0069:         for axis, (grad_dim, target_dim) in enumerate(zip(grad_shape, target_shape)):
0070:             if target_dim == 1 and grad_dim != 1:
0071:                 grad = grad.sum(axis=axis, keepdims=True)
0072:         return grad
0073: 
0074: class Multiply(Function):
0075:     @staticmethod
0076:     def forward(ctx, a, b):
0077:         if not isinstance(a, Tensor):
0078:             a = Tensor(a)
0079:         if not isinstance(b, Tensor):
0080:             b = Tensor(b)
0081:             
0082:         shape_a = a.data.shape
0083:         shape_b = b.data.shape
0084:         
0085:         # Check if shapes can be broadcast according to NumPy rules
0086:         try:
0087:             # Test broadcast compatibility without actually performing the operation
0088:             np.broadcast_shapes(shape_a, shape_b)
0089:             # If we get here, shapes are compatible
0090:             result = a.data * b.data
0091:             ctx.save_for_backward(a, b)
0092:             return Tensor(result)
0093:         except ValueError:
0094:             raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
0095:         
0096:     @staticmethod
0097:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0098:         a, b = ctx.saved_tensors
0099: 
0100:         if a.requires_grad:
0101:             grad_a = grad_output * b.data
0102:             grad_a = Multiply._reduce_grad(grad_a, a.data.shape)
0103:             if id(a) not in grad_dict or grad_dict[id(a)] is None:
0104:                 grad_dict[id(a)] = grad_a
0105:             else:
0106:                 grad_dict[id(a)] += grad_a  # Accumulate gradients
0107: 
0108:         if b.requires_grad:
0109:             grad_b = grad_output * a.data
0110:             grad_b = Multiply._reduce_grad(grad_b, b.data.shape)
0111:             if id(b) not in grad_dict or grad_dict[id(b)] is None:
0112:                 grad_dict[id(b)] = grad_b
0113:             else:
0114:                 grad_dict[id(b)] += grad_b  # Accumulate gradients
0115: 
0116:     @staticmethod
0117:     def _reduce_grad(grad, target_shape):
0118:         """
0119:         Reduces the gradient to match the target shape by summing over broadcasted dimensions.
0120:         """
0121:         # Convert target_shape to a tuple if it's not
0122:         if not isinstance(target_shape, tuple):
0123:             target_shape = tuple(target_shape)
0124:         
0125:         # Align the dimensions by prepending 1s if necessary
0126:         grad_shape = grad.shape
0127:         target_shape = (1,) * (len(grad_shape) - len(target_shape)) + target_shape
0128:         for axis, (grad_dim, target_dim) in enumerate(zip(grad_shape, target_shape)):
0129:             if target_dim == 1 and grad_dim != 1:
0130:                 grad = grad.sum(axis=axis, keepdims=True)
0131:         return grad
0132: 
0133: class MatMul(Function):
0134:     """Matrix multiplication operation with support for batched operations."""
0135:     
0136:     @staticmethod
0137:     def forward(ctx, a, b):
0138:         if not isinstance(a, Tensor):
0139:             a = Tensor(a)
0140:         if not isinstance(b, Tensor):
0141:             b = Tensor(b)
0142:             
0143:         ctx.save_for_backward(a, b)
0144:         return Tensor(np.matmul(a.data, b.data))
0145:         
0146:     @staticmethod
0147:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0148:         a, b = ctx.saved_tensors
0149:         
0150:         if a.requires_grad:
0151:             # Handle batched matrix multiplication for gradients
0152:             if len(grad_output.shape) == len(b.data.shape):
0153:                 grad_a = np.matmul(grad_output, b.data.swapaxes(-1, -2))
0154:             else:
0155:                 # Reshape grad_output to match dimensions if necessary
0156:                 grad_a = np.matmul(grad_output.reshape(-1, grad_output.shape[-1]), 
0157:                                  b.data.reshape(-1, b.data.shape[-1]).T)
0158:                 grad_a = grad_a.reshape(grad_output.shape[:-1] + (b.data.shape[-2],))
0159:                 
0160:             if id(a) not in grad_dict:
0161:                 grad_dict[id(a)] = grad_a
0162:             else:
0163:                 grad_dict[id(a)] += grad_a
0164:                 
0165:         if b.requires_grad:
0166:             # Handle batched matrix multiplication for gradients
0167:             if len(grad_output.shape) == len(a.data.shape):
0168:                 grad_b = np.matmul(a.data.swapaxes(-1, -2), grad_output)
0169:             else:
0170:                 # Reshape grad_output to match dimensions if necessary
0171:                 grad_b = np.matmul(a.data.reshape(-1, a.data.shape[-2]).T,
0172:                                  grad_output.reshape(-1, grad_output.shape[-1]))
0173:                 grad_b = grad_b.reshape(a.data.shape[:-2] + (a.data.shape[-1], grad_output.shape[-1]))
0174:                 
0175:             if id(b) not in grad_dict:
0176:                 grad_dict[id(b)] = grad_b
0177:             else:
0178:                 grad_dict[id(b)] += grad_b
0179: 
0180: class Softmax(Function):
0181:     """Softmax activation function operation."""
0182:     
0183:     @staticmethod
0184:     def forward(ctx, x, dim=-1):
0185:         if not isinstance(x, Tensor):
0186:             x = Tensor(x)
0187:             
0188:         # Convert negative dim to positive for consistency
0189:         if dim < 0:
0190:             dim = len(x.data.shape) + dim
0191:             
0192:         # Compute max along specified dimension for numerical stability
0193:         x_max = np.max(x.data, axis=dim, keepdims=True)
0194:         exp_x = np.exp(x.data - x_max)
0195:         softmax_out = exp_x / np.sum(exp_x, axis=dim, keepdims=True)
0196:         
0197:         # Save inputs and outputs for backward pass
0198:         ctx.save_for_backward(x, Tensor(softmax_out))
0199:         ctx.dim = dim  # Save dim as a regular integer
0200:         
0201:         return Tensor(softmax_out)
0202:         
0203:     @staticmethod
0204:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0205:         x, softmax_out = ctx.saved_tensors
0206:         dim = ctx.dim  # Get dim from context as regular integer
0207:         
0208:         if x.requires_grad:
0209:             # Gradient of softmax:
0210:             # dsoftmax_i/dx_j = softmax_i * (1{i=j} - softmax_j)
0211:             grad_x = softmax_out.data * (grad_output - 
0212:                                        np.sum(grad_output * softmax_out.data, axis=dim, keepdims=True))
0213:             
0214:             if id(x) not in grad_dict:
0215:                 grad_dict[id(x)] = grad_x
0216:             else:
0217:                 grad_dict[id(x)] += grad_x
0218: 
0219: class Clip(Function):
0220:     """Clips tensor values between minimum and maximum."""
0221:     
0222:     @staticmethod
0223:     def forward(ctx, x, min_val, max_val):
0224:         if not isinstance(x, Tensor):
0225:             x = Tensor(x)
0226:             
0227:         # Save input and clip values for backward pass
0228:         ctx.save_for_backward(x)
0229:         ctx.min_val = min_val
0230:         ctx.max_val = max_val
0231:         
0232:         return Tensor(np.clip(x.data, min_val, max_val))
0233:         
0234:     @staticmethod
0235:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0236:         x, = ctx.saved_tensors
0237:         min_val = ctx.min_val
0238:         max_val = ctx.max_val
0239:         
0240:         if x.requires_grad:
0241:             # Gradient is zero where input was clipped
0242:             grad = grad_output * ((x.data >= min_val) & (x.data <= max_val))
0243:             
0244:             if id(x) not in grad_dict:
0245:                 grad_dict[id(x)] = grad
0246:             else:
0247:                 grad_dict[id(x)] += grad  # Accumulate gradients

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\cnn.py
// ----------------------------------------
0001: from typing import Tuple, Dict, Optional, Union, List
0002: import numpy as np
0003: from ..core import Function, Tensor, Context
0004: 
0005: class ConvMode:
0006:     """Enumeration of convolution modes."""
0007:     STANDARD = "standard"
0008:     TRANSPOSED = "transposed"
0009:     DEFORMABLE = "deformable"
0010: 
0011: def _validate_conv_params(
0012:     x_shape: tuple,
0013:     weight_shape: tuple,
0014:     stride: tuple,
0015:     padding: tuple,
0016:     dilation: tuple,
0017:     groups: int,
0018:     mode: str = ConvMode.STANDARD,
0019:     offset: Optional[Tensor] = None,
0020:     weight: Optional[Tensor] = None,
0021:     mask: Optional[Tensor] = None
0022: ) -> None:
0023:     """Validates convolution parameters."""
0024:     N, C_in, H, W = x_shape
0025:     C_out, C_in_per_group, kH, kW = weight_shape
0026: 
0027:     # Basic validations for all modes
0028:     if mode not in [ConvMode.STANDARD, ConvMode.TRANSPOSED, ConvMode.DEFORMABLE]:
0029:         raise ValueError(f"Invalid convolution mode: {mode}")
0030: 
0031:     # Validate groups configuration
0032:     if C_in % groups != 0:
0033:         raise ValueError(f"Input channels ({C_in}) must be divisible by groups ({groups})")
0034:     if C_out % groups != 0:
0035:         raise ValueError(f"Output channels ({C_out}) must be divisible by groups ({groups})")
0036: 
0037:     # Validate kernel dimensions
0038:     if kH <= 0 or kW <= 0:
0039:         raise ValueError(f"Kernel dimensions must be positive, got ({kH}, {kW})")
0040: 
0041:     # Validate stride and dilation
0042:     if any(s <= 0 for s in stride):
0043:         raise ValueError(f"Stride values must be positive, got {stride}")
0044:     if any(d <= 0 for d in dilation):
0045:         raise ValueError(f"Dilation values must be positive, got {dilation}")
0046:     
0047:     # Validate padding
0048:     if any(p < 0 for p in padding):
0049:         raise ValueError(f"Padding values must be non-negative, got {padding}")
0050: 
0051:     if mode == ConvMode.TRANSPOSED:
0052:         # For transposed conv:
0053:         # - x shape is (N, C_in, H, W)
0054:         # - weight shape should be (C_out, C_in, kH, kW)
0055:         # - C_in from x should match C_in_per_group * groups from weight
0056:         if C_in_per_group != C_in // groups:
0057:             raise ValueError(
0058:                 f"For transposed conv, expected {C_in // groups} input channels per group, "
0059:                 f"got {C_in_per_group}"
0060:             )
0061:             
0062:         # Calculate and validate output size
0063:         H_out = (H - 1) * stride[0] - 2 * padding[0] + kH
0064:         W_out = (W - 1) * stride[1] - 2 * padding[1] + kW
0065:         if H_out <= 0 or W_out <= 0:
0066:             raise ValueError(
0067:                 f"Transposed conv output size would be negative or zero: ({H_out}, {W_out})"
0068:             )
0069:     
0070:     else:  # Standard and deformable validation
0071:         # Validate channels per group
0072:         if C_in_per_group != C_in // groups:
0073:             raise ValueError(f"Expected {C_in // groups} input channels per group, got {C_in_per_group}")
0074:         
0075:         # Calculate and validate output size
0076:         H_out = ((H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0077:         W_out = ((W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0078:         if H_out <= 0 or W_out <= 0:
0079:             raise ValueError(
0080:                 f"Conv output size would be negative or zero: ({H_out}, {W_out})"
0081:             )
0082: 
0083:     if mode == ConvMode.DEFORMABLE:
0084:         # Validate offset tensor presence and shape
0085:         if offset is None and weight is not None:
0086:             offset = getattr(weight, 'offset', None)
0087:             
0088:         if offset is None:
0089:             raise ValueError("Deformable convolution requires offset parameter")
0090:         
0091:         # Calculate output size for offset validation
0092:         H_out = (H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0] + 1
0093:         W_out = (W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1] + 1
0094:         
0095:         # Validate offset tensor shape
0096:         expected_offset_shape = (N, 2 * kH * kW, H_out, W_out)
0097:         if offset.shape != expected_offset_shape:
0098:             raise ValueError(f"Expected offset shape {expected_offset_shape}, got {offset.shape}")
0099:             
0100:         # Validate mask tensor if present
0101:         if mask is not None:
0102:             expected_mask_shape = (N, kH * kW, H_out, W_out)
0103:             if mask.shape != expected_mask_shape:
0104:                 raise ValueError(f"Expected mask shape {expected_mask_shape}, got {mask.shape}")
0105: 
0106: def _pad_input(x: np.ndarray, padding: Tuple[int, int]) -> np.ndarray:
0107:     """
0108:     Pads input tensor with zeros.
0109:     
0110:     Args:
0111:         x: Input tensor
0112:         padding: (padding_height, padding_width)
0113:     """
0114:     if padding[0] == 0 and padding[1] == 0:
0115:         return x
0116:     pad_width = ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1]))
0117:     return np.pad(x, pad_width, mode='constant', constant_values=0)
0118: 
0119: def _get_output_shape(input_shape: Tuple[int, ...], kernel_size: Tuple[int, int],
0120:                     stride: Tuple[int, int], padding: Tuple[int, int],
0121:                     dilation: Tuple[int, int], mode: str = ConvMode.STANDARD,
0122:                     output_padding: Tuple[int, int] = (0, 0)) -> Tuple[int, int]:
0123:     """
0124:     Calculates output shape for different convolution types.
0125:     
0126:     Args:
0127:         input_shape: Shape of input tensor (N, C, H, W)
0128:         kernel_size: Size of convolution kernel (kH, kW)
0129:         stride: Stride of convolution (sH, sW)
0130:         padding: Zero-padding size (pH, pW)
0131:         dilation: Dilation rate (dH, dW)
0132:         mode: Convolution mode (standard, transposed, or deformable)
0133:         output_padding: Additional size added to output shape (only for transposed conv)
0134:         
0135:     Returns:
0136:         Tuple of output height and width (H_out, W_out)
0137:     """
0138:     if mode == ConvMode.STANDARD:
0139:         H = ((input_shape[2] + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) 
0140:              // stride[0] + 1)
0141:         W = ((input_shape[3] + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) 
0142:              // stride[1] + 1)
0143:     elif mode == ConvMode.TRANSPOSED:
0144:         H = (input_shape[2] - 1) * stride[0] - 2 * padding[0] + kernel_size[0] + output_padding[0]
0145:         W = (input_shape[3] - 1) * stride[1] - 2 * padding[1] + kernel_size[1] + output_padding[1]
0146:     else:  # Deformable follows standard conv shape
0147:         H = ((input_shape[2] + 2 * padding[0] - kernel_size[0]) // stride[0] + 1)
0148:         W = ((input_shape[3] + 2 * padding[1] - kernel_size[1]) // stride[1] + 1)
0149:     return H, W
0150: 
0151: def _get_deformable_offsets(offset_data: np.ndarray, 
0152:                          kernel_size: Tuple[int, int],
0153:                          input_shape: Tuple[int, ...],
0154:                          dilation: Tuple[int, int] = (1, 1)) -> np.ndarray:
0155:     """
0156:     Computes sampling locations for deformable convolution.
0157:     
0158:     Args:
0159:         offset_data: Offset tensor of shape (N, 2*kH*kW, H_out, W_out)
0160:         kernel_size: Tuple of (kH, kW)
0161:         input_shape: Shape of input tensor (N, C, H, W)
0162:         dilation: Dilation rate
0163:         
0164:     Returns:
0165:         Sampling locations of shape (N, H_out*W_out, kH*kW, 2)
0166:     """
0167:     N, _, H_out, W_out = offset_data.shape
0168:     kH, kW = kernel_size
0169:     
0170:     # Generate base grid for the kernel
0171:     h_range = np.arange(kH) * dilation[0]
0172:     w_range = np.arange(kW) * dilation[1]
0173:     h_grid, w_grid = np.meshgrid(h_range, w_range, indexing='ij')
0174:     kernel_grid = np.stack([h_grid, w_grid], axis=-1)  # (kH, kW, 2)
0175:     kernel_grid = kernel_grid.reshape(-1, 2)  # (kH*kW, 2)
0176:     
0177:     # Reshape offsets
0178:     offset = offset_data.reshape(N, 2, kH*kW, H_out, W_out)
0179:     offset = offset.transpose(0, 3, 4, 2, 1)  # (N, H_out, W_out, kH*kW, 2)
0180:     offset = offset.reshape(N, H_out*W_out, kH*kW, 2)
0181:     
0182:     # Add base grid to offsets
0183:     kernel_grid = np.expand_dims(np.expand_dims(kernel_grid, 0), 0)  # (1, 1, kH*kW, 2)
0184:     sampling_locations = kernel_grid + offset
0185:     
0186:     return sampling_locations
0187: 
0188: def _bilinear_interpolate(input: np.ndarray, points: np.ndarray, align_corners: bool = True) -> np.ndarray:
0189:     """
0190:     Performs bilinear interpolation on the input tensor at specified points.
0191:     
0192:     Args:
0193:         input: Input tensor (N, C, H, W)
0194:         points: Points to sample (N, P, 2) in normalized coordinates [-1, 1]
0195:         align_corners: Whether to align corners in interpolation
0196:         
0197:     Returns:
0198:         Interpolated values (N, C, P)
0199:     """
0200:     N, C, H, W = input.shape
0201:     
0202:     # Ensure points is correct shape (N, P, 2)
0203:     if points.ndim == 4:
0204:         points = points.reshape(points.shape[0], -1, 2)
0205:     
0206:     _, P, _ = points.shape
0207:     
0208:     # Convert normalized coordinates to pixel coordinates
0209:     if align_corners:
0210:         x = (points[..., 0] + 1) * (W - 1) / 2
0211:         y = (points[..., 1] + 1) * (H - 1) / 2
0212:     else:
0213:         x = ((points[..., 0] + 1) * W - 1) / 2
0214:         y = ((points[..., 1] + 1) * H - 1) / 2
0215:     
0216:     # Get corner indices
0217:     x0 = np.floor(x).astype(np.int32)
0218:     x1 = x0 + 1
0219:     y0 = np.floor(y).astype(np.int32)
0220:     y1 = y0 + 1
0221:     
0222:     # Clip to image boundaries
0223:     x0 = np.clip(x0, 0, W - 1)
0224:     x1 = np.clip(x1, 0, W - 1)
0225:     y0 = np.clip(y0, 0, H - 1)
0226:     y1 = np.clip(y1, 0, H - 1)
0227:     
0228:     # Calculate interpolation weights
0229:     wa = (x1 - x) * (y1 - y)
0230:     wb = (x1 - x) * (y - y0)
0231:     wc = (x - x0) * (y1 - y)
0232:     wd = (x - x0) * (y - y0)
0233:     
0234:     # Reshape weights for broadcasting
0235:     wa = wa[..., None]
0236:     wb = wb[..., None]
0237:     wc = wc[..., None]
0238:     wd = wd[..., None]
0239:     
0240:     # Gather corner values and compute weighted sum
0241:     output = np.zeros((N, C, P))
0242:     for n in range(N):
0243:         output[n] = (wa[n] * input[n, :, y0[n], x0[n]] +
0244:                     wb[n] * input[n, :, y1[n], x0[n]] +
0245:                     wc[n] * input[n, :, y0[n], x1[n]] +
0246:                     wd[n] * input[n, :, y1[n], x1[n]])
0247:     
0248:     return output
0249: 
0250: def _im2col_dilated(x: np.ndarray, 
0251:                     kernel_size: Tuple[int, int],
0252:                     stride: Tuple[int, int], 
0253:                     dilation: Tuple[int, int],
0254:                     padding: Tuple[int, int],
0255:                     mode: str = ConvMode.STANDARD,
0256:                     sampling_locations: Optional[np.ndarray] = None) -> np.ndarray:
0257:     """Rearranges dilated image blocks into columns."""
0258:     N, C, H, W = x.shape
0259:     kH, kW = kernel_size
0260:     
0261:     # Calculate output dimensions
0262:     H_out = ((H - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0263:     W_out = ((W - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0264:     
0265:     # Initialize output array
0266:     # For standard convolution:
0267:     # - Each column represents a dot product of the kernel with a specific output position
0268:     # - Number of rows = C * kH * kW (all values needed for one output value)
0269:     # - Number of columns = N * H_out * W_out (total number of output values)
0270:     cols = np.zeros((C * kH * kW, N * H_out * W_out))
0271:     
0272:     # Process each input position in the kernel
0273:     for c in range(C):
0274:         for kh in range(kH):
0275:             for kw in range(kW):
0276:                 h_start = np.arange(H_out) * stride[0]
0277:                 w_start = np.arange(W_out) * stride[1]
0278:                 
0279:                 h_offset = kh * dilation[0]
0280:                 w_offset = kw * dilation[1]
0281:                 
0282:                 h_pos, w_pos = np.meshgrid(h_start + h_offset, w_start + w_offset, indexing='ij')
0283:                 h_pos = h_pos.reshape(-1)
0284:                 w_pos = w_pos.reshape(-1)
0285:                 
0286:                 row_idx = c * kH * kW + kh * kW + kw
0287:                 for n in range(N):
0288:                     col_idx = n * H_out * W_out + np.arange(H_out * W_out)
0289:                     cols[row_idx, col_idx] = x[n, c, h_pos, w_pos]
0290:     
0291:     print(f"im2col output shape: {cols.shape}")
0292:     print(f"Expected reshape: C*kH*kW={C*kH*kW}, N={N}, H_out={H_out}, W_out={W_out}")
0293:     return cols
0294: 
0295: def _get_output_size(input_shape: Tuple[int, ...], 
0296:                     kernel_size: Tuple[int, int],
0297:                     stride: Tuple[int, int], 
0298:                     padding: Tuple[int, int],
0299:                     dilation: Tuple[int, int],
0300:                     mode: str) -> Tuple[int, int]:
0301:     """Calculate output dimensions for different convolution modes."""
0302:     _, _, H, W = input_shape
0303:     kH, kW = kernel_size
0304: 
0305:     if mode == ConvMode.TRANSPOSED:
0306:         H_out = (H - 1) * stride[0] - 2 * padding[0] + kH
0307:         W_out = (W - 1) * stride[1] - 2 * padding[1] + kW
0308:     else:
0309:         H_out = ((H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0310:         W_out = ((W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0311: 
0312:     return H_out, W_out
0313: 
0314: def _col2im_dilated(cols: np.ndarray, output_size: Tuple[int, ...],
0315:                     kernel_size: Tuple[int, int], stride: Tuple[int, int],
0316:                     dilation: Tuple[int, int], mode: str = ConvMode.STANDARD) -> np.ndarray:
0317:     """Convert columns back to dilated image."""
0318:     N, C, H, W = output_size
0319:     kH, kW = kernel_size
0320:     
0321:     # Calculate output dimensions based on mode
0322:     if mode == ConvMode.TRANSPOSED:
0323:         H_out = H * stride[0]
0324:         W_out = W * stride[1]
0325:     else:
0326:         H_out = ((H - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0327:         W_out = ((W - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0328:     
0329:     output = np.zeros(output_size)
0330:     weights = np.zeros(output_size)  # For averaging overlapping values
0331:     
0332:     if mode == ConvMode.TRANSPOSED:
0333:         for h in range(H):
0334:             for w in range(W):
0335:                 for c in range(C):
0336:                     for kh in range(kH):
0337:                         for kw in range(kW):
0338:                             h_out = h * stride[0] + kh
0339:                             w_out = w * stride[1] + kw
0340:                             
0341:                             if h_out < H_out and w_out < W_out:
0342:                                 col_idx = c * kH * kW + kh * kW + kw
0343:                                 row_idx = h * W + w
0344:                                 
0345:                                 for n in range(N):
0346:                                     output[n, c, h_out, w_out] += cols[col_idx, n * H * W + row_idx]
0347:                                     weights[n, c, h_out, w_out] += 1
0348:     else:
0349:         for h_out in range(H_out):
0350:             for w_out in range(W_out):
0351:                 for c in range(C):
0352:                     for i in range(kH):
0353:                         for j in range(kW):
0354:                             h_in = h_out * stride[0] + i * dilation[0]
0355:                             w_in = w_out * stride[1] + j * dilation[1]
0356:                             
0357:                             if 0 <= h_in < H and 0 <= w_in < W:
0358:                                 col_idx = c * kH * kW + i * kW + j
0359:                                 row_idx = h_out * W_out + w_out
0360:                                 
0361:                                 for n in range(N):
0362:                                     output[n, c, h_in, w_in] += cols[col_idx, n * H_out * W_out + row_idx]
0363:                                     weights[n, c, h_in, w_in] += 1
0364:     
0365:     # Average overlapping values
0366:     np.divide(output, weights, out=output, where=weights != 0)
0367:     return output
0368: 
0369: def _compute_conv_output_shape(input_size: int, kernel_size: int, stride: int,
0370:                              padding: int, dilation: int) -> int:
0371:     """Computes output dimension for a single axis."""
0372:     numerator = input_size + 2 * padding - dilation * (kernel_size - 1) - 1
0373:     return numerator // stride + 1
0374: 
0375: def _compute_conv_grad_input_padding(grad_output_size: int, input_size: int,
0376:                                    kernel_size: int, stride: int, padding: int,
0377:                                    dilation: int) -> Tuple[int, int]:
0378:     """Computes padding needed for gradient computation."""
0379:     grad_input_padding = kernel_size - 1 - padding
0380:     return grad_input_padding
0381: 
0382: def _compute_output_padding(input_size: int, output_size: int, kernel_size: int,
0383:                           stride: int, padding: int, dilation: int) -> int:
0384:     """Computes additional padding needed for transposed convolution."""
0385:     expected_output = (input_size - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + 1
0386:     return output_size - expected_output
0387: 
0388: def _unfold(input_tensor: np.ndarray,
0389:            kernel_size: Tuple[int, ...],
0390:            dilation: Tuple[int, ...],
0391:            padding: Tuple[int, ...],
0392:            stride: Tuple[int, ...]) -> np.ndarray:
0393:     """Extracts sliding local blocks from input tensor."""
0394:     N, C, H, W = input_tensor.shape
0395:     kH, kW = kernel_size
0396:     
0397:     # Apply padding if needed
0398:     if padding[0] > 0 or padding[1] > 0:
0399:         input_tensor = np.pad(input_tensor,
0400:                           ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])),
0401:                           mode='constant')
0402:     
0403:     # Calculate output dimensions
0404:     H_out = ((H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0405:     W_out = ((W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0406:     
0407:     # Initialize output array with correct shape
0408:     output = np.zeros((C * kH * kW, N * H_out * W_out))
0409:     
0410:     # Extract patches
0411:     for h in range(H_out):
0412:         for w in range(W_out):
0413:             for i in range(kH):
0414:                 for j in range(kW):
0415:                     h_start = h * stride[0] + i * dilation[0]
0416:                     w_start = w * stride[1] + j * dilation[1]
0417:                     
0418:                     # Extract patch for all channels and batches
0419:                     patch = input_tensor[:, :, h_start:h_start+1, w_start:w_start+1]
0420:                     
0421:                     # Place in output array
0422:                     row_idx = (i * kW + j) * C + np.arange(C)
0423:                     col_idx = h * W_out + w + np.arange(N) * H_out * W_out
0424:                     output[row_idx[:, None], col_idx] = patch.reshape(N, C).T
0425:     
0426:     return output
0427: 
0428: def _fold(input: np.ndarray,
0429:          output_size: Tuple[int, ...],
0430:          kernel_size: Tuple[int, ...],
0431:          dilation: Tuple[int, ...],
0432:          padding: Tuple[int, ...],
0433:          stride: Tuple[int, ...]) -> np.ndarray:
0434:     """Combines an array of sliding local blocks into a large tensor."""
0435:     H, W = output_size
0436:     kH, kW = kernel_size
0437:     C = input.shape[0] // (kH * kW)
0438:     N = input.shape[1] // ((H + 2 * padding[0] - kH + 1) * (W + 2 * padding[1] - kW + 1))
0439:     
0440:     # Initialize output tensor
0441:     output = np.zeros((N, C, H + 2 * padding[0], W + 2 * padding[1]))
0442:     divisor = np.zeros_like(output)  # For averaging overlapping values
0443:     
0444:     # Calculate output dimensions
0445:     H_out = ((H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0446:     W_out = ((W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0447:     
0448:     # Fold patches back
0449:     for h in range(H_out):
0450:         for w in range(W_out):
0451:             for i in range(kH):
0452:                 for j in range(kW):
0453:                     h_start = h * stride[0] + i * dilation[0]
0454:                     w_start = w * stride[1] + j * dilation[1]
0455:                     
0456:                     row_idx = (i * kW + j) * C + np.arange(C)
0457:                     col_idx = h * W_out + w + np.arange(N) * H_out * W_out
0458:                     
0459:                     patch = input[row_idx[:, None], col_idx].T.reshape(N, C, 1, 1)
0460:                     output[:, :, h_start:h_start+1, w_start:w_start+1] += patch
0461:                     divisor[:, :, h_start:h_start+1, w_start:w_start+1] += 1
0462:     
0463:     # Average overlapping values
0464:     output = np.divide(output, divisor, where=divisor != 0)
0465:     
0466:     # Remove padding if necessary
0467:     if padding[0] > 0 or padding[1] > 0:
0468:         output = output[:, :, padding[0]:-padding[0] if padding[0] > 0 else None,
0469:                        padding[1]:-padding[1] if padding[1] > 0 else None]
0470:     
0471:     return output
0472: 
0473: def _dilate(input: np.ndarray, dilation: Tuple[int, ...]) -> np.ndarray:
0474:     """
0475:     Dilates the input tensor by inserting zeros between elements.
0476:     
0477:     Args:
0478:         input: Input tensor
0479:         dilation: Dilation factors for each dimension
0480:         
0481:     Returns:
0482:         Dilated tensor
0483:     """
0484:     if all(d == 1 for d in dilation):
0485:         return input
0486: 
0487:     N, C, H, W = input.shape
0488:     dH, dW = dilation
0489:     
0490:     H_dilated = H + (H - 1) * (dH - 1)
0491:     W_dilated = W + (W - 1) * (dW - 1)
0492:     
0493:     output = np.zeros((N, C, H_dilated, W_dilated))
0494:     output[:, :, ::dH, ::dW] = input
0495:     
0496:     return output
0497: 
0498: def _bilinear_interpolate(input: np.ndarray,
0499:                        points: np.ndarray,
0500:                        align_corners: bool = True) -> np.ndarray:
0501:     """
0502:     Performs bilinear interpolation on the input tensor at specified points.
0503:     
0504:     Args:
0505:         input: Input tensor (N, C, H, W)
0506:         points: Points to sample (N, P, 2) in normalized coordinates [-1, 1]
0507:         align_corners: Whether to align corners in interpolation
0508:         
0509:     Returns:
0510:         Interpolated values (N, C, P)
0511:     """ 
0512:     N, C, H, W = input.shape
0513:     _, P, _ = points.shape
0514: 
0515:     # Convert normalized coordinates to pixel coordinates
0516:     if align_corners:
0517:         x = (points[..., 0] + 1) * (W - 1) / 2
0518:         y = (points[..., 1] + 1) * (H - 1) / 2
0519:     else:
0520:         x = ((points[..., 0] + 1) * W - 1) / 2
0521:         y = ((points[..., 1] + 1) * H - 1) / 2
0522: 
0523:     # Get corner indices
0524:     x0 = np.floor(x).astype(np.int32)
0525:     x1 = x0 + 1
0526:     y0 = np.floor(y).astype(np.int32)
0527:     y1 = y0 + 1
0528: 
0529:     # Clip to image boundaries
0530:     x0 = np.clip(x0, 0, W - 1)
0531:     x1 = np.clip(x1, 0, W - 1)
0532:     y0 = np.clip(y0, 0, H - 1)
0533:     y1 = np.clip(y1, 0, H - 1)
0534: 
0535:     # Calculate interpolation weights
0536:     wa = (x1 - x) * (y1 - y)
0537:     wb = (x1 - x) * (y - y0)
0538:     wc = (x - x0) * (y1 - y)
0539:     wd = (x - x0) * (y - y0)
0540: 
0541:     # Gather corner values
0542:     Ia = np.zeros((N, C, P))
0543:     Ib = np.zeros((N, C, P))
0544:     Ic = np.zeros((N, C, P))
0545:     Id = np.zeros((N, C, P))
0546: 
0547:     for n in range(N):
0548:         for p in range(P):
0549:             Ia[n, :, p] = input[n, :, y0[n, p], x0[n, p]]
0550:             Ib[n, :, p] = input[n, :, y1[n, p], x0[n, p]]
0551:             Ic[n, :, p] = input[n, :, y0[n, p], x1[n, p]]
0552:             Id[n, :, p] = input[n, :, y1[n, p], x1[n, p]]
0553: 
0554:     # Reshape weights for broadcasting
0555:     wa = wa.reshape(N, 1, P)
0556:     wb = wb.reshape(N, 1, P)
0557:     wc = wc.reshape(N, 1, P)
0558:     wd = wd.reshape(N, 1, P)
0559: 
0560:     # Interpolate
0561:     out = wa * Ia + wb * Ib + wc * Ic + wd * Id
0562:     return out
0563: 
0564: def _bilinear_interpolate_gradient(grad_output: np.ndarray,
0565:                                 points: np.ndarray,
0566:                                 input_size: Tuple[int, ...],
0567:                                 input_tensor: np.ndarray,
0568:                                 align_corners: bool = True) -> Tuple[np.ndarray, np.ndarray]:
0569:     """
0570:     Computes gradients for bilinear interpolation.
0571:     
0572:     Args:
0573:         grad_output: Gradient of loss with respect to interpolated values (can be any shape)
0574:         points: Points that were sampled (N, P, 2) in normalized coordinates [-1, 1]
0575:         input_size: Size of the input tensor (H, W)
0576:         input_tensor: The input tensor being sampled from (N, C, H, W)
0577:         align_corners: Whether corners were aligned in interpolation
0578:         
0579:     Returns:
0580:         Tuple of:
0581:             - grad_input: Gradient with respect to input tensor (N, C, H, W)
0582:             - grad_points: Gradient with respect to sampling points (N, P, 2)
0583:             
0584:     Raises:
0585:         ValueError: If input shapes are incompatible
0586:     """
0587:     # Ensure points is properly shaped first
0588:     if points.ndim == 2:
0589:         points = points.reshape(1, *points.shape)
0590:     
0591:     # Get number of points from points tensor
0592:     N, P, _ = points.shape
0593:     
0594:     # Ensure grad_output is properly shaped (N, C, P) to match points
0595:     if grad_output.ndim == 1:
0596:         grad_output = grad_output.reshape(1, 1, -1)
0597:     elif grad_output.ndim == 2:
0598:         grad_output = grad_output.reshape(1, -1, 1)
0599:         
0600:     # Broadcast grad_output to match number of points if necessary
0601:     if grad_output.shape[2] == 1:
0602:         grad_output = np.broadcast_to(grad_output, (grad_output.shape[0], grad_output.shape[1], P))
0603:     elif grad_output.shape[2] != P:
0604:         raise ValueError(f"Gradient shape {grad_output.shape} cannot be broadcast to number of points {P}")
0605:         
0606:     C = grad_output.shape[1]
0607:     H, W = input_size
0608: 
0609:     # Validate input_tensor shape
0610:     if input_tensor.shape[2:] != input_size:
0611:         raise ValueError(f"Input tensor spatial dimensions {input_tensor.shape[2:]} "
0612:                         f"don't match input_size {input_size}")
0613:     
0614:     # Convert normalized coordinates to pixel coordinates
0615:     if align_corners:
0616:         x = (points[..., 0] + 1) * (W - 1) / 2
0617:         y = (points[..., 1] + 1) * (H - 1) / 2
0618:     else:
0619:         x = ((points[..., 0] + 1) * W - 1) / 2
0620:         y = ((points[..., 1] + 1) * H - 1) / 2
0621:     
0622:     # Get corner indices
0623:     x0 = np.floor(x).astype(np.int32)
0624:     x1 = x0 + 1
0625:     y0 = np.floor(y).astype(np.int32)
0626:     y1 = y0 + 1
0627:     
0628:     # Clip to image boundaries
0629:     x0 = np.clip(x0, 0, W - 1)
0630:     x1 = np.clip(x1, 0, W - 1)
0631:     y0 = np.clip(y0, 0, H - 1)
0632:     y1 = np.clip(y1, 0, H - 1)
0633:     
0634:     # Compute weights for bilinear interpolation
0635:     wa = (x1 - x) * (y1 - y)
0636:     wb = (x1 - x) * (y - y0)
0637:     wc = (x - x0) * (y1 - y)
0638:     wd = (x - x0) * (y - y0)
0639:     
0640:     # Reshape weights for broadcasting with channel dimension
0641:     wa = wa[..., None]  # Shape: (N, P, 1)
0642:     wb = wb[..., None]
0643:     wc = wc[..., None]
0644:     wd = wd[..., None]
0645:     
0646:     # Initialize gradients
0647:     grad_input = np.zeros((N, C, H, W))
0648:     grad_points = np.zeros_like(points)  # Shape: (N, P, 2)
0649:     
0650:     # Compute gradients with respect to input
0651:     for n in range(N):
0652:         for c in range(C):
0653:             grad_chan = grad_output[n, c]  # Shape: (P,)
0654:             for p in range(P):
0655:                 grad = grad_chan[p]
0656:                 grad_input[n, c, y0[n, p], x0[n, p]] += grad * wa[n, p, 0]
0657:                 grad_input[n, c, y1[n, p], x0[n, p]] += grad * wb[n, p, 0]
0658:                 grad_input[n, c, y0[n, p], x1[n, p]] += grad * wc[n, p, 0]
0659:                 grad_input[n, c, y1[n, p], x1[n, p]] += grad * wd[n, p, 0]
0660:     
0661:     # Compute scaling factors for coordinate gradients
0662:     if align_corners:
0663:         dx = (W - 1) / 2
0664:         dy = (H - 1) / 2
0665:     else:
0666:         dx = W / 2
0667:         dy = H / 2
0668:     
0669:     # Compute gradients with respect to sampling points
0670:     for n in range(N):
0671:         for p in range(P):
0672:             grad = grad_output[n, :, p].sum()  # Sum over channels
0673:             
0674:             # Gradient with respect to x
0675:             gx = grad * (
0676:                 (y1[n, p] - y[n, p]) * (
0677:                     input_tensor[n, :, y0[n, p], x1[n, p]] - 
0678:                     input_tensor[n, :, y0[n, p], x0[n, p]]
0679:                 ).sum() +
0680:                 (y[n, p] - y0[n, p]) * (
0681:                     input_tensor[n, :, y1[n, p], x1[n, p]] - 
0682:                     input_tensor[n, :, y1[n, p], x0[n, p]]
0683:                 ).sum()
0684:             ) * dx
0685:             
0686:             # Gradient with respect to y
0687:             gy = grad * (
0688:                 (x1[n, p] - x[n, p]) * (
0689:                     input_tensor[n, :, y1[n, p], x0[n, p]] - 
0690:                     input_tensor[n, :, y0[n, p], x0[n, p]]
0691:                 ).sum() +
0692:                 (x[n, p] - x0[n, p]) * (
0693:                     input_tensor[n, :, y1[n, p], x1[n, p]] - 
0694:                     input_tensor[n, :, y0[n, p], x1[n, p]]
0695:                 ).sum()
0696:             ) * dy
0697:             
0698:             grad_points[n, p] = [gx, gy]
0699:     
0700:     return grad_input, grad_points
0701: 
0702: def _generate_grid(batch_size: int, height: int, width: int,
0703:                  align_corners: bool = True) -> np.ndarray:
0704:     """
0705:     Generates a coordinate grid for grid sampling.
0706:     
0707:     Args:
0708:         batch_size: Number of samples in batch
0709:         height: Height of the grid
0710:         width: Width of the grid
0711:         align_corners: Whether to align corners
0712:         
0713:     Returns:
0714:         Grid tensor of shape (N, H, W, 2) with normalized coordinates
0715:     """
0716:     if align_corners:
0717:         x = np.linspace(-1, 1, width)
0718:         y = np.linspace(-1, 1, height)
0719:     else:
0720:         x = np.linspace(-1 + (1/width), 1 - (1/width), width)
0721:         y = np.linspace(-1 + (1/height), 1 - (1/height), height)
0722: 
0723:     x_coords, y_coords = np.meshgrid(x, y)
0724:     grid = np.stack([x_coords, y_coords], axis=-1)
0725:     grid = np.tile(grid[None], (batch_size, 1, 1, 1))
0726:     
0727:     return grid
0728: 
0729: def _deform_grid(grid: np.ndarray, offset: np.ndarray) -> np.ndarray:
0730:     """
0731:     Deforms a regular grid using offset values.
0732:     
0733:     Args:
0734:         grid: Regular coordinate grid (N, H, W, 2)
0735:         offset: Offset values for deformation (N, 2, H, W)
0736:         
0737:     Returns:
0738:         Deformed grid (N, H, W, 2)
0739:     """
0740:     N, H, W, _ = grid.shape
0741:     
0742:     # Reshape offset to match grid shape
0743:     offset = offset.transpose(0, 2, 3, 1)
0744:     
0745:     # Add offset to grid
0746:     deformed_grid = grid + offset
0747:     
0748:     # Clamp values to [-1, 1] to ensure valid sampling
0749:     return np.clip(deformed_grid, -1, 1)
0750: 
0751: def _modulated_deform_grid(grid: np.ndarray, offset: np.ndarray, 
0752:                         mask: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
0753:     """
0754:     Deforms a regular grid using offset values and modulation mask.
0755:     Used in Deformable ConvNets v2.
0756:     
0757:     Args:
0758:         grid: Regular coordinate grid (N, H, W, 2)
0759:         offset: Offset values for deformation (N, 2, H, W)
0760:         mask: Modulation mask (N, 1, H, W)
0761:         
0762:     Returns:
0763:         Tuple of deformed grid and modulation mask
0764:     """
0765:     # Deform grid
0766:     deformed_grid = _deform_grid(grid, offset)
0767:     
0768:     # Reshape mask to match sampling points
0769:     mask = mask.transpose(0, 2, 3, 1)
0770:     
0771:     return deformed_grid, mask
0772: 
0773: def _compute_indices_weights(points: np.ndarray, size: Tuple[int, int]) -> Tuple[np.ndarray, ...]:
0774:     """
0775:     Computes indices and weights for bilinear interpolation.
0776:     
0777:     Args:
0778:         points: Sampling points (N, H, W, 2)
0779:         size: Size of the input feature map (H, W)
0780:         
0781:     Returns:
0782:         Tuple of indices and weights for bilinear interpolation
0783:     """
0784:     H, W = size
0785:     
0786:     # Convert points to pixel coordinates
0787:     x = (points[..., 0] + 1) * (W - 1) / 2
0788:     y = (points[..., 1] + 1) * (H - 1) / 2
0789:     
0790:     # Get corner indices
0791:     x0 = np.floor(x).astype(np.int32)
0792:     x1 = x0 + 1
0793:     y0 = np.floor(y).astype(np.int32)
0794:     y1 = y0 + 1
0795:     
0796:     # Clip to image boundaries
0797:     x0 = np.clip(x0, 0, W - 1)
0798:     x1 = np.clip(x1, 0, W - 1)
0799:     y0 = np.clip(y0, 0, H - 1)
0800:     y1 = np.clip(y1, 0, H - 1)
0801:     
0802:     # Compute weights
0803:     wa = (x1 - x) * (y1 - y)
0804:     wb = (x1 - x) * (y - y0)
0805:     wc = (x - x0) * (y1 - y)
0806:     wd = (x - x0) * (y - y0)
0807:     
0808:     return (x0, x1, y0, y1), (wa, wb, wc, wd)
0809: 
0810: def _apply_deform_conv(input: np.ndarray, weight: np.ndarray, offset: np.ndarray,
0811:                     stride: Tuple[int, int], padding: Tuple[int, int],
0812:                     dilation: Tuple[int, int], mask: Optional[np.ndarray] = None) -> np.ndarray:
0813:     """
0814:     Applies deformable convolution operation.
0815:     
0816:     Args:
0817:         input: Input feature map (N, C_in, H, W)
0818:         weight: Convolution weights (C_out, C_in, kH, kW)
0819:         offset: Sampling offsets (N, 2*kH*kW, H_out, W_out)
0820:         stride: Convolution stride
0821:         padding: Zero-padding size
0822:         dilation: Dilation rate
0823:         mask: Optional modulation mask for v2 (N, kH*kW, H_out, W_out)
0824:         
0825:     Returns:
0826:         Output feature map (N, C_out, H_out, W_out)
0827:     """
0828:     N, C_in, H, W = input.shape
0829:     C_out, _, kH, kW = weight.shape
0830:     H_out = (H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0] + 1
0831:     W_out = (W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1] + 1
0832:     
0833:     # Generate sampling grid
0834:     grid = _generate_grid(N, H_out, W_out)
0835:     
0836:     # Deform grid using offsets
0837:     if mask is not None:
0838:         deformed_grid, modulation = _modulated_deform_grid(grid, offset, mask)
0839:     else:
0840:         deformed_grid = _deform_grid(grid, offset)
0841:         modulation = None
0842:     
0843:     # Get sampling indices and weights
0844:     indices, weights = _compute_indices_weights(deformed_grid, (H, W))
0845:     x0, x1, y0, y1 = indices
0846:     wa, wb, wc, wd = weights
0847:     
0848:     # Initialize output
0849:     output = np.zeros((N, C_out, H_out, W_out))
0850:     
0851:     # Apply convolution with deformed sampling
0852:     for i in range(kH):
0853:         for j in range(kW):
0854:             # Get values from input feature map
0855:             values = (wa[..., None] * input[:, :, y0, x0] +
0856:                      wb[..., None] * input[:, :, y1, x0] +
0857:                      wc[..., None] * input[:, :, y0, x1] +
0858:                      wd[..., None] * input[:, :, y1, x1])
0859:                      
0860:             # Apply modulation if available
0861:             if modulation is not None:
0862:                 values = values * modulation[:, i*kW + j, ..., None]
0863:                 
0864:             # Accumulate weighted values
0865:             for cout in range(C_out):
0866:                 output[:, cout] += np.sum(values * weight[cout, :, i, j], axis=1)
0867:     
0868:     return output
0869:         
0870: class Conv2dFunction(Function):
0871:     @staticmethod
0872:     def forward(ctx: Context, x: Tensor, weight: Tensor, bias: Optional[Tensor] = None,
0873:             stride: Tuple[int, int] = (1, 1), padding: Tuple[int, int] = (0, 0),
0874:             dilation: Tuple[int, int] = (1, 1), groups: int = 1,
0875:             mode: str = ConvMode.STANDARD, offset: Optional[Tensor] = None,
0876:             mask: Optional[Tensor] = None, output_padding: Tuple[int, int] = (0, 0)) -> Tensor:
0877:         """Forward pass of flexible 2D convolution."""
0878:         # Validate parameters
0879:         _validate_conv_params(x.shape, weight.shape, stride, padding, dilation, groups,
0880:                             mode, offset, weight, mask)
0881:         
0882:         # Save tensors and info for backward pass
0883:         if mode == ConvMode.DEFORMABLE:
0884:             ctx.save_for_backward(x, weight, bias, offset)
0885:         else:
0886:             ctx.save_for_backward(x, weight, bias)
0887:             
0888:         ctx.save_arguments(stride=stride, padding=padding, dilation=dilation,
0889:                         groups=groups, mode=mode, sampling_locations=None,
0890:                         output_padding=output_padding)
0891: 
0892:         if mode == ConvMode.TRANSPOSED:
0893:             # Get shapes
0894:             batch_size, C_in, H_in, W_in = x.shape
0895:             C_out, C_in_per_group, kH, kW = weight.shape
0896:             C_in_per_group = C_in // groups
0897:             C_out_per_group = C_out // groups
0898: 
0899:             # Calculate output dimensions
0900:             H_out = (H_in - 1) * stride[0] - 2 * padding[0] + kH + output_padding[0]
0901:             W_out = (W_in - 1) * stride[1] - 2 * padding[1] + kW + output_padding[1]
0902: 
0903:             # Initialize output tensor
0904:             output = np.zeros((batch_size, C_out, H_out, W_out))
0905: 
0906:             # Process each group
0907:             for g in range(groups):
0908:                 # Get input and weight for current group
0909:                 x_g = x.data[:, g*C_in_per_group:(g+1)*C_in_per_group]
0910:                 w_g = weight.data[g*C_out_per_group:(g+1)*C_out_per_group]
0911:                 
0912:                 # Flip kernel for transposed convolution
0913:                 w_g = np.flip(np.flip(w_g, 2), 3).transpose(1, 0, 2, 3)
0914:                 
0915:                 # Create the output tensor for this group
0916:                 output_g = np.zeros((batch_size, C_out_per_group, H_out, W_out))
0917:                 
0918:                 # Process each input element
0919:                 for n in range(batch_size):
0920:                     for h in range(H_in):
0921:                         for w in range(W_in):
0922:                             # Get input value for all channels
0923:                             x_val = x_g[n, :, h, w]  # Shape: (C_in_per_group,)
0924:                             
0925:                             # Calculate output position
0926:                             h_start = h * stride[0] - padding[0]
0927:                             w_start = w * stride[1] - padding[1]
0928:                             
0929:                             # Apply kernel at this position
0930:                             for c_out in range(C_out_per_group):
0931:                                 for c_in in range(C_in_per_group):
0932:                                     for kh in range(kH):
0933:                                         for kw in range(kW):
0934:                                             h_out = h_start + kh
0935:                                             w_out = w_start + kw
0936:                                             
0937:                                             if (0 <= h_out < H_out and 0 <= w_out < W_out):
0938:                                                 output_g[n, c_out, h_out, w_out] += (
0939:                                                     x_val[c_in] * w_g[c_in, c_out, kh, kw]
0940:                                                 )
0941:                 
0942:                 # Add this group's output to the final output
0943:                 output[:, g*C_out_per_group:(g+1)*C_out_per_group] = output_g
0944: 
0945:             if bias is not None:
0946:                 output += bias.data.reshape(1, -1, 1, 1)
0947: 
0948:             return Tensor(output)
0949:         else:
0950:             # Rest of the code for standard/deformable convolution remains the same
0951:             x_padded = _pad_input(x.data, padding)
0952:             C_in_per_group = x.shape[1] // groups
0953:             C_out_per_group = weight.shape[0] // groups
0954: 
0955:             H_out = ((x.shape[2] + 2 * padding[0] - dilation[0] * (weight.shape[2] - 1) - 1)
0956:                     // stride[0] + 1)
0957:             W_out = ((x.shape[3] + 2 * padding[1] - dilation[1] * (weight.shape[3] - 1) - 1)
0958:                     // stride[1] + 1)
0959: 
0960:             output = np.zeros((x.shape[0], weight.shape[0], H_out, W_out))
0961: 
0962:             for g in range(groups):
0963:                 x_g = x_padded[:, g*C_in_per_group:(g+1)*C_in_per_group]
0964:                 w_g = weight.data[g*C_out_per_group:(g+1)*C_out_per_group]
0965: 
0966:                 x_cols = _im2col_dilated(x_g, weight.shape[2:], stride, dilation, padding,
0967:                         mode=mode, sampling_locations=None)
0968:                 w_reshaped = w_g.reshape(C_out_per_group, -1)
0969:                 out = w_reshaped @ x_cols
0970: 
0971:                 out = out.reshape(C_out_per_group, H_out * W_out, x.shape[0])
0972:                 out = out.transpose(2, 0, 1).reshape(x.shape[0], C_out_per_group, H_out, W_out)
0973:                 output[:, g*C_out_per_group:(g+1)*C_out_per_group] = out
0974: 
0975:             if bias is not None:
0976:                 output += bias.data.reshape(1, -1, 1, 1)
0977: 
0978:             return Tensor(output)
0979:     
0980:     @staticmethod
0981:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0982:         """Backward pass of 2D convolution."""
0983:         # Retrieve saved tensors and arguments
0984:         saved_tensors = ctx.saved_tensors
0985:         num_saved = len(saved_tensors)
0986:         
0987:         # Initialize variables
0988:         offset_tensor = None
0989:         mask = None
0990:         grad_offset = None
0991:         grad_mask = None
0992:         
0993:         # Get saved tensors based on mode
0994:         if num_saved == 4:  # Deformable case
0995:             x, weight, bias, offset_tensor = saved_tensors
0996:         else:  # Standard/transposed case
0997:             x, weight, bias = saved_tensors
0998:             
0999:         # Get saved arguments
1000:         stride = ctx.saved_arguments['stride']
1001:         padding = ctx.saved_arguments['padding']
1002:         dilation = ctx.saved_arguments['dilation']
1003:         groups = ctx.saved_arguments['groups']
1004:         mode = ctx.saved_arguments['mode']
1005:         sampling_locations = ctx.saved_arguments.get('sampling_locations', None)
1006:         
1007:         # Calculate dimensions
1008:         N, C_in, H, W = x.shape
1009:         C_out, _, kH, kW = weight.shape
1010:         C_in_per_group = C_in // groups
1011:         C_out_per_group = C_out // groups
1012:         H_out, W_out = _get_output_shape(x.shape, weight.shape[2:], stride, padding, dilation, mode)
1013:         
1014:         # Initialize gradients based on requires_grad flags
1015:         grad_x = None
1016:         grad_x_padded = None
1017:         grad_weight = None
1018:         grad_bias = None
1019:         
1020:         if x.requires_grad:
1021:             if mode in [ConvMode.STANDARD, ConvMode.DEFORMABLE]:
1022:                 x_padded = _pad_input(x.data, padding)
1023:                 grad_x_padded = np.zeros_like(x_padded)
1024:             else:  # Transposed
1025:                 grad_x = np.zeros_like(x.data)
1026:                 
1027:         if weight.requires_grad:
1028:             grad_weight = np.zeros_like(weight.data)
1029:             
1030:         if bias is not None and bias.requires_grad:
1031:             grad_bias = np.zeros_like(bias.data)
1032:             
1033:         if mode == ConvMode.DEFORMABLE:
1034:             if offset_tensor is not None and offset_tensor.requires_grad:
1035:                 grad_offset = np.zeros_like(offset_tensor.data)
1036:                 
1037:         # Compute gradients based on mode
1038:         if mode == ConvMode.STANDARD:
1039:             Conv2dFunction._backward_standard(
1040:                 grad_output, grad_x_padded, grad_weight, grad_bias,
1041:                 x_padded, weight, bias, stride, padding, dilation, groups,
1042:                 N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
1043:             )
1044:         elif mode == ConvMode.TRANSPOSED:
1045:             Conv2dFunction._backward_transposed(
1046:                 grad_output, grad_x, grad_weight, grad_bias,
1047:                 x, weight, bias, stride, padding, dilation, groups,
1048:                 N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
1049:             )
1050:         elif mode == ConvMode.DEFORMABLE:
1051:             Conv2dFunction._backward_deformable(
1052:                 grad_output, grad_x_padded, grad_weight, grad_bias, grad_offset, grad_mask,
1053:                 x_padded, weight, bias, offset_tensor, mask, stride, padding, dilation, groups,
1054:                 sampling_locations, N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
1055:             )
1056:             
1057:         # Assign gradients to grad_dict
1058:         if x.requires_grad:
1059:             if mode in [ConvMode.STANDARD, ConvMode.DEFORMABLE]:
1060:                 if padding[0] > 0 or padding[1] > 0:
1061:                     grad_x = grad_x_padded[:, :, 
1062:                                        padding[0]:grad_x_padded.shape[2]-padding[0],
1063:                                        padding[1]:grad_x_padded.shape[3]-padding[1]]
1064:                 else:
1065:                     grad_x = grad_x_padded
1066:             grad_dict[id(x)] = grad_x
1067:             
1068:         if weight.requires_grad:
1069:             grad_dict[id(weight)] = grad_weight
1070:             
1071:         if bias is not None and bias.requires_grad:
1072:             grad_dict[id(bias)] = grad_bias
1073:             
1074:         if offset_tensor is not None and offset_tensor.requires_grad:
1075:             grad_dict[id(offset_tensor)] = grad_offset
1076: 
1077:     @staticmethod
1078:     def _backward_standard(grad_output, grad_x_padded, grad_weight, grad_bias,
1079:                         x_padded, weight, bias, stride, padding, dilation, groups,
1080:                         N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out):
1081:         """Backward pass for standard convolution."""
1082:         for g in range(groups):
1083:             # Get weight and grad_output for current group
1084:             w_g = weight.data[g * C_out_per_group:(g + 1) * C_out_per_group]
1085:             grad_out_g = grad_output[:, g * C_out_per_group:(g + 1) * C_out_per_group]
1086:             
1087:             # Convert grad_output to columns
1088:             grad_out_col = grad_out_g.transpose(1, 0, 2, 3).reshape(C_out_per_group, -1)
1089:             
1090:             # Get input columns
1091:             x_cols = _im2col_dilated(
1092:                 x_padded[:, g * C_in_per_group:(g + 1) * C_in_per_group],
1093:                 (kH, kW), stride, dilation, padding, ConvMode.STANDARD
1094:             )
1095:             
1096:             # Compute weight gradients
1097:             grad_weight[g * C_out_per_group:(g + 1) * C_out_per_group] = \
1098:                 (grad_out_col @ x_cols.T).reshape(C_out_per_group, C_in_per_group, kH, kW)
1099:             
1100:             # Compute input gradients
1101:             w_reshaped = w_g.reshape(C_out_per_group, -1).T
1102:             grad_cols = w_reshaped @ grad_out_col
1103:             
1104:             # Reshape grad_cols to match the expected shape
1105:             grad_cols = grad_cols.reshape(-1, N * H_out * W_out)
1106:             
1107:             # Convert columns back to image format
1108:             grad_x_padded[:, g * C_in_per_group:(g + 1) * C_in_per_group] += \
1109:                 _col2im_dilated(grad_cols, x_padded.shape, (kH, kW), stride, dilation)
1110:         
1111:         # Compute bias gradients if needed
1112:         if bias is not None and grad_bias is not None:
1113:             grad_bias[:] = grad_output.sum(axis=(0, 2, 3))
1114: 
1115:     @staticmethod
1116:     def _backward_transposed(
1117:         grad_output_padded, grad_x, grad_weight, grad_bias,
1118:         x, weight, bias, stride, padding, dilation, groups,
1119:         N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
1120:     ):
1121:         """
1122:         Backward pass for transposed convolution.
1123:         """
1124:         for g in range(groups):
1125:             # Slice weights and grad_output for the current group
1126:             w_g = weight.data[g * C_out_per_group:(g + 1) * C_out_per_group]
1127:             grad_out_g = grad_output_padded[:, g * C_out_per_group:(g + 1) * C_out_per_group]
1128: 
1129:             # Reshape weights for gradient computation
1130:             w_flipped = np.flip(np.flip(w_g, 2), 3).transpose(1, 0, 2, 3)  # Flip kernel
1131:             w_reshaped = w_flipped.reshape(-1, C_out_per_group)  # Shape: (C_in_per_group * kH * kW, C_out_per_group)
1132: 
1133:             # Compute gradient columns with swapped stride and dilation
1134:             grad_cols = _im2col_dilated(
1135:                 grad_out_g,
1136:                 weight.shape[2:],
1137:                 dilation,  # Use dilation as stride
1138:                 stride,    # Use stride as dilation
1139:                 ConvMode.STANDARD
1140:             )
1141: 
1142:             # Compute gradient for input
1143:             grad_x[:, g * C_in_per_group:(g + 1) * C_in_per_group] += \
1144:                 (w_reshaped @ grad_cols).reshape(N, C_in_per_group, *x.shape[2:])
1145: 
1146:             # Compute gradient for weights
1147:             x_original = x.data[:, g * C_in_per_group:(g + 1) * C_in_per_group]
1148:             x_cols = _im2col_dilated(
1149:                 x_original,
1150:                 weight.shape[2:], stride, dilation, ConvMode.STANDARD, sampling_locations=None
1151:             )
1152:             for n in range(N):
1153:                 grad_out_n = grad_out_g[n].reshape(C_out_per_group, -1)  # Shape: (C_out_per_group, H_out * W_out)
1154:                 grad_weight[g * C_out_per_group:(g + 1) * C_out_per_group] += \
1155:                     (grad_out_n @ x_cols[:, n * H_out * W_out:(n + 1) * H_out * W_out].T).reshape(
1156:                         C_out_per_group, C_in_per_group, kH, kW
1157:                     )
1158: 
1159:         # Compute gradient for bias
1160:         if bias is not None:
1161:             grad_bias += grad_output_padded.sum(axis=(0, 2, 3))
1162: 
1163:     @staticmethod
1164:     def _backward_deformable(grad_output, grad_x_padded, grad_weight, grad_bias, grad_offset, grad_mask,
1165:                         x_padded, weight, bias, offset_tensor, mask, stride, padding, dilation, groups,
1166:                         sampling_locations, N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out,
1167:                         align_corners=True):
1168:         """Backward pass for deformable convolution."""
1169:         
1170:         if sampling_locations is None and offset_tensor is not None:
1171:             # If sampling_locations weren't provided but we have offset tensor, compute them
1172:             sampling_locations = _get_deformable_offsets(
1173:                 offset_tensor.data,
1174:                 (kH, kW),
1175:                 x_padded.shape,
1176:                 dilation
1177:             )
1178:         
1179:         # Now proceed with backward pass using sampling_locations
1180:         for g in range(groups):
1181:             w_g = weight.data[g * C_out_per_group:(g + 1) * C_out_per_group]
1182:             grad_out_g = grad_output[:, g * C_out_per_group:(g + 1) * C_out_per_group]
1183: 
1184:             for n in range(N):
1185:                 for h in range(H_out):
1186:                     for w_ in range(W_out):
1187:                         i = h * W_out + w_
1188:                         grad_out_slice = grad_out_g[n, :, h, w_]
1189: 
1190:                         for c_out in range(C_out_per_group):
1191:                             grad = grad_out_slice[c_out]
1192: 
1193:                             for c_in in range(C_in_per_group):
1194:                                 for kh in range(kH):
1195:                                     for kw in range(kW):
1196:                                         k_idx = kh * kW + kw
1197:                                         
1198:                                         if sampling_locations is not None:
1199:                                             loc = sampling_locations[n, i, k_idx]
1200:                                             h_in = int(loc[0])
1201:                                             w_in = int(loc[1])
1202:                                             
1203:                                             if (0 <= h_in < x_padded.shape[2] and 
1204:                                                 0 <= w_in < x_padded.shape[3]):
1205:                                                 # Update gradients
1206:                                                 if grad_x_padded is not None:
1207:                                                     grad_x_padded[n, g*C_in_per_group + c_in, h_in, w_in] += (
1208:                                                         grad * w_g[c_out, c_in, kh, kw]
1209:                                                     )
1210:                                                 if grad_weight is not None:
1211:                                                     grad_weight[g*C_out_per_group + c_out, c_in, kh, kw] += (
1212:                                                         grad * x_padded[n, g*C_in_per_group + c_in, h_in, w_in]
1213:                                                     )
1214: 
1215:         # Compute gradients for bias if needed
1216:         if grad_bias is not None and bias is not None:
1217:             grad_bias[:] = grad_output.sum((0, 2, 3))

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\elementwise.py
// ----------------------------------------
0001: from typing import Dict
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class Log(Function):
0006:     """
0007:     Natural logarithm operation.
0008:     
0009:     Forward: f(x) = ln(x)
0010:     Backward: f'(x) = 1/x
0011:     """
0012:     @staticmethod
0013:     def forward(ctx, x):
0014:         if not isinstance(x, Tensor):
0015:             x = Tensor(x)
0016:             
0017:         # Check for negative values
0018:         if np.any(x.data <= 0):
0019:             raise ValueError("Log of negative numbers or zero is undefined")
0020:             
0021:         ctx.save_for_backward(x)
0022:         return Tensor(np.log(x.data))
0023:         
0024:     @staticmethod
0025:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0026:         x, = ctx.saved_tensors
0027:         if x.requires_grad:
0028:             # d/dx(log(x)) = 1/x
0029:             grad_dict[id(x)] = grad_output / x.data
0030: 
0031: class Exp(Function):
0032:     """
0033:     Exponential operation.
0034:     
0035:     Forward: f(x) = exp(x)
0036:     Backward: f'(x) = exp(x)
0037:     """
0038:     @staticmethod
0039:     def forward(ctx, x):
0040:         if not isinstance(x, Tensor):
0041:             x = Tensor(x)
0042:             
0043:         result = np.exp(x.data)
0044:         ctx.save_for_backward(x)  # Save x for backward pass
0045:         ctx.save_arguments(exp_x=result)  # Save exp(x) as argument
0046:         return Tensor(result)
0047:         
0048:     @staticmethod
0049:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0050:         x, = ctx.saved_tensors
0051:         exp_x = ctx.saved_arguments['exp_x']
0052:         
0053:         if x.requires_grad:
0054:             # d/dx(exp(x)) = exp(x)
0055:             grad_dict[id(x)] = grad_output * exp_x

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\loss.py
// ----------------------------------------
0001: from typing import Dict, Optional
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class MSELoss(Function):
0006:     """
0007:     Mean Squared Error Loss: L = 1/N * Σ(y - ŷ)²
0008:     
0009:     Args:
0010:         reduction (str): Specifies the reduction to apply to the output:
0011:             'mean' (default) | 'sum' | 'none'
0012:     """
0013:     
0014:     @staticmethod
0015:     def forward(ctx, predictions, targets, reduction='mean'):
0016:         if not isinstance(predictions, Tensor):
0017:             predictions = Tensor(predictions)
0018:         if not isinstance(targets, Tensor):
0019:             targets = Tensor(targets)
0020:             
0021:         if predictions.shape != targets.shape:
0022:             raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
0023:             
0024:         diff = predictions.data - targets.data
0025:         squared_diff = diff * diff
0026:         
0027:         if reduction == 'none':
0028:             result = squared_diff
0029:         elif reduction == 'sum':
0030:             result = np.sum(squared_diff)
0031:         elif reduction == 'mean':
0032:             result = np.mean(squared_diff)
0033:         else:
0034:             raise ValueError(f"Invalid reduction method: {reduction}")
0035:             
0036:         ctx.save_for_backward(predictions, targets)
0037:         ctx.save_arguments(reduction=reduction)
0038:         return Tensor(result)
0039:         
0040:     @staticmethod
0041:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0042:         predictions, targets = ctx.saved_tensors
0043:         reduction = ctx.saved_arguments['reduction']
0044:         
0045:         diff = predictions.data - targets.data
0046:         
0047:         if reduction == 'mean':
0048:             grad = grad_output * 2 * diff / np.prod(diff.shape)
0049:         elif reduction == 'sum':
0050:             grad = grad_output * 2 * diff
0051:         else:  # 'none'
0052:             grad = grad_output * 2 * diff
0053:             
0054:         if predictions.requires_grad:
0055:             grad_dict[id(predictions)] = grad
0056: 
0057: class CrossEntropyLoss(Function):
0058:     """
0059:     Cross Entropy Loss with built-in LogSoftmax: L = -Σ y_true * log(softmax(y_pred))
0060:     
0061:     Args:
0062:         reduction (str): Specifies the reduction to apply to the output:
0063:             'mean' (default) | 'sum' | 'none'
0064:     """
0065:     
0066:     @staticmethod
0067:     def _log_softmax(x):
0068:         # Compute log(softmax(x)) in a numerically stable way
0069:         max_x = np.max(x, axis=1, keepdims=True)
0070:         exp_x = np.exp(x - max_x)
0071:         sum_exp_x = np.sum(exp_x, axis=1, keepdims=True)
0072:         return (x - max_x) - np.log(sum_exp_x)
0073:         
0074:     @staticmethod
0075:     def forward(ctx, predictions, targets, reduction='mean'):
0076:         if not isinstance(predictions, Tensor):
0077:             predictions = Tensor(predictions)
0078:         if not isinstance(targets, Tensor):
0079:             targets = Tensor(targets)
0080:             
0081:         log_softmax = CrossEntropyLoss._log_softmax(predictions.data)
0082:         nll_loss = -np.sum(targets.data * log_softmax, axis=1)
0083:         
0084:         if reduction == 'none':
0085:             result = nll_loss
0086:         elif reduction == 'sum':
0087:             result = np.sum(nll_loss)
0088:         elif reduction == 'mean':
0089:             result = np.mean(nll_loss)
0090:         else:
0091:             raise ValueError(f"Invalid reduction method: {reduction}")
0092:             
0093:         ctx.save_for_backward(predictions, targets)
0094:         ctx.save_arguments(reduction=reduction, log_softmax=log_softmax)
0095:         return Tensor(result)
0096:         
0097:     @staticmethod
0098:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0099:         predictions, targets = ctx.saved_tensors
0100:         reduction = ctx.saved_arguments['reduction']
0101:         log_softmax = ctx.saved_arguments['log_softmax']
0102:         
0103:         grad_output = np.array(grad_output)
0104:         if reduction == 'mean':
0105:             grad_output = grad_output / len(targets.data)
0106:         
0107:         softmax = np.exp(log_softmax)
0108:         grad = grad_output.reshape(-1, 1) * (softmax - targets.data)
0109:             
0110:         if predictions.requires_grad:
0111:             grad_dict[id(predictions)] = grad
0112: 
0113: class BinaryCrossEntropyLoss(Function):
0114:     """
0115:     Binary Cross Entropy Loss: L = -Σ (y * log(p) + (1-y) * log(1-p))
0116:     
0117:     Args:
0118:         reduction (str): Specifies the reduction to apply to the output:
0119:             'mean' (default) | 'sum' | 'none'
0120:         eps (float): Small value for numerical stability
0121:     """
0122:     
0123:     @staticmethod
0124:     def forward(ctx, predictions, targets, reduction='mean', eps=1e-7):
0125:         if not isinstance(predictions, Tensor):
0126:             predictions = Tensor(predictions)
0127:         if not isinstance(targets, Tensor):
0128:             targets = Tensor(targets)
0129: 
0130:         # Check valid probability values
0131:         if np.any(predictions.data < 0) or np.any(predictions.data > 1):
0132:             raise ValueError("Predictions must be in range [0, 1]")
0133:             
0134:         # Clip predictions to prevent log(0)
0135:         predictions_clipped = np.clip(predictions.data, eps, 1 - eps)
0136:         
0137:         loss = -(targets.data * np.log(predictions_clipped) + 
0138:                 (1 - targets.data) * np.log(1 - predictions_clipped))
0139:                 
0140:         if reduction == 'none':
0141:             result = loss
0142:         elif reduction == 'sum':
0143:             result = float(np.sum(loss))  # Convert to scalar
0144:         elif reduction == 'mean':
0145:             result = float(np.mean(loss))  # Convert to scalar
0146:         else:
0147:             raise ValueError(f"Invalid reduction method: {reduction}")
0148:             
0149:         ctx.save_for_backward(predictions, targets)
0150:         ctx.save_arguments(reduction=reduction, eps=eps)
0151:         return Tensor(result)
0152:         
0153:     @staticmethod
0154:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0155:         predictions, targets = ctx.saved_tensors
0156:         reduction = ctx.saved_arguments['reduction']
0157:         eps = ctx.saved_arguments['eps']
0158:         
0159:         predictions_clipped = np.clip(predictions.data, eps, 1 - eps)
0160:         
0161:         grad = grad_output * (predictions_clipped - targets.data) / (
0162:             predictions_clipped * (1 - predictions_clipped))
0163:             
0164:         if reduction == 'mean':
0165:             grad = grad / np.prod(targets.shape)
0166:             
0167:         if predictions.requires_grad:
0168:             grad_dict[id(predictions)] = grad
0169: 
0170: class L1Loss(Function):
0171:     """
0172:     L1 Loss (Mean Absolute Error): L = |y - ŷ|
0173:     
0174:     Args:
0175:         reduction (str): Specifies the reduction to apply to the output:
0176:             'mean' (default) | 'sum' | 'none'
0177:     """
0178:     
0179:     @staticmethod
0180:     def forward(ctx, predictions, targets, reduction='mean'):
0181:         if not isinstance(predictions, Tensor):
0182:             predictions = Tensor(predictions)
0183:         if not isinstance(targets, Tensor):
0184:             targets = Tensor(targets)
0185:             
0186:         if predictions.shape != targets.shape:
0187:             raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
0188:             
0189:         diff = predictions.data - targets.data
0190:         abs_diff = np.abs(diff)
0191:         
0192:         if reduction == 'none':
0193:             result = abs_diff
0194:         elif reduction == 'sum':
0195:             result = np.sum(abs_diff)
0196:         elif reduction == 'mean':
0197:             result = np.mean(abs_diff)
0198:         else:
0199:             raise ValueError(f"Invalid reduction method: {reduction}")
0200:             
0201:         ctx.save_for_backward(predictions, targets)
0202:         ctx.save_arguments(reduction=reduction)
0203:         return Tensor(result)
0204:         
0205:     @staticmethod
0206:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0207:         predictions, targets = ctx.saved_tensors
0208:         reduction = ctx.saved_arguments['reduction']
0209:         
0210:         diff = predictions.data - targets.data
0211:         grad = np.sign(diff)
0212:         
0213:         if reduction == 'mean':
0214:             grad = grad * grad_output / np.prod(diff.shape)
0215:         else:  # 'sum' or 'none'
0216:             grad = grad * grad_output
0217:             
0218:         if predictions.requires_grad:
0219:             grad_dict[id(predictions)] = grad
0220: 
0221: class KLDivLoss(Function):
0222:     """
0223:     Kullback-Leibler Divergence Loss.
0224:     KL divergence measures the relative entropy between two probability distributions.
0225:     
0226:     Args:
0227:         reduction (str): Specifies the reduction to apply to the output:
0228:             'mean' (default) | 'sum' | 'none'
0229:         log_target (bool): If True, target is expected to be log-probabilities
0230:     """
0231:     
0232:     @staticmethod
0233:     def forward(ctx, predictions, targets, reduction='mean', log_target=False):
0234:         if not isinstance(predictions, Tensor):
0235:             predictions = Tensor(predictions)
0236:         if not isinstance(targets, Tensor):
0237:             targets = Tensor(targets)
0238:             
0239:         if not log_target:
0240:             targets_log = np.log(np.clip(targets.data, 1e-7, 1.0))
0241:         else:
0242:             targets_log = targets.data
0243:             
0244:         # KL divergence formula: KL(P||Q) = P * (log(P) - log(Q))
0245:         loss = np.exp(targets_log) * (targets_log - predictions.data)
0246:         loss = -loss  # Correct the sign to make it positive
0247:         
0248:         if reduction == 'none':
0249:             result = loss
0250:         elif reduction == 'sum':
0251:             result = np.sum(loss)
0252:         elif reduction == 'mean':
0253:             result = np.mean(loss)
0254:         else:
0255:             raise ValueError(f"Invalid reduction method: {reduction}")
0256:             
0257:         ctx.save_for_backward(predictions, targets)
0258:         ctx.save_arguments(reduction=reduction, log_target=log_target)
0259:         return Tensor(result)
0260:         
0261:     @staticmethod
0262:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0263:         predictions, targets = ctx.saved_tensors
0264:         reduction = ctx.saved_arguments['reduction']
0265:         log_target = ctx.saved_arguments['log_target']
0266:         
0267:         if not log_target:
0268:             targets_log = np.log(np.clip(targets.data, 1e-7, 1.0))
0269:         else:
0270:             targets_log = targets.data
0271:             
0272:         grad = -np.exp(targets_log) * grad_output
0273:         
0274:         if reduction == 'mean':
0275:             grad = grad / np.prod(predictions.shape)
0276:             
0277:         if predictions.requires_grad:
0278:             grad_dict[id(predictions)] = grad
0279: 
0280: class CosineSimilarityLoss(Function):
0281:     """
0282:     Cosine Similarity Loss.
0283:     Measures the cosine similarity between two vectors.
0284:     
0285:     Args:
0286:         dim (int): Dimension along which cosine similarity is computed
0287:         eps (float): Small value to avoid division by zero
0288:         reduction (str): Specifies the reduction to apply to the output
0289:     """
0290:     
0291:     @staticmethod
0292:     def forward(ctx, x1, x2, dim=1, eps=1e-8, reduction='mean'):
0293:         if not isinstance(x1, Tensor):
0294:             x1 = Tensor(x1)
0295:         if not isinstance(x2, Tensor):
0296:             x2 = Tensor(x2)
0297:             
0298:         # Compute norms
0299:         norm1 = np.sqrt(np.sum(x1.data * x1.data, axis=dim, keepdims=True))
0300:         norm2 = np.sqrt(np.sum(x2.data * x2.data, axis=dim, keepdims=True))
0301:         
0302:         # Normalize vectors
0303:         x1_normalized = x1.data / np.maximum(norm1, eps)
0304:         x2_normalized = x2.data / np.maximum(norm2, eps)
0305:         
0306:         # Compute cosine similarity
0307:         cos_sim = np.sum(x1_normalized * x2_normalized, axis=dim)
0308:         
0309:         # For orthogonal vectors, cos_sim = 0, we want loss = 1
0310:         # For identical vectors, cos_sim = 1, we want loss = 0
0311:         # Therefore, loss = 1 - cos_sim
0312:         if reduction == 'none':
0313:             result = 1 - cos_sim
0314:         elif reduction == 'sum':
0315:             result = float(np.sum(1 - cos_sim))
0316:         elif reduction == 'mean':
0317:             result = float(np.mean(1 - cos_sim))
0318:         else:
0319:             raise ValueError(f"Invalid reduction method: {reduction}")
0320:             
0321:         ctx.save_for_backward(x1, x2)
0322:         ctx.save_arguments(dim=dim, eps=eps, reduction=reduction,
0323:                          x1_normalized=x1_normalized,
0324:                          x2_normalized=x2_normalized)
0325:         return Tensor(result)
0326:         
0327:     @staticmethod
0328:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0329:         x1, x2 = ctx.saved_tensors
0330:         dim = ctx.saved_arguments['dim']
0331:         eps = ctx.saved_arguments['eps']
0332:         reduction = ctx.saved_arguments['reduction']
0333:         x1_normalized = ctx.saved_arguments['x1_normalized']
0334:         x2_normalized = ctx.saved_arguments['x2_normalized']
0335:         
0336:         if reduction == 'mean':
0337:             grad_output = grad_output / x1.shape[0]
0338:         
0339:         # Gradient with respect to x1
0340:         if x1.requires_grad:
0341:             grad_x1 = -grad_output[..., None] * x2_normalized
0342:             grad_dict[id(x1)] = grad_x1
0343:             
0344:         # Gradient with respect to x2
0345:         if x2.requires_grad:
0346:             grad_x2 = -grad_output[..., None] * x1_normalized
0347:             grad_dict[id(x2)] = grad_x2
0348: 
0349: class HingeLoss(Function):
0350:     """
0351:     Hinge Loss (max-margin loss).
0352:     Commonly used for SVM training.
0353:     L = max(0, margin - y * f(x))
0354:     
0355:     Args:
0356:         margin (float): Margin in the hinge loss
0357:         reduction (str): Specifies the reduction to apply to the output
0358:     """
0359:     
0360:     @staticmethod
0361:     def forward(ctx, predictions, targets, margin=1.0, reduction='mean'):
0362:         if not isinstance(predictions, Tensor):
0363:             predictions = Tensor(predictions)
0364:         if not isinstance(targets, Tensor):
0365:             targets = Tensor(targets)
0366:             
0367:         # Convert targets to ±1
0368:         signed_targets = 2.0 * targets.data - 1.0
0369:         
0370:         # Compute raw hinge loss
0371:         loss = np.maximum(0, margin - signed_targets * predictions.data)
0372:         
0373:         if reduction == 'none':
0374:             result = loss
0375:         elif reduction == 'sum':
0376:             result = np.sum(loss)
0377:         elif reduction == 'mean':
0378:             result = np.mean(loss)
0379:         else:
0380:             raise ValueError(f"Invalid reduction method: {reduction}")
0381:             
0382:         ctx.save_for_backward(predictions, targets)
0383:         ctx.save_arguments(margin=margin, reduction=reduction,
0384:                          signed_targets=signed_targets)
0385:         return Tensor(result)
0386:         
0387:     @staticmethod
0388:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0389:         predictions, targets = ctx.saved_tensors
0390:         margin = ctx.saved_arguments['margin']
0391:         reduction = ctx.saved_arguments['reduction']
0392:         signed_targets = ctx.saved_arguments['signed_targets']
0393:         
0394:         # Gradient is -y when margin - y*f(x) > 0, 0 otherwise
0395:         mask = (margin - signed_targets * predictions.data) > 0
0396:         grad = -signed_targets * mask * grad_output
0397:         
0398:         if reduction == 'mean':
0399:             grad = grad / np.prod(predictions.shape)
0400:             
0401:         if predictions.requires_grad:
0402:             grad_dict[id(predictions)] = grad
0403: 
0404: class FocalLoss(Function):
0405:     """
0406:     Focal Loss.
0407:     Addresses class imbalance by down-weighting easily classified examples.
0408:     FL(p) = -alpha * (1-p)^gamma * log(p)
0409:     
0410:     Args:
0411:         alpha (float): Weighting factor for rare classes
0412:         gamma (float): Focusing parameter
0413:         reduction (str): Specifies the reduction to apply to the output
0414:     """
0415:     
0416:     @staticmethod
0417:     def forward(ctx, predictions, targets, alpha=0.25, gamma=2.0, reduction='mean'):
0418:         if not isinstance(predictions, Tensor):
0419:             predictions = Tensor(predictions)
0420:         if not isinstance(targets, Tensor):
0421:             targets = Tensor(targets)
0422:             
0423:         # Clip predictions for numerical stability
0424:         eps = 1e-7
0425:         predictions_clipped = np.clip(predictions.data, eps, 1 - eps)
0426:         
0427:         # Compute pt (probability of target class)
0428:         pt = predictions_clipped * targets.data + (1 - predictions_clipped) * (1 - targets.data)
0429:         
0430:         # Compute focal weight
0431:         focal_weight = alpha * ((1 - pt) ** gamma)
0432:         
0433:         # Compute binary cross entropy
0434:         bce = -(targets.data * np.log(predictions_clipped) + 
0435:                 (1 - targets.data) * np.log(1 - predictions_clipped))
0436:         
0437:         # Apply focal weight
0438:         loss = focal_weight * bce
0439:         
0440:         if reduction == 'none':
0441:             result = loss
0442:         elif reduction == 'sum':
0443:             result = np.sum(loss)
0444:         elif reduction == 'mean':
0445:             result = np.mean(loss)
0446:         else:
0447:             raise ValueError(f"Invalid reduction method: {reduction}")
0448:             
0449:         ctx.save_for_backward(predictions, targets)
0450:         ctx.save_arguments(alpha=alpha, gamma=gamma, reduction=reduction,
0451:                          pt=pt, focal_weight=focal_weight)
0452:         return Tensor(result)
0453:         
0454:     @staticmethod
0455:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0456:         predictions, targets = ctx.saved_tensors
0457:         alpha = ctx.saved_arguments['alpha']
0458:         gamma = ctx.saved_arguments['gamma']
0459:         reduction = ctx.saved_arguments['reduction']
0460:         pt = ctx.saved_arguments['pt']
0461:         focal_weight = ctx.saved_arguments['focal_weight']
0462:         
0463:         # Compute gradient
0464:         grad = grad_output * focal_weight * (
0465:             gamma * pt * np.log(pt) + pt - targets.data
0466:         )
0467:         
0468:         if reduction == 'mean':
0469:             grad = grad / np.prod(predictions.shape)
0470:             
0471:         if predictions.requires_grad:
0472:             grad_dict[id(predictions)] = grad
0473: 
0474: class HuberLoss(Function):
0475:     """
0476:     Huber Loss: L = 0.5 * (y - ŷ)² if |y - ŷ| <= delta else delta * |y - ŷ| - 0.5 * delta²
0477:     
0478:     This loss combines the best properties of MSE and L1 loss.
0479:     For small errors it behaves like MSE, for large errors it behaves like L1.
0480:     
0481:     Args:
0482:         delta (float): Threshold where loss transitions from squared to linear
0483:         reduction (str): Specifies the reduction to apply to the output:
0484:             'mean' (default) | 'sum' | 'none'
0485:     """
0486:     
0487:     @staticmethod
0488:     def forward(ctx, predictions, targets, delta=1.0, reduction='mean'):
0489:         if not isinstance(predictions, Tensor):
0490:             predictions = Tensor(predictions)
0491:         if not isinstance(targets, Tensor):
0492:             targets = Tensor(targets)
0493:             
0494:         if predictions.shape != targets.shape:
0495:             raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
0496:             
0497:         diff = predictions.data - targets.data
0498:         abs_diff = np.abs(diff)
0499:         quadratic = np.minimum(abs_diff, delta)
0500:         linear = abs_diff - quadratic
0501:         loss = 0.5 * quadratic ** 2 + delta * linear
0502:         
0503:         if reduction == 'none':
0504:             result = loss
0505:         elif reduction == 'sum':
0506:             result = np.sum(loss)
0507:         elif reduction == 'mean':
0508:             result = np.mean(loss)
0509:         else:
0510:             raise ValueError(f"Invalid reduction method: {reduction}")
0511:             
0512:         ctx.save_for_backward(predictions, targets)
0513:         ctx.save_arguments(delta=delta, reduction=reduction)
0514:         return Tensor(result)
0515:         
0516:     @staticmethod
0517:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0518:         predictions, targets = ctx.saved_tensors
0519:         delta = ctx.saved_arguments['delta']
0520:         reduction = ctx.saved_arguments['reduction']
0521:         
0522:         diff = predictions.data - targets.data
0523:         abs_diff = np.abs(diff)
0524:         
0525:         # Gradient is diff/|diff| * min(|diff|, delta)
0526:         grad = np.sign(diff) * np.minimum(abs_diff, delta)
0527:         
0528:         if reduction == 'mean':
0529:             grad = grad * grad_output / np.prod(diff.shape)
0530:         else:  # 'sum' or 'none'
0531:             grad = grad * grad_output
0532:             
0533:         if predictions.requires_grad:
0534:             grad_dict[id(predictions)] = grad

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\matrix.py
// ----------------------------------------
0001: from typing import Dict, Optional, Union, Tuple
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class Transpose(Function):
0006:     @staticmethod
0007:     def forward(ctx, x, axes: Optional[Tuple[int, ...]] = None):
0008:         if not isinstance(x, Tensor):
0009:             x = Tensor(x)
0010:             
0011:         ctx.save_for_backward(x)
0012:         ctx.save_arguments(axes=axes)
0013:         
0014:         if axes is None:
0015:             return Tensor(np.transpose(x.data))
0016:         return Tensor(np.transpose(x.data, axes))
0017:         
0018:     @staticmethod
0019:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0020:         x, = ctx.saved_tensors
0021:         axes = ctx.saved_arguments['axes']
0022:         
0023:         if x.requires_grad:
0024:             if axes is None:
0025:                 # For standard transpose, just transpose the gradient
0026:                 grad_dict[id(x)] = np.transpose(grad_output)
0027:             else:
0028:                 # For specific axes, need to invert the permutation
0029:                 inverse_axes = np.argsort(axes)
0030:                 grad_dict[id(x)] = np.transpose(grad_output, inverse_axes)
0031: 
0032: class Compare(Function):
0033:     """Base class for comparison operations"""
0034:     @staticmethod
0035:     def _compare(op, x1, x2):
0036:         if not isinstance(x1, Tensor):
0037:             x1 = Tensor(x1)
0038:         if not isinstance(x2, Tensor):
0039:             x2 = Tensor(x2)
0040:             
0041:         return Tensor(op(x1.data, x2.data))
0042:         
0043:     @staticmethod
0044:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0045:         # Comparison operations have no gradient
0046:         pass
0047: 
0048: class Greater(Compare):
0049:     @staticmethod
0050:     def forward(ctx, x1, x2):
0051:         return Compare._compare(np.greater, x1, x2)
0052: 
0053: class GreaterEqual(Compare):
0054:     @staticmethod
0055:     def forward(ctx, x1, x2):
0056:         return Compare._compare(np.greater_equal, x1, x2)
0057: 
0058: class Less(Compare):
0059:     @staticmethod
0060:     def forward(ctx, x1, x2):
0061:         return Compare._compare(np.less, x1, x2)
0062: 
0063: class LessEqual(Compare):
0064:     @staticmethod
0065:     def forward(ctx, x1, x2):
0066:         return Compare._compare(np.less_equal, x1, x2)
0067: 
0068: class Equal(Compare):
0069:     @staticmethod
0070:     def forward(ctx, x1, x2):
0071:         return Compare._compare(np.equal, x1, x2)
0072: 
0073: class NotEqual(Compare):
0074:     @staticmethod
0075:     def forward(ctx, x1, x2):
0076:         return Compare._compare(np.not_equal, x1, x2)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\pooling.py
// ----------------------------------------
0001: from typing import Tuple, Dict
0002: import numpy as np
0003: from ..core import Tensor, Function
0004: from ..core.context import Context
0005: 
0006: def _compute_output_shape(input_size: Tuple[int, int], kernel_size: Tuple[int, int],
0007:                        stride: Tuple[int, int], padding: Tuple[int, int]) -> Tuple[int, int]:
0008:     """Calculate output shape for pooling operations."""
0009:     H_out = ((input_size[0] + 2 * padding[0] - kernel_size[0]) // stride[0] + 1)
0010:     W_out = ((input_size[1] + 2 * padding[1] - kernel_size[1]) // stride[1] + 1)
0011:     return (H_out, W_out)
0012: 
0013: def _pad_input(x: np.ndarray, padding: Tuple[int, int]) -> np.ndarray:
0014:     """Add padding to input tensor."""
0015:     if padding[0] == 0 and padding[1] == 0:
0016:         return x
0017:     return np.pad(x, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])),
0018:                  mode='constant', constant_values=0)
0019: 
0020: class MaxPool2dFunction(Function):
0021:     """Function implementing 2D max pooling."""
0022:     
0023:     @staticmethod
0024:     def forward(ctx: Context, x: Tensor, kernel_size: Tuple[int, int],
0025:               stride: Tuple[int, int], padding: Tuple[int, int]) -> Tensor:
0026:         # Save params for backward pass
0027:         ctx.save_arguments(kernel_size=kernel_size, stride=stride, padding=padding)
0028:         
0029:         # Apply padding
0030:         x_padded = _pad_input(x.data, padding)
0031:         N, C, H, W = x_padded.shape
0032:         kH, kW = kernel_size
0033:         sH, sW = stride
0034:         
0035:         # Calculate output dimensions
0036:         H_out = ((H - kH) // sH) + 1
0037:         W_out = ((W - kW) // sW) + 1
0038:         
0039:         # Initialize output and max indices for backward pass
0040:         output = np.zeros((N, C, H_out, W_out))
0041:         max_indices = np.zeros((N, C, H_out, W_out, 2), dtype=np.int32)
0042:         
0043:         # Compute max pooling
0044:         for n in range(N):
0045:             for c in range(C):
0046:                 for h in range(H_out):
0047:                     for w in range(W_out):
0048:                         h_start = h * sH
0049:                         h_end = h_start + kH
0050:                         w_start = w * sW
0051:                         w_end = w_start + kW
0052:                         
0053:                         window = x_padded[n, c, h_start:h_end, w_start:w_end]
0054:                         output[n, c, h, w] = np.max(window)
0055:                         max_idx = np.unravel_index(np.argmax(window), window.shape)
0056:                         max_indices[n, c, h, w] = [h_start + max_idx[0], w_start + max_idx[1]]
0057:                         
0058:         ctx.save_for_backward(x)
0059:         ctx.save_arguments(max_indices=max_indices)
0060:         return Tensor(output)
0061:     
0062:     @staticmethod
0063:     def backward(ctx: Context, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0064:         x, = ctx.saved_tensors
0065:         max_indices = ctx.saved_arguments['max_indices']
0066:         
0067:         if x.requires_grad:
0068:             grad = np.zeros_like(x.data)
0069:             N, C, H_out, W_out = grad_output.shape
0070:             
0071:             # Distribute gradients to max positions
0072:             for n in range(N):
0073:                 for c in range(C):
0074:                     for h in range(H_out):
0075:                         for w in range(W_out):
0076:                             h_max, w_max = max_indices[n, c, h, w]
0077:                             grad[n, c, h_max, w_max] += grad_output[n, c, h, w]
0078:                             
0079:             grad_dict[id(x)] = grad
0080: 
0081: class AvgPool2dFunction(Function):
0082:     """Function implementing 2D average pooling."""
0083:     
0084:     @staticmethod
0085:     def forward(ctx: Context, x: Tensor, kernel_size: Tuple[int, int],
0086:               stride: Tuple[int, int], padding: Tuple[int, int]) -> Tensor:
0087:         # Save params for backward pass
0088:         ctx.save_arguments(kernel_size=kernel_size, stride=stride, padding=padding)
0089:         
0090:         # Apply padding
0091:         x_padded = _pad_input(x.data, padding)
0092:         N, C, H, W = x_padded.shape
0093:         kH, kW = kernel_size
0094:         sH, sW = stride
0095:         
0096:         # Calculate output dimensions
0097:         H_out = ((H - kH) // sH) + 1
0098:         W_out = ((W - kW) // sW) + 1
0099:         
0100:         output = np.zeros((N, C, H_out, W_out))
0101:         
0102:         # Compute average pooling
0103:         for n in range(N):
0104:             for c in range(C):
0105:                 for h in range(H_out):
0106:                     for w in range(W_out):
0107:                         h_start = h * sH
0108:                         h_end = h_start + kH
0109:                         w_start = w * sW
0110:                         w_end = w_start + kW
0111:                         window = x_padded[n, c, h_start:h_end, w_start:w_end]
0112:                         output[n, c, h, w] = np.mean(window)
0113:                         
0114:         ctx.save_for_backward(x)
0115:         return Tensor(output)
0116:     
0117:     @staticmethod
0118:     def backward(ctx: Context, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0119:         x, = ctx.saved_tensors
0120:         kernel_size = ctx.saved_arguments['kernel_size']
0121:         stride = ctx.saved_arguments['stride']
0122:         padding = ctx.saved_arguments['padding']
0123:         
0124:         if x.requires_grad:
0125:             kH, kW = kernel_size
0126:             grad = np.zeros_like(x.data)
0127:             N, C, H_out, W_out = grad_output.shape
0128:             
0129:             # Distribute gradients uniformly within each pooling window
0130:             scale = 1.0 / (kH * kW)
0131:             for n in range(N):
0132:                 for c in range(C):
0133:                     for h in range(H_out):
0134:                         for w in range(W_out):
0135:                             h_start = h * stride[0]
0136:                             h_end = h_start + kH
0137:                             w_start = w * stride[1]
0138:                             w_end = w_start + kW
0139:                             grad[n, c, h_start:h_end, w_start:w_end] += grad_output[n, c, h, w] * scale
0140:                             
0141:             grad_dict[id(x)] = grad

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\power.py
// ----------------------------------------
0001: from typing import Dict
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class Power(Function):
0006:     @staticmethod
0007:     def forward(ctx, base, exponent):
0008:         if not isinstance(base, Tensor):
0009:             base = Tensor(base)
0010:         if not isinstance(exponent, (Tensor, int, float)):
0011:             raise TypeError("Exponent must be a Tensor, int, or float")
0012:             
0013:         # Convert Tensor exponent to scalar if possible
0014:         if isinstance(exponent, Tensor):
0015:             if exponent.data.size == 1:
0016:                 exponent = float(exponent.data)
0017:             else:
0018:                 raise ValueError("Only scalar exponents are supported")
0019:                 
0020:         ctx.save_for_backward(base)
0021:         ctx.save_arguments(exponent=exponent)
0022:         
0023:         return Tensor(np.power(base.data, exponent))
0024:         
0025:     @staticmethod
0026:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0027:         base, = ctx.saved_tensors
0028:         exponent = ctx.saved_arguments['exponent']
0029:         
0030:         if base.requires_grad:
0031:             # d/dx(x^n) = nx^(n-1)
0032:             grad = grad_output * exponent * np.power(base.data, exponent - 1)
0033:             grad_dict[id(base)] = grad
0034: 
0035: class Divide(Function):
0036:     @staticmethod
0037:     def forward(ctx, numerator, denominator):
0038:         if not isinstance(numerator, Tensor):
0039:             numerator = Tensor(numerator)
0040:         if not isinstance(denominator, Tensor):
0041:             denominator = Tensor(denominator)
0042:             
0043:         # Check for division by zero
0044:         if np.any(denominator.data == 0):
0045:             raise ValueError("Division by zero encountered")
0046:             
0047:         ctx.save_for_backward(numerator, denominator)
0048:         return Tensor(numerator.data / denominator.data)
0049:         
0050:     @staticmethod
0051:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0052:         numerator, denominator = ctx.saved_tensors
0053:         
0054:         if numerator.requires_grad:
0055:             # d/dx(x/y) = 1/y
0056:             grad_dict[id(numerator)] = grad_output / denominator.data
0057:             
0058:         if denominator.requires_grad:
0059:             # d/dy(x/y) = -x/y^2
0060:             grad_dict[id(denominator)] = -grad_output * numerator.data / (denominator.data ** 2)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\reduction.py
// ----------------------------------------
0001: from typing import Dict, Optional, Union, Tuple
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class Sum(Function):
0006:     @staticmethod
0007:     def forward(ctx, x, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False):
0008:         if not isinstance(x, Tensor):
0009:             x = Tensor(x)
0010:             
0011:         ctx.save_for_backward(x)
0012:         ctx.save_arguments(axis=axis, keepdims=keepdims, input_shape=x.shape)
0013:         
0014:         return Tensor(np.sum(x.data, axis=axis, keepdims=keepdims))
0015:         
0016:     @staticmethod
0017:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0018:         x, = ctx.saved_tensors
0019:         axis = ctx.saved_arguments['axis']
0020:         keepdims = ctx.saved_arguments['keepdims']
0021:         input_shape = ctx.saved_arguments['input_shape']
0022:         
0023:         if x.requires_grad:
0024:             # If not keeping dims, need to reshape grad_output to match broadcast
0025:             if not keepdims and axis is not None:
0026:                 grad_output = np.expand_dims(grad_output, axis=axis)
0027:                 
0028:             # Broadcast gradient to match input shape
0029:             grad = np.broadcast_to(grad_output, input_shape)
0030:             grad_dict[id(x)] = grad
0031: 
0032: class Mean(Function):
0033:     @staticmethod
0034:     def forward(ctx, x, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False):
0035:         if not isinstance(x, Tensor):
0036:             x = Tensor(x)
0037:             
0038:         ctx.save_for_backward(x)
0039:         ctx.save_arguments(axis=axis, keepdims=keepdims, input_shape=x.shape)
0040:         
0041:         return Tensor(np.mean(x.data, axis=axis, keepdims=keepdims))
0042:         
0043:     @staticmethod
0044:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0045:         x, = ctx.saved_tensors
0046:         axis = ctx.saved_arguments['axis']
0047:         keepdims = ctx.saved_arguments['keepdims']
0048:         input_shape = ctx.saved_arguments['input_shape']
0049:         
0050:         if x.requires_grad:
0051:             # If not keeping dims, need to reshape grad_output to match broadcast
0052:             if not keepdims and axis is not None:
0053:                 grad_output = np.expand_dims(grad_output, axis=axis)
0054:                 
0055:             # Calculate number of elements we're taking mean over
0056:             if axis is None:
0057:                 n = np.prod(input_shape)
0058:             else:
0059:                 n = np.prod([input_shape[i] for i in (axis,) if i < len(input_shape)])
0060:                 
0061:             # Broadcast gradient to match input shape and divide by n
0062:             grad = np.broadcast_to(grad_output, input_shape) / n
0063:             grad_dict[id(x)] = grad
0064: 
0065: class Max(Function):
0066:     @staticmethod
0067:     def forward(ctx, x, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False):
0068:         if not isinstance(x, Tensor):
0069:             x = Tensor(x)
0070:             
0071:         result = np.amax(x.data, axis=axis, keepdims=True)
0072:         ctx.save_for_backward(x)
0073:         ctx.save_arguments(axis=axis, keepdims=keepdims, max_vals=result)
0074:         
0075:         if not keepdims:
0076:             result = np.squeeze(result, axis=axis)
0077:             
0078:         return Tensor(result)
0079:         
0080:     @staticmethod
0081:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0082:         x, = ctx.saved_tensors
0083:         axis = ctx.saved_arguments['axis']
0084:         keepdims = ctx.saved_arguments['keepdims']
0085:         max_vals = ctx.saved_arguments['max_vals']
0086:         
0087:         if x.requires_grad:
0088:             # If not keeping dims, need to reshape grad_output
0089:             if not keepdims and axis is not None:
0090:                 grad_output = np.expand_dims(grad_output, axis=axis)
0091:                 
0092:             # Create gradient mask (1 where x equals max, 0 elsewhere)
0093:             mask = (x.data == max_vals)
0094:             
0095:             # In case of multiple maxima, distribute gradient equally
0096:             mask = mask / np.sum(mask, axis=axis, keepdims=True)
0097:             
0098:             grad_dict[id(x)] = grad_output * mask

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\reshape.py
// ----------------------------------------
0001: # DLpy/ops/reshape.py
0002: from ..core.function import Function
0003: from ..core.tensor import Tensor
0004: import numpy as np
0005: from typing import Dict
0006: 
0007: class Reshape(Function):
0008:     @staticmethod
0009:     def forward(ctx, tensor, shape):
0010:         # Save both the input tensor and the target shape
0011:         ctx.save_for_backward(tensor)
0012:         ctx.save_arguments(target_shape=shape)
0013:         # Create and return a new tensor with the reshaped data
0014:         return Tensor(tensor.data.reshape(shape))
0015:         
0016:     @staticmethod
0017:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0018:         # Get the original tensor and reshape the gradient back to its shape
0019:         original_tensor, = ctx.saved_tensors
0020:         if original_tensor.requires_grad:
0021:             # Reshape gradient back to the original tensor's shape
0022:             grad_dict[id(original_tensor)] = grad_output.reshape(original_tensor.shape)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\__init__.py
// ----------------------------------------
0001: """
0002: Optimization algorithms for DLpy.
0003: 
0004: This module implements various optimization algorithms used in deep learning.
0005: """
0006: 
0007: from .optimizer import Optimizer
0008: from .sgd import SGD
0009: from .adam import Adam
0010: from .rmsprop import RMSprop
0011: from .adagrad import AdaGrad
0012: from .adadelta import AdaDelta
0013: from .adamax import AdaMax
0014: 
0015: __all__ = [
0016:     'Optimizer',
0017:     'SGD',
0018:     'Adam',
0019:     'RMSprop',
0020:     'AdaGrad',
0021:     'AdaDelta',
0022:     'AdaMax'
0023: ]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\adadelta.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Dict, Iterator, Optional
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class AdaDelta(Optimizer):
0007:     """
0008:     Implements AdaDelta algorithm.
0009:     
0010:     AdaDelta is a more robust extension of AdaGrad that adapts learning rates based on a 
0011:     moving window of gradient updates, instead of accumulating all past squared gradients.
0012:     The main advantage is that it doesn't need an initial learning rate.
0013:     
0014:     Args:
0015:         params: Iterable of parameters to optimize
0016:         rho (float): Coefficient for computing a running average of squared gradients (default: 0.9)
0017:         eps (float): Term added to denominator to improve numerical stability (default: 1e-6)
0018:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0019:     """
0020:     
0021:     def __init__(self, params, rho: float = 0.9, eps: float = 1e-6, weight_decay: float = 0):
0022:         if not 0.0 <= rho <= 1.0:
0023:             raise ValueError(f"Invalid rho value: {rho}")
0024:         if not 0.0 <= eps:
0025:             raise ValueError(f"Invalid epsilon value: {eps}")
0026:         if not 0.0 <= weight_decay:
0027:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0028:         
0029:         defaults = dict(rho=rho, eps=eps, weight_decay=weight_decay)
0030:         super().__init__(params, defaults)
0031: 
0032:         # Initialize state for each parameter
0033:         for group in self._params:
0034:             state = self.state[id(group)]
0035:             state['step'] = 0
0036:             state['square_avg'] = np.zeros_like(group.data, dtype=np.float64)  # E[g^2]
0037:             state['acc_delta'] = np.zeros_like(group.data, dtype=np.float64)   # E[Δx^2]
0038: 
0039:     def step(self) -> None:
0040:         """
0041:         Performs a single optimization step.
0042:         
0043:         For each parameter:
0044:         1. Compute running average of squared gradients
0045:         2. Compute parameter update using accumulated squared updates
0046:         3. Update running average of squared updates
0047:         4. Apply update to parameters
0048:         """
0049:         for p in self._params:
0050:             if p.grad is None:
0051:                 continue
0052:             
0053:             grad = p.grad
0054:             state = self.state[id(p)]
0055: 
0056:             # Apply weight decay if specified
0057:             if self.defaults['weight_decay'] != 0:
0058:                 grad = grad + self.defaults['weight_decay'] * p.data
0059: 
0060:             state['step'] += 1
0061: 
0062:             # Get parameters
0063:             rho = self.defaults['rho']
0064:             eps = self.defaults['eps']
0065: 
0066:             # Update running average of squared gradients
0067:             square_avg = state['square_avg']
0068:             acc_delta = state['acc_delta']
0069:             
0070:             # Update square_avg using numpy operations
0071:             square_avg = rho * square_avg + (1 - rho) * grad * grad
0072:             state['square_avg'] = square_avg
0073:             
0074:             # Compute update
0075:             std = np.sqrt(acc_delta + eps)
0076:             delta = np.sqrt(square_avg + eps)
0077:             update = grad * std / delta
0078: 
0079:             # Update running average of squared updates
0080:             acc_delta = rho * acc_delta + (1 - rho) * update * update
0081:             state['acc_delta'] = acc_delta
0082:             
0083:             # Apply update
0084:             p.data -= update
0085: 
0086:     def state_dict(self) -> Dict:
0087:         """Returns the state of the optimizer as a Dict."""
0088:         return {
0089:             'state': self.state,
0090:             'defaults': self.defaults
0091:         }
0092: 
0093:     def load_state_dict(self, state_dict: Dict) -> None:
0094:         """Loads the optimizer state."""
0095:         self.state = state_dict['state']
0096:         self.defaults = state_dict['defaults']

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\adagrad.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Dict, Iterator, Optional
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class AdaGrad(Optimizer):
0007:     """
0008:     Implements AdaGrad algorithm.
0009:     
0010:     AdaGrad is an optimizer with parameter-specific learning rates,
0011:     which are adapted based on historical gradient information.
0012:     
0013:     Args:
0014:         params: Iterable of parameters to optimize
0015:         lr (float): Learning rate (default: 1e-2)
0016:         lr_decay (float): Learning rate decay (default: 0)
0017:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0018:         eps (float): Term added to denominator to improve numerical stability (default: 1e-10)
0019:         initial_accumulator_value (float): Initial value for accumulator (default: 0)
0020:     """
0021:     
0022:     def __init__(self, params, lr: float = 1e-2, lr_decay: float = 0,
0023:                  weight_decay: float = 0, initial_accumulator_value: float = 0,
0024:                  eps: float = 1e-10):
0025:         if not 0.0 <= lr:
0026:             raise ValueError(f"Invalid learning rate: {lr}")
0027:         if not 0.0 <= lr_decay:
0028:             raise ValueError(f"Invalid lr_decay value: {lr_decay}")
0029:         if not 0.0 <= weight_decay:
0030:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0031:         if not 0.0 <= initial_accumulator_value:
0032:             raise ValueError(f"Invalid initial_accumulator_value value: {initial_accumulator_value}")
0033:         if not 0.0 <= eps:
0034:             raise ValueError(f"Invalid epsilon value: {eps}")
0035:             
0036:         defaults = dict(lr=lr, lr_decay=lr_decay, eps=eps, 
0037:                        weight_decay=weight_decay,
0038:                        initial_accumulator_value=initial_accumulator_value)
0039:         super().__init__(params, defaults)
0040: 
0041:         for group in self._params:
0042:             state = self.state[id(group)]
0043:             state['step'] = 0
0044:             state['sum'] = np.full_like(group.data, initial_accumulator_value, dtype=np.float64)
0045: 
0046:     def step(self) -> None:
0047:         """
0048:         Performs a single optimization step.
0049:         
0050:         For each parameter p, accumulates the square of the gradient and then
0051:         updates the parameter using the formula:
0052:         p = p - lr * g / (sqrt(accumulator) + eps)
0053:         where g is the gradient.
0054:         """
0055:         for p in self._params:
0056:             if p.grad is None:
0057:                 continue
0058:                 
0059:             grad = p.grad
0060:             state = self.state[id(p)]
0061: 
0062:             state['step'] += 1
0063: 
0064:             if self.defaults['weight_decay'] != 0:
0065:                 grad = grad + self.defaults['weight_decay'] * p.data
0066: 
0067:             # Update accumulator with squared gradient
0068:             state['sum'] += grad * grad
0069: 
0070:             # Compute the adaptive learning rate
0071:             std = np.sqrt(state['sum'])
0072:             
0073:             # Add epsilon for numerical stability before division
0074:             denom = std + self.defaults['eps']
0075: 
0076:             # Apply learning rate decay if specified
0077:             if self.defaults['lr_decay'] != 0:
0078:                 lr = self.defaults['lr'] / (1 + (state['step'] - 1) * self.defaults['lr_decay'])
0079:             else:
0080:                 lr = self.defaults['lr']
0081: 
0082:             # Update parameters
0083:             p.data -= lr * grad / denom
0084: 
0085:     def reset_state(self) -> None:
0086:         """
0087:         Resets the state of the optimizer.
0088:         
0089:         This can be useful when you want to restart training or when you want to 
0090:         reset the accumulated gradients without creating a new optimizer instance.
0091:         """
0092:         initial_accumulator_value = self.defaults['initial_accumulator_value']
0093:         
0094:         for group in self._params:
0095:             state = self.state[id(group)]
0096:             state['step'] = 0
0097:             state['sum'] = np.full_like(group.data, initial_accumulator_value, dtype=np.float64)
0098: 
0099:     def state_dict(self) -> Dict:
0100:         """
0101:         Returns the state of the optimizer as a Dict.
0102:         
0103:         The returned state dict contains two entries:
0104:             * state - a dict holding current optimization state. Its content
0105:                 differs between optimizer classes.
0106:             * param_groups - a dict containing all parameter groups
0107:         """
0108:         return {
0109:             'state': self.state,
0110:             'defaults': self.defaults
0111:         }
0112: 
0113:     def load_state_dict(self, state_dict: Dict) -> None:
0114:         """
0115:         Loads the optimizer state.
0116:         
0117:         Args:
0118:             state_dict (dict): Optimizer state. Should be an object returned
0119:                 from a call to state_dict().
0120:         """
0121:         self.state = state_dict['state']
0122:         self.defaults = state_dict['defaults']

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\adam.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Dict, Iterator, Optional
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class Adam(Optimizer):
0007:     """
0008:     Implements Adam algorithm.
0009:     
0010:     Args:
0011:         params: Iterable of parameters to optimize
0012:         lr (float): Learning rate (default: 0.001)
0013:         betas (tuple): Coefficients for computing running averages of gradient and its square
0014:             (default: (0.9, 0.999))
0015:         eps (float): Term added to denominator to improve numerical stability (default: 1e-8)
0016:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0017:         amsgrad (bool): Whether to use the AMSGrad variant (default: False)
0018:     """
0019:     
0020:     def __init__(self, params, lr: float = 0.001, betas: tuple = (0.9, 0.999),
0021:                  eps: float = 1e-8, weight_decay: float = 0, amsgrad: bool = False):
0022:         if not 0.0 <= lr:
0023:             raise ValueError(f"Invalid learning rate: {lr}")
0024:         if not 0.0 <= eps:
0025:             raise ValueError(f"Invalid epsilon value: {eps}")
0026:         if not 0.0 <= betas[0] < 1.0:
0027:             raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
0028:         if not 0.0 <= betas[1] < 1.0:
0029:             raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")
0030:         if not 0.0 <= weight_decay:
0031:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0032:             
0033:         defaults = dict(lr=lr, betas=betas, eps=eps,
0034:                        weight_decay=weight_decay, amsgrad=amsgrad)
0035:         super().__init__(params, defaults)
0036:         
0037:     def step(self) -> None:
0038:         """Performs a single optimization step."""
0039:         for p in self._params:
0040:             if p.grad is None:
0041:                 continue
0042:                 
0043:             grad = p.grad
0044:             
0045:             # Get optimizer state
0046:             state = self.state[id(p)]
0047:             
0048:             # State initialization
0049:             if len(state) == 0:
0050:                 state['step'] = 0
0051:                 # Exponential moving average of gradient values
0052:                 state['exp_avg'] = np.zeros_like(p.data)
0053:                 # Exponential moving average of squared gradient values
0054:                 state['exp_avg_sq'] = np.zeros_like(p.data)
0055:                 if self.defaults['amsgrad']:
0056:                     # Maintains max of all exp. moving avg. of sq. grad. values
0057:                     state['max_exp_avg_sq'] = np.zeros_like(p.data)
0058:                     
0059:             exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
0060:             if self.defaults['amsgrad']:
0061:                 max_exp_avg_sq = state['max_exp_avg_sq']
0062:             beta1, beta2 = self.defaults['betas']
0063:             
0064:             state['step'] += 1
0065:             bias_correction1 = 1 - beta1 ** state['step']
0066:             bias_correction2 = 1 - beta2 ** state['step']
0067:             
0068:             if self.defaults['weight_decay'] != 0:
0069:                 grad = grad + self.defaults['weight_decay'] * p.data
0070:                 
0071:             # Decay the first and second moment running average coefficient
0072:             exp_avg = beta1 * exp_avg + (1 - beta1) * grad
0073:             exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * grad * grad
0074:             
0075:             if self.defaults['amsgrad']:
0076:                 # Maintains the maximum of all 2nd moment running avg. till now
0077:                 max_exp_avg_sq = np.maximum(max_exp_avg_sq, exp_avg_sq)
0078:                 # Use the max. for normalizing running avg. of gradient
0079:                 denom = (np.sqrt(max_exp_avg_sq) / np.sqrt(bias_correction2)) + self.defaults['eps']
0080:             else:
0081:                 denom = (np.sqrt(exp_avg_sq) / np.sqrt(bias_correction2)) + self.defaults['eps']
0082:                 
0083:             step_size = self.defaults['lr'] / bias_correction1
0084:             
0085:             p.data -= step_size * exp_avg / denom
0086:             
0087:             # Save state
0088:             state['exp_avg'] = exp_avg
0089:             state['exp_avg_sq'] = exp_avg_sq
0090:             if self.defaults['amsgrad']:
0091:                 state['max_exp_avg_sq'] = max_exp_avg_sq

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\adamax.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Dict, Iterator, Optional
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class AdaMax(Optimizer):
0007:     """
0008:     Implements AdaMax algorithm, a variant of Adam based on the infinity norm.
0009:     
0010:     AdaMax is a variant of Adam that adopts the infinity norm in place of the L2 norm.
0011:     It tends to be more stable than Adam in some cases.
0012:     
0013:     Args:
0014:         params: Iterable of parameters to optimize
0015:         lr (float): Learning rate (default: 0.002)
0016:         betas (tuple): Coefficients for computing running averages (default: (0.9, 0.999))
0017:         eps (float): Term added to denominator to improve numerical stability (default: 1e-8)
0018:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0019:     """
0020:     
0021:     def __init__(self, params, lr: float = 0.002, betas: tuple = (0.9, 0.999), 
0022:                  eps: float = 1e-8, weight_decay: float = 0):
0023:         if not 0.0 <= lr:
0024:             raise ValueError(f"Invalid learning rate: {lr}")
0025:         if not 0.0 <= eps:
0026:             raise ValueError(f"Invalid epsilon value: {eps}")
0027:         if not 0.0 <= betas[0] < 1.0:
0028:             raise ValueError(f"Invalid beta1 parameter: {betas[0]}")
0029:         if not 0.0 <= betas[1] < 1.0:
0030:             raise ValueError(f"Invalid beta2 parameter: {betas[1]}")
0031:         if not 0.0 <= weight_decay:
0032:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0033:             
0034:         defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
0035:         super().__init__(params, defaults)
0036:         
0037:         # Initialize state for each parameter
0038:         for group in self._params:
0039:             state = self.state[id(group)]
0040:             state['step'] = 0
0041:             state['exp_avg'] = np.zeros_like(group.data, dtype=np.float64)  # m_t
0042:             state['exp_inf'] = np.zeros_like(group.data, dtype=np.float64)  # u_t
0043: 
0044:     def step(self) -> None:
0045:         """
0046:         Performs a single optimization step.
0047:         
0048:         For each parameter:
0049:         1. Update biased first moment estimate
0050:         2. Update the exponentially weighted infinity norm
0051:         3. Compute bias-corrected learning rate
0052:         4. Update parameters
0053:         """
0054:         for p in self._params:
0055:             if p.grad is None:
0056:                 continue
0057:                 
0058:             grad = p.grad
0059:             state = self.state[id(p)]
0060: 
0061:             # Apply weight decay if specified
0062:             if self.defaults['weight_decay'] != 0:
0063:                 grad = grad + self.defaults['weight_decay'] * p.data
0064: 
0065:             # Get parameters
0066:             beta1, beta2 = self.defaults['betas']
0067:             lr = self.defaults['lr']
0068:             eps = self.defaults['eps']
0069:             
0070:             state['step'] += 1
0071:             bias_correction = 1 - beta1 ** state['step']
0072: 
0073:             # Get momentum buffer
0074:             exp_avg = state['exp_avg']
0075:             exp_inf = state['exp_inf']
0076: 
0077:             # Update biased first moment estimate using numpy operations
0078:             exp_avg = beta1 * exp_avg + (1 - beta1) * grad
0079:             state['exp_avg'] = exp_avg
0080:             
0081:             # Update the exponentially weighted infinity norm
0082:             exp_inf = np.maximum(beta2 * exp_inf, np.abs(grad))
0083:             state['exp_inf'] = exp_inf
0084: 
0085:             # Compute bias-corrected learning rate
0086:             step_size = lr / bias_correction
0087: 
0088:             # Update parameters
0089:             p.data -= step_size * exp_avg / (exp_inf + eps)
0090: 
0091:     def state_dict(self) -> Dict:
0092:         """Returns the state of the optimizer as a Dict."""
0093:         return {
0094:             'state': self.state,
0095:             'defaults': self.defaults
0096:         }
0097: 
0098:     def load_state_dict(self, state_dict: Dict) -> None:
0099:         """Loads the optimizer state."""
0100:         self.state = state_dict['state']
0101:         self.defaults = state_dict['defaults']

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\optimizer.py
// ----------------------------------------
0001: from typing import Dict, Iterator, Optional
0002: from ..core import Tensor
0003: 
0004: class Optimizer:
0005:     """
0006:     Base class for all optimizers.
0007:     
0008:     Args:
0009:         params: An iterable of parameters to optimize or a dict of parameter groups
0010:         defaults: Dictionary of default hyperparameter values
0011:     """
0012:     
0013:     def __init__(self, params, defaults: Dict):
0014:         self.defaults = defaults
0015:         self._params = list(params)  # Convert iterator to list
0016:         self.state: Dict = {}  # State dict for optimizer states
0017:         
0018:         # Initialize state for each parameter
0019:         for p in self._params:
0020:             self.state[id(p)] = {}
0021:             
0022:     def zero_grad(self) -> None:
0023:         """Clears the gradients of all optimized parameters."""
0024:         for p in self._params:
0025:             if p.grad is not None:
0026:                 p.grad.fill(0)
0027:                 
0028:     def step(self) -> None:
0029:         """Performs a single optimization step.
0030:         
0031:         This method should be overridden by all optimizers.
0032:         """
0033:         raise NotImplementedError
0034:         
0035:     def add_param_group(self, param_group: Dict) -> None:
0036:         """Add a param group to the optimizer's param groups.
0037:         
0038:         Args:
0039:             param_group (dict): Specifies parameters and parameter-specific options
0040:         """
0041:         params = param_group['params']
0042:         if isinstance(params, Tensor):
0043:             param_group['params'] = [params]
0044:         elif isinstance(params, set):
0045:             param_group['params'] = list(params)
0046:             
0047:         for param in param_group['params']:
0048:             if id(param) not in self.state:
0049:                 self.state[id(param)] = {}
0050:             self._params.append(param)
0051:             
0052:     def load_state_dict(self, state_dict: Dict) -> None:
0053:         """Loads the optimizer state.
0054:         
0055:         Args:
0056:             state_dict (dict): Optimizer state dict
0057:         """
0058:         self.state = state_dict['state']
0059:         
0060:     def state_dict(self) -> Dict:
0061:         """Returns the state of the optimizer as a dict.
0062:         
0063:         Returns:
0064:             dict: The state of the optimizer
0065:         """
0066:         return {
0067:             'state': self.state,
0068:         }

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\rmsprop.py
// ----------------------------------------
0001: import numpy as np 
0002: from typing import Dict, Iterator, Optional
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class RMSprop(Optimizer):
0007:     """
0008:     Implements RMSprop algorithm.
0009:     
0010:     Args:
0011:         params: Iterable of parameters to optimize
0012:         lr (float): Learning rate (default: 0.01)
0013:         alpha (float): Smoothing constant (default: 0.99)
0014:         eps (float): Term added to denominator to improve numerical stability (default: 1e-8)
0015:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0016:         momentum (float): Momentum factor (default: 0)
0017:         centered (bool): If True, compute centered RMSprop, gradients normalized by their variance
0018:     """
0019:     
0020:     def __init__(self, params, lr: float = 0.01, alpha: float = 0.99,
0021:                  eps: float = 1e-8, weight_decay: float = 0,
0022:                  momentum: float = 0, centered: bool = False):
0023:         if not 0.0 <= lr:
0024:             raise ValueError(f"Invalid learning rate: {lr}")
0025:         if not 0.0 <= eps:
0026:             raise ValueError(f"Invalid epsilon value: {eps}")
0027:         if not 0.0 <= momentum:
0028:             raise ValueError(f"Invalid momentum value: {momentum}")
0029:         if not 0.0 <= alpha:
0030:             raise ValueError(f"Invalid alpha value: {alpha}")
0031:         if not 0.0 <= weight_decay:
0032:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0033:             
0034:         defaults = dict(lr=lr, alpha=alpha, eps=eps, weight_decay=weight_decay,
0035:                        momentum=momentum, centered=centered)
0036:         super().__init__(params, defaults)
0037:         
0038:     def step(self) -> None:
0039:         """Performs a single optimization step."""
0040:         for p in self._params:
0041:             if p.grad is None:
0042:                 continue
0043:                 
0044:             grad = p.grad
0045:             state = self.state[id(p)]
0046:             
0047:             # State initialization
0048:             if len(state) == 0:
0049:                 state['step'] = 0
0050:                 state['square_avg'] = np.zeros_like(p.data)
0051:                 if self.defaults['momentum'] > 0:
0052:                     state['momentum_buffer'] = np.zeros_like(p.data)
0053:                 if self.defaults['centered']:
0054:                     state['grad_avg'] = np.zeros_like(p.data)
0055:                     
0056:             square_avg = state['square_avg']
0057:             alpha = self.defaults['alpha']
0058:             
0059:             state['step'] += 1
0060:             
0061:             if self.defaults['weight_decay'] != 0:
0062:                 grad = grad + self.defaults['weight_decay'] * p.data
0063:                 
0064:             # Update squared average
0065:             square_avg = alpha * square_avg + (1 - alpha) * grad * grad
0066:             
0067:             if self.defaults['centered']:
0068:                 grad_avg = state['grad_avg']
0069:                 grad_avg = alpha * grad_avg + (1 - alpha) * grad
0070:                 avg = square_avg - grad_avg * grad_avg
0071:                 state['grad_avg'] = grad_avg
0072:             else:
0073:                 avg = square_avg
0074:                 
0075:             # Apply momentum if enabled
0076:             if self.defaults['momentum'] > 0:
0077:                 buf = state.get('momentum_buffer', np.zeros_like(grad))
0078:                 buf = self.defaults['momentum'] * buf + grad / (np.sqrt(avg) + self.defaults['eps'])
0079:                 state['momentum_buffer'] = buf
0080:                 p.data -= self.defaults['lr'] * buf
0081:             else:
0082:                 p.data -= self.defaults['lr'] * grad / (np.sqrt(avg) + self.defaults['eps'])
0083:                 
0084:             # Save state
0085:             state['square_avg'] = square_avg

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\sgd.py
// ----------------------------------------
0001: from typing import Dict, Iterator, Optional
0002: import numpy as np
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class SGD(Optimizer):
0007:     """
0008:     Implements stochastic gradient descent with momentum.
0009:     
0010:     Args:
0011:         params: Iterable of parameters to optimize
0012:         lr (float): Learning rate (default: 0.1)
0013:         momentum (float): Momentum factor (default: 0)
0014:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0015:         dampening (float): Dampening for momentum (default: 0)
0016:         nesterov (bool): Enables Nesterov momentum (default: False)
0017:     """
0018:     
0019:     def __init__(self, params, lr: float = 0.1, momentum: float = 0.0,
0020:                  weight_decay: float = 0.0, dampening: float = 0.0,
0021:                  nesterov: bool = False):
0022:         if lr < 0.0:
0023:             raise ValueError(f"Invalid learning rate: {lr}")
0024:         if momentum < 0.0:
0025:             raise ValueError(f"Invalid momentum value: {momentum}")
0026:         if weight_decay < 0.0:
0027:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0028:             
0029:         defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay,
0030:                        dampening=dampening, nesterov=nesterov)
0031:         super().__init__(params, defaults)
0032:         
0033:     def step(self) -> None:
0034:         """Performs a single optimization step."""
0035:         
0036:         for p in self._params:
0037:             if p.grad is None:
0038:                 continue
0039:                 
0040:             grad = p.grad
0041:             
0042:             # Apply weight decay
0043:             if self.defaults['weight_decay'] != 0:
0044:                 grad = grad + self.defaults['weight_decay'] * p.data
0045:                 
0046:             # Get or initialize momentum buffer
0047:             if 'momentum_buffer' not in self.state[id(p)]:
0048:                 buf = self.state[id(p)]['momentum_buffer'] = np.zeros_like(p.data)
0049:             else:
0050:                 buf = self.state[id(p)]['momentum_buffer']
0051:                 
0052:             # Update momentum buffer
0053:             if self.defaults['momentum'] != 0:
0054:                 buf *= self.defaults['momentum']
0055:                 if self.defaults['dampening'] != 0:
0056:                     grad *= 1 - self.defaults['dampening']
0057:                 buf += grad
0058:             else:
0059:                 buf = grad
0060:                 
0061:             # Nesterov momentum
0062:             if self.defaults['nesterov']:
0063:                 grad += self.defaults['momentum'] * buf
0064:             else:
0065:                 grad = buf
0066:                 
0067:             # Update parameters
0068:             p.data -= self.defaults['lr'] * grad
0069:             
0070:             # Store updated momentum buffer
0071:             self.state[id(p)]['momentum_buffer'] = buf

// File: C:\Users\aluja\Desktop\DLpy\examples\basic_autograd.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\examples\neural_network.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\setup.py
// ----------------------------------------
0001: from setuptools import setup, find_packages
0002: 
0003: setup(
0004:     name="DLpy",  # Changed from DLpy to DLpy
0005:     version="0.1.0",
0006:     author="Antonio Lujano Luna",
0007:     packages=find_packages(include=["DLpy"]), 
0008:     install_requires=[
0009:         "numpy>=1.20.0",
0010:     ],
0011:     extras_require={
0012:         "dev": [
0013:             "pytest>=7.0.0",
0014:             "pytest-cov>=4.0.0",
0015:             "pytest-xdist>=3.0.0",
0016:             "black>=22.0.0",
0017:             "isort>=5.0.0",
0018:             "mypy>=1.0.0",
0019:         ],
0020:     },
0021:     python_requires=">=3.8",
0022: )

// ----------------------------------------
// Total Python files found: 45
