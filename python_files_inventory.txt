// Python Files Concatenated on 01/19/2025 17:42:01
// ----------------------------------------



// File: C:\Users\aluja\Desktop\DLpy\DLpy\__init__.py
// ----------------------------------------
0001: """
0002: DLpy: A Deep Learning Library with DAG-based Autograd
0003: 
0004: This library provides a PyTorch-like interface for building and training neural networks,
0005: with a focus on clear implementation and educational value.
0006: """
0007: 
0008: from .core import Tensor, Function, Context
0009: from .ops import Add, Multiply, Reshape
0010: 
0011: __version__ = "0.1.0"
0012: 
0013: __all__ = [
0014:     'Tensor',
0015:     'Function',
0016:     'Context',
0017:     'Add',
0018:     'Multiply',
0019:     'Reshape',
0020: ]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\__init__.py
// ----------------------------------------
0001: """
0002: Core functionality for DLpy.
0003: 
0004: This module contains the fundamental building blocks of the deep learning library.
0005: """
0006: 
0007: from .tensor import Tensor
0008: from .function import Function
0009: from .context import Context
0010: from .autograd import AutogradEngine, get_autograd_engine
0011: 
0012: __all__ = ['Tensor', 'Function', 'Context', 'AutogradEngine', 'get_autograd_engine']

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\autograd.py
// ----------------------------------------
0001: from typing import Dict, Set, List, Optional, Tuple, Union
0002: import numpy as np
0003: from collections import defaultdict
0004: import warnings
0005: 
0006: class Edge:
0007:     """
0008:     Represents a directed edge in the computational graph.
0009:     
0010:     Each edge connects a source node (input tensor) to a destination node
0011:     (output tensor) and stores gradient information for that connection.
0012:     """
0013:     
0014:     def __init__(self, src: 'Node', dst: 'Node'):
0015:         self.src = src
0016:         self.dst = dst
0017:         self.grad: Optional[np.ndarray] = None
0018:         
0019: class Node:
0020:     """
0021:     Represents a node in the computational graph.
0022:     
0023:     Each node corresponds to an operation in the computation and maintains
0024:     connections to its inputs and outputs through edges.
0025:     """
0026:     
0027:     def __init__(self, tensor: 'Tensor'):
0028:         self.tensor = tensor
0029:         self.in_edges: List[Edge] = []
0030:         self.out_edges: List[Edge] = []
0031:         self._backward_fn = tensor._backward_fn
0032: 
0033: class AutogradEngine:
0034:     """
0035:     Engine for managing automatic differentiation computations.
0036:     
0037:     This class handles the creation and execution of the computational graph,
0038:     manages gradient computation and accumulation, and provides utilities for
0039:     graph manipulation and visualization.
0040:     """
0041:     
0042:     def __init__(self):
0043:         self._nodes: Dict[int, Node] = {}
0044:         self._edges: Set[Edge] = set()
0045:         self._currently_computing_gradients = False
0046:         
0047:     def register_tensor(self, tensor: 'Tensor') -> None:
0048:         """
0049:         Registers a tensor with the autograd engine.
0050:         
0051:         Args:
0052:             tensor: Tensor to register
0053:         """
0054:         if id(tensor) not in self._nodes:
0055:             self._nodes[id(tensor)] = Node(tensor)
0056:             
0057:     def add_edge(self, src: 'Tensor', dst: 'Tensor') -> None:
0058:         """
0059:         Adds a directed edge between two tensors in the computational graph.
0060:         
0061:         Args:
0062:             src: Source tensor
0063:             dst: Destination tensor
0064:         """
0065:         src_node = self._nodes[id(src)]
0066:         dst_node = self._nodes[id(dst)]
0067:         
0068:         edge = Edge(src_node, dst_node)
0069:         src_node.out_edges.append(edge)
0070:         dst_node.in_edges.append(edge)
0071:         self._edges.add(edge)
0072:         
0073:     def backward(self, tensor: 'Tensor', gradient: Optional[np.ndarray] = None) -> None:
0074:         """Executes backward pass starting from the given tensor."""
0075:         if self._currently_computing_gradients:
0076:             raise RuntimeError("Nested gradient computation detected")
0077:             
0078:         self._currently_computing_gradients = True
0079:         try:
0080:             # Initialize grad_dict as a regular dictionary
0081:             grad_dict: Dict[int, np.ndarray] = {}
0082:             
0083:             # If no gradient is provided, assume it's 1 (for scalar outputs)
0084:             if gradient is None:
0085:                 if tensor.data.shape == ():
0086:                     grad_dict[id(tensor)] = np.array(1.0)
0087:                 else:
0088:                     grad_dict[id(tensor)] = np.ones_like(tensor.data)
0089:             else:
0090:                 grad_dict[id(tensor)] = gradient
0091:             
0092:             # Perform topological sort
0093:             sorted_nodes = self._topological_sort(tensor)
0094:             
0095:             # Traverse nodes in reverse topological order
0096:             for node in reversed(sorted_nodes):
0097:                 node_id = id(node.tensor)
0098:                 if node_id not in grad_dict or not node.tensor.requires_grad:
0099:                     continue  # No gradient to propagate
0100:                 
0101:                 current_grad = grad_dict[node_id]
0102:                 
0103:                 if node.tensor._backward_fn is not None:
0104:                     node.tensor._backward_fn(current_grad, grad_dict)
0105:                 
0106:                 # Accumulate gradients for leaf nodes
0107:                 if len(node.in_edges) == 0 and node.tensor.requires_grad:
0108:                     if node.tensor.grad is None:
0109:                         node.tensor.grad = current_grad
0110:                     else:
0111:                         try:
0112:                             node.tensor.grad += current_grad
0113:                         except ValueError:
0114:                             # If shapes don't match, reshape current_grad
0115:                             node.tensor.grad += current_grad.reshape(node.tensor.grad.shape)
0116:         finally:
0117:             self._currently_computing_gradients = False
0118: 
0119:     def _topological_sort(self, start_tensor: 'Tensor') -> List[Node]:
0120:         """
0121:         Performs topological sort on the computation graph.
0122:         
0123:         Args:
0124:             start_tensor: Tensor to start the sort from
0125:             
0126:         Returns:
0127:             List of nodes in topological order
0128:             
0129:         Raises:
0130:             RuntimeError: If graph contains cycles
0131:         """
0132:         result: List[Node] = []
0133:         visited: Set[Node] = set()
0134:         temp_visited: Set[Node] = set()
0135:         
0136:         def visit(node: Node) -> None:
0137:             if node in temp_visited:
0138:                 raise RuntimeError("Cycle detected in computation graph")
0139:                 
0140:             if node not in visited:
0141:                 temp_visited.add(node)
0142:                 for edge in node.in_edges:
0143:                     visit(edge.src)
0144:                 temp_visited.remove(node)
0145:                 visited.add(node)
0146:                 result.append(node)
0147:                 
0148:         visit(self._nodes[id(start_tensor)])
0149:         return result
0150:             
0151:     def clear(self) -> None:
0152:         """Clears the computational graph."""
0153:         self._nodes.clear()
0154:         self._edges.clear()
0155:     
0156:     def validate_graph(self) -> List[str]:
0157:         """
0158:         Validates the computational graph structure.
0159:         """
0160:         warnings: List[str] = []
0161:         
0162:         # If no nodes in graph
0163:         if not self._nodes:
0164:             return warnings
0165: 
0166:         # Step 1: Find all nodes that are part of computations
0167:         active_nodes = set()
0168:         output_nodes = []
0169:         for node in self._nodes.values():
0170:             if not node.out_edges:  # Output node
0171:                 output_nodes.append(node)
0172:             if node.in_edges or node.out_edges:  # Node is part of a computation
0173:                 active_nodes.add(node)
0174: 
0175:         # Step 2: Find all connected nodes starting from outputs
0176:         connected_nodes = set()
0177:         for output_node in output_nodes:
0178:             stack = [output_node]
0179:             while stack:
0180:                 curr = stack.pop()
0181:                 connected_nodes.add(curr)
0182:                 for edge in curr.in_edges:
0183:                     if edge.src not in connected_nodes:
0184:                         stack.append(edge.src)
0185:                         
0186:         # Step 3: Find nodes not connected to outputs
0187:         all_nodes = set(self._nodes.values())
0188:         unconnected_nodes = all_nodes - connected_nodes
0189: 
0190:         # Step 4: Find completely isolated nodes
0191:         isolated_nodes = all_nodes - active_nodes
0192: 
0193:         # Add appropriate warnings
0194:         if unconnected_nodes:
0195:             warnings.append(f"Found {len(unconnected_nodes)} nodes not connected to any output")
0196:             
0197:         if isolated_nodes:
0198:             warnings.append(f"Found {len(isolated_nodes)} isolated nodes")
0199:             
0200:         # Check gradient shapes
0201:         for edge in self._edges:
0202:             if edge.grad is not None:
0203:                 src_shape = edge.src.tensor.shape
0204:                 grad_shape = edge.grad.shape
0205:                 if src_shape != grad_shape:
0206:                     warnings.append(
0207:                         f"Gradient shape mismatch: grad shape {grad_shape} vs tensor shape {src_shape}"
0208:                     )
0209:                     
0210:         return warnings
0211: 
0212: 
0213: # Global autograd engine instance
0214: _autograd_engine = AutogradEngine()
0215: 
0216: def get_autograd_engine() -> AutogradEngine:
0217:     """Returns the global autograd engine instance."""
0218:     return _autograd_engine

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\context.py
// ----------------------------------------
0001: from typing import Any, Dict, List, Tuple
0002: from dataclasses import dataclass, field
0003: 
0004: @dataclass
0005: class Context:
0006:     """
0007:     Context class for storing information needed during the backward pass.
0008:     
0009:     The Context class serves as a storage mechanism for tensors and metadata that are 
0010:     needed during backpropagation. It's passed to both forward and backward functions
0011:     to maintain state between the two passes.
0012:     
0013:     Attributes:
0014:         _saved_tensors: List of tensors saved during forward pass for use in backward pass
0015:         _non_tensor_args: Dictionary of additional arguments needed for backward pass
0016:         _intermediate_values: Dictionary storing intermediate computations
0017:     """
0018:     
0019:     _saved_tensors: List[Any] = field(default_factory=list)
0020:     _non_tensor_args: Dict[str, Any] = field(default_factory=dict)
0021:     _intermediate_values: Dict[str, Any] = field(default_factory=dict)
0022: 
0023:     def save_for_backward(self, *args: Any) -> None:
0024:         """
0025:         Saves tensors that will be needed for the backward pass.
0026:         
0027:         Args:
0028:             *args: Variable number of tensors to save
0029:         """
0030:         self._saved_tensors = list(args)
0031: 
0032:     def save_arguments(self, **kwargs: Any) -> None:
0033:         """
0034:         Saves additional arguments that will be needed for the backward pass.
0035:         
0036:         Args:
0037:             **kwargs: Keyword arguments to save
0038:         """
0039:         self._non_tensor_args.update(kwargs)
0040:         
0041:     def store_intermediate(self, name: str, value: Any) -> None:
0042:         """
0043:         Stores intermediate values computed during forward pass that may be
0044:         useful during backward pass or for debugging.
0045:         
0046:         Args:
0047:             name: Identifier for the intermediate value
0048:             value: The value to store
0049:         """
0050:         self._intermediate_values[name] = value
0051: 
0052:     @property
0053:     def saved_tensors(self) -> Tuple[Any, ...]:
0054:         """Returns the saved tensors as a tuple."""
0055:         return tuple(self._saved_tensors)
0056: 
0057:     @property
0058:     def saved_arguments(self) -> Dict[str, Any]:
0059:         """Returns the saved non-tensor arguments."""
0060:         return self._non_tensor_args.copy()
0061:         
0062:     def get_intermediate(self, name: str) -> Any:
0063:         """
0064:         Retrieves a stored intermediate value.
0065:         
0066:         Args:
0067:             name: Identifier for the intermediate value
0068:             
0069:         Returns:
0070:             The stored value
0071:             
0072:         Raises:
0073:             KeyError: If no value exists for the given name
0074:         """
0075:         return self._intermediate_values[name]
0076: 
0077:     def clear(self) -> None:
0078:         """Clears all saved data from the context."""
0079:         self._saved_tensors.clear()
0080:         self._non_tensor_args.clear()
0081:         self._intermediate_values.clear()

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\function.py
// ----------------------------------------
0001: from abc import ABC, abstractmethod
0002: from typing import Any, Tuple, Optional, Dict
0003: import numpy as np
0004: 
0005: from .context import Context
0006: from .tensor import Tensor  # This will be implemented next
0007: 
0008: class Function(ABC):
0009:     """
0010:     Base class for all autograd operations.
0011:     
0012:     This class defines the interface for creating differentiable operations.
0013:     Each operation should implement both a forward pass (computing the result)
0014:     and a backward pass (computing gradients).
0015:     
0016:     The Function class follows a similar design pattern to PyTorch's autograd.Function,
0017:     but with some simplifications and additional features for clarity and debugging.
0018:     """
0019:     
0020:     requires_grad: bool = True
0021:     
0022:     @staticmethod
0023:     @abstractmethod
0024:     def forward(ctx: Context, *args: Any, **kwargs: Any) -> Tensor:
0025:         """
0026:         Performs the forward computation.
0027:         
0028:         Args:
0029:             ctx: Context object for saving information needed in backward pass
0030:             *args: Input tensors and other arguments
0031:             **kwargs: Additional keyword arguments for the operation
0032:             
0033:         Returns:
0034:             Result of the computation as a Tensor
0035:         """
0036:         raise NotImplementedError
0037:         
0038:     @staticmethod
0039:     @abstractmethod
0040:     def backward(ctx: Context, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0041:         """
0042:         Computes gradients of the operation with respect to its inputs.
0043:         
0044:         Args:
0045:             ctx: Context object containing saved tensors from forward pass
0046:             grad_output: Gradient of the loss with respect to the output
0047:             grad_dict: Dictionary mapping tensor IDs to their gradients
0048:         """
0049:         raise NotImplementedError
0050:         
0051:     @classmethod
0052:     def apply(cls, *args: Any, **kwargs: Any) -> Tensor:
0053:         """
0054:         Applies the function to the given inputs.
0055:         
0056:         This method:
0057:         1. Creates a Context object for storing intermediate values
0058:         2. Runs the forward pass
0059:         3. Sets up the computational graph for gradient computation
0060:         4. Returns the result
0061:         """
0062:         ctx = Context()
0063:         result = cls.forward(ctx, *args, **kwargs)
0064:         
0065:         # Check if we need to compute gradients
0066:         needs_grad = cls.requires_grad and any(
0067:             isinstance(arg, Tensor) and arg.requires_grad 
0068:             for arg in args
0069:         )
0070:         
0071:         if needs_grad:
0072:             def backward_fn(grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0073:                 cls.backward(ctx, grad_output, grad_dict)
0074:             
0075:             result._backward_fn = backward_fn
0076:             result.requires_grad_(True)
0077:             
0078:             # Get autograd engine and register edges
0079:             from .autograd import get_autograd_engine
0080:             engine = get_autograd_engine()
0081:             for arg in args:
0082:                 if isinstance(arg, Tensor):
0083:                     engine.add_edge(arg, result)
0084:         
0085:         return result  # Return result in all cases
0086:         
0087:         
0088:     @staticmethod
0089:     def verify_backward(
0090:         forward_fn: Any,
0091:         backward_fn: Any,
0092:         inputs: Tuple[np.ndarray, ...],
0093:         epsilon: float = 1e-6
0094:     ) -> bool:
0095:         """
0096:         Verifies backward pass implementation using numerical gradients.
0097:         
0098:         This helper method compares analytically computed gradients with
0099:         numerically computed gradients to check for correctness.
0100:         
0101:         Args:
0102:             forward_fn: The forward pass function
0103:             backward_fn: The backward pass function
0104:             inputs: Tuple of input arrays
0105:             epsilon: Small value for numerical gradient computation
0106:             
0107:         Returns:
0108:             True if gradients match within tolerance, False otherwise
0109:         """
0110:         def compute_numerical_gradient(idx: int, inp: np.ndarray) -> np.ndarray:
0111:             grad = np.zeros_like(inp)
0112:             it = np.nditer(inp, flags=['multi_index'])
0113:             
0114:             while not it.finished:
0115:                 ix = it.multi_index
0116:                 old_value = inp[ix]
0117:                 
0118:                 # Compute f(x + epsilon)
0119:                 inp[ix] = old_value + epsilon
0120:                 pos_inputs = list(inputs)
0121:                 pos_inputs[idx] = inp.copy()
0122:                 pos_output = forward_fn(*pos_inputs)
0123:                 
0124:                 # Compute f(x - epsilon)
0125:                 inp[ix] = old_value - epsilon
0126:                 neg_inputs = list(inputs)
0127:                 neg_inputs[idx] = inp.copy()
0128:                 neg_output = forward_fn(*neg_inputs)
0129:                 
0130:                 # Restore original value
0131:                 inp[ix] = old_value
0132:                 
0133:                 # Compute numerical gradient
0134:                 grad[ix] = np.sum(pos_output - neg_output) / (2 * epsilon)
0135:                 it.iternext()
0136:                 
0137:             return grad
0138:             
0139:         # Compute analytical gradients
0140:         ctx = Context()
0141:         output = forward_fn(*inputs)
0142:         grad_output = np.ones_like(output)
0143:         analytical_grads = backward_fn(ctx, grad_output)
0144:         
0145:         # Compute numerical gradients
0146:         numerical_grads = tuple(
0147:             compute_numerical_gradient(i, inp.copy()) 
0148:             for i, inp in enumerate(inputs)
0149:         )
0150:         
0151:         # Compare gradients
0152:         for analytical, numerical in zip(analytical_grads, numerical_grads):
0153:             if analytical is not None:
0154:                 rel_error = np.max(
0155:                     np.abs(analytical - numerical) /
0156:                     (np.maximum(np.abs(analytical), np.abs(numerical)) + epsilon)
0157:                 )
0158:                 if rel_error > 1e-5:
0159:                     return False
0160:                     
0161:         return True

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\tensor.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Optional, Union, List, Tuple, Callable, Dict, Set
0003: from numbers import Number
0004: 
0005: class Tensor:
0006:     """
0007:     A multidimensional array with autograd capabilities.
0008:     
0009:     The Tensor class wraps numpy arrays and adds automatic differentiation
0010:     capabilities. It tracks the computational graph and enables gradient
0011:     computation through backpropagation.
0012:     
0013:     Attributes:
0014:         data: The underlying numpy array holding the tensor's values
0015:         grad: Gradient of the loss with respect to this tensor
0016:         requires_grad: Whether to compute gradients for this tensor
0017:         _prev: Set of immediate predecessor nodes in computational graph
0018:         _backward_fn: Function to compute gradients during backpropagation
0019:         _is_leaf: Whether this tensor is a leaf node (created by user)
0020:     """
0021:     
0022:     def __init__(
0023:         self,
0024:         data: Union[np.ndarray, List, Number],
0025:         requires_grad: bool = False,
0026:         dtype: Optional[np.dtype] = None
0027:     ):
0028:         # Convert scalars to scalar arrays with shape ()
0029:         if isinstance(data, (int, float)):
0030:             self.data = np.array(data, dtype=dtype or np.float64)  # Will have shape ()
0031:         elif isinstance(data, Tensor):
0032:             self.data = data.data
0033:         elif isinstance(data, list):
0034:             self.data = np.array(data, dtype=dtype)
0035:         else:
0036:             self.data = data.astype(dtype) if dtype else data
0037:             
0038:         self.grad: Optional[np.ndarray] = None
0039:         self._requires_grad = requires_grad
0040:         self._backward_fn: Optional[Callable] = None
0041:         self._prev: Set['Tensor'] = set()
0042:         self._is_leaf = True
0043: 
0044:         # Register with autograd engine
0045:         from .autograd import get_autograd_engine
0046:         engine = get_autograd_engine()
0047:         engine.register_tensor(self)
0048:         
0049:         if requires_grad:
0050:             self.zero_grad()
0051: 
0052:     @property
0053:     def shape(self) -> Tuple[int, ...]:
0054:         """Returns the shape of the tensor."""
0055:         return self.data.shape
0056:         
0057:     @property
0058:     def dtype(self) -> np.dtype:
0059:         """Returns the data type of the tensor."""
0060:         return self.data.dtype
0061:         
0062:     @property
0063:     def requires_grad(self) -> bool:
0064:         """Returns whether the tensor requires gradient computation."""
0065:         return self._requires_grad
0066:         
0067:     def requires_grad_(self, requires_grad: bool = True) -> 'Tensor':
0068:         """Sets gradient computation requirement and returns self."""
0069:         self._requires_grad = requires_grad
0070:         if requires_grad and self.grad is None:
0071:             self.zero_grad()
0072:         return self
0073: 
0074:     def zero_grad(self) -> None:
0075:         """Zeros out the gradient."""
0076:         if self.data.shape == ():  # For scalar tensors
0077:             self.grad = np.zeros(1, dtype=np.float64)  # Force 1D array
0078:         else:
0079:             self.grad = np.zeros_like(self.data, dtype=np.float64)
0080:         
0081:     def backward(self, gradient: Optional[np.ndarray] = None) -> None:
0082:         """
0083:         Computes gradients of the loss with respect to this tensor.
0084:         """
0085:         if not self.requires_grad:
0086:             return
0087: 
0088:         # Handle default gradient for scalar tensors
0089:         if gradient is None:
0090:             if np.prod(self.shape) == 1:
0091:                 if self.shape == ():  # scalar tensor
0092:                     gradient = np.array(1.0)
0093:                 else:
0094:                     gradient = np.ones(self.shape)
0095:             else:
0096:                 raise RuntimeError("grad can be implicitly created only for scalar outputs")
0097: 
0098:         # Ensure gradient is numpy array
0099:         if isinstance(gradient, (int, float)):
0100:             gradient = np.array(gradient)
0101:             
0102:         # Ensure matching shapes for scalar case
0103:         if self.shape == () and gradient.shape != ():
0104:             gradient = gradient.sum()
0105:         elif self.shape != () and gradient.shape == ():
0106:             gradient = np.full(self.shape, gradient)
0107: 
0108:         # Get autograd engine and execute backward pass
0109:         from .autograd import get_autograd_engine
0110:         engine = get_autograd_engine()
0111:         engine.backward(self, gradient)
0112: 
0113: 
0114:     def __repr__(self) -> str:
0115:         return f"Tensor({self.data}, requires_grad={self.requires_grad})"
0116: 
0117:     # Basic arithmetic operations that will be connected to Function implementations
0118:     def __add__(self, other: Union['Tensor', Number]) -> 'Tensor':
0119:         from ..ops.basic import Add
0120:         return Add.apply(self, other)
0121:         
0122:     def __mul__(self, other: Union['Tensor', Number]) -> 'Tensor':
0123:         from ..ops.basic import Multiply
0124:         return Multiply.apply(self, other)
0125:         
0126:     def __matmul__(self, other: 'Tensor') -> 'Tensor':
0127:         from ..ops.basic import MatMul
0128:         return MatMul.apply(self, other)
0129:         
0130:     def __neg__(self) -> 'Tensor':
0131:         return self * (-1)
0132:         
0133:     def __sub__(self, other: Union['Tensor', Number]) -> 'Tensor':
0134:         return self + (-other)
0135: 
0136:     def reshape(self, *shape: int) -> 'Tensor':
0137:         from ..ops.reshape import Reshape
0138:         return Reshape.apply(self, shape)
0139: 
0140:     # Helper methods for numpy compatibility
0141:     def numpy(self) -> np.ndarray:
0142:         """Returns the underlying numpy array."""
0143:         return self.data
0144:         
0145:     @classmethod
0146:     def from_numpy(cls, array: np.ndarray, requires_grad: bool = False) -> 'Tensor':
0147:         """Creates a Tensor from a numpy array."""
0148:         return cls(array.copy(), requires_grad=requires_grad)
0149: 
0150:     # Shape manipulation methods
0151:     def reshape(self, *shape: int) -> 'Tensor':
0152:         """Returns a tensor with the same data and new shape."""
0153:         from ..ops import Reshape
0154:         return Reshape.apply(self, shape)
0155: 
0156:     def pow(self, exponent: Union['Tensor', float]) -> 'Tensor':
0157:         """Returns tensor raised to the power of exponent."""
0158:         from ..ops import Power
0159:         return Power.apply(self, exponent)
0160: 
0161:     def div(self, other: Union['Tensor', float]) -> 'Tensor':
0162:         """Returns self divided by other."""
0163:         from ..ops import Divide
0164:         return Divide.apply(self, other)
0165: 
0166:     def log(self) -> 'Tensor':
0167:         """Returns the natural logarithm of the tensor."""
0168:         from ..ops import Log
0169:         return Log.apply(self)
0170: 
0171:     def exp(self) -> 'Tensor':
0172:         """Returns e raised to the power of each element in the tensor."""
0173:         from ..ops import Exp
0174:         return Exp.apply(self)
0175: 
0176:     def sigmoid(self) -> 'Tensor':
0177:         """Returns the sigmoid of the tensor."""
0178:         from ..ops import Sigmoid
0179:         return Sigmoid.apply(self)
0180: 
0181:     def tanh(self) -> 'Tensor':
0182:         """Returns the hyperbolic tangent of the tensor."""
0183:         from ..ops import Tanh
0184:         return Tanh.apply(self)
0185: 
0186:     def sum(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':
0187:         """Returns the sum of all elements in the tensor."""
0188:         from ..ops import Sum
0189:         return Sum.apply(self, axis, keepdims)
0190: 
0191:     def mean(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':
0192:         """Returns the mean of all elements in the tensor."""
0193:         from ..ops import Mean
0194:         return Mean.apply(self, axis, keepdims)
0195: 
0196:     def max(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':
0197:         """Returns the maximum value of all elements in the tensor."""
0198:         from ..ops import Max
0199:         return Max.apply(self, axis, keepdims)
0200: 
0201:     def t(self) -> 'Tensor':
0202:         """Returns the transpose of the tensor."""
0203:         from ..ops import Transpose
0204:         return Transpose.apply(self)
0205: 
0206:     def transpose(self, *axes: int) -> 'Tensor':
0207:         """Returns the transposed tensor."""
0208:         from ..ops import Transpose
0209:         return Transpose.apply(self, axes if axes else None)
0210: 
0211:     # Comparison operations
0212:     def __gt__(self, other: Union['Tensor', float]) -> 'Tensor':
0213:         from ..ops import Greater
0214:         return Greater.apply(self, other)
0215: 
0216:     def __ge__(self, other: Union['Tensor', float]) -> 'Tensor':
0217:         from ..ops import GreaterEqual
0218:         return GreaterEqual.apply(self, other)
0219: 
0220:     def __lt__(self, other: Union['Tensor', float]) -> 'Tensor':
0221:         from ..ops import Less
0222:         return Less.apply(self, other)
0223: 
0224:     def __le__(self, other: Union['Tensor', float]) -> 'Tensor':
0225:         from ..ops import LessEqual
0226:         return LessEqual.apply(self, other)
0227: 
0228:     def __eq__(self, other: Union['Tensor', float]) -> 'Tensor':
0229:         from ..ops import Equal
0230:         return Equal.apply(self, other)
0231: 
0232:     def __ne__(self, other: Union['Tensor', float]) -> 'Tensor':
0233:         from ..ops import NotEqual
0234:         return NotEqual.apply(self, other)
0235: 
0236:     def __truediv__(self, other: Union['Tensor', float]) -> 'Tensor':
0237:         """Implements division using the / operator."""
0238:         from ..ops import Divide
0239:         return Divide.apply(self, other)
0240: 
0241:     def __pow__(self, exponent: Union['Tensor', float]) -> 'Tensor':
0242:         """Implements power using the ** operator."""
0243:         from ..ops import Power
0244:         return Power.apply(self, exponent)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\__init__.py
// ----------------------------------------
0001: """
0002: Neural network module for DLpy.
0003: 
0004: This module contains all components needed for building neural networks.
0005: """
0006: 
0007: from .modules import Module
0008: from .linear import Linear
0009: from .activations import (
0010:     relu, leaky_relu, elu, gelu, sigmoid, tanh,
0011:     ReLU, LeakyReLU, ELU, GELU, Sigmoid, Tanh
0012: )
0013: 
0014: from .conv2d import Conv2d 
0015: 
0016: __all__ = [
0017:     # Base module
0018:     'Module',
0019:     
0020:     # Layers
0021:     'Linear',
0022:     
0023:     # Activation functions
0024:     'relu',
0025:     'leaky_relu',
0026:     'elu',
0027:     'gelu',
0028:     'sigmoid',
0029:     'tanh',
0030:     'ReLU',
0031:     'LeakyReLU',
0032:     'ELU',
0033:     'GELU',
0034:     'Sigmoid',
0035:     'Tanh',
0036: 
0037:     # Convolutional layers
0038:     'Conv2d'
0039: ]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\activations.py
// ----------------------------------------
0001: """
0002: Activation functions module for DLpy.
0003: 
0004: This module contains all standard activation functions used in neural networks.
0005: Each activation function is implemented as a Function subclass for autograd support.
0006: """
0007: 
0008: from typing import Dict, Optional
0009: import numpy as np
0010: from ..core import Function, Tensor
0011: 
0012: class ReLU(Function):
0013:     """
0014:     Rectified Linear Unit activation function.
0015:     
0016:     Forward: f(x) = max(0, x)
0017:     Backward: f'(x) = 1 if x > 0 else 0
0018:     """
0019:     
0020:     @staticmethod
0021:     def forward(ctx, x):
0022:         if not isinstance(x, Tensor):
0023:             x = Tensor(x)
0024:             
0025:         ctx.save_for_backward(x)
0026:         return Tensor(np.maximum(0, x.data))
0027:         
0028:     @staticmethod
0029:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0030:         x, = ctx.saved_tensors
0031:         if x.requires_grad:
0032:             grad = grad_output * (x.data > 0)
0033:             grad_dict[id(x)] = grad
0034: 
0035: class LeakyReLU(Function):
0036:     """
0037:     Leaky Rectified Linear Unit activation function.
0038:     
0039:     Forward: f(x) = x if x > 0 else negative_slope * x
0040:     Backward: f'(x) = 1 if x > 0 else negative_slope
0041:     
0042:     Args:
0043:         negative_slope: Controls slope for negative values. Default: 0.01
0044:     """
0045:     
0046:     @staticmethod
0047:     def forward(ctx, x, negative_slope: float = 0.01):
0048:         if not isinstance(x, Tensor):
0049:             x = Tensor(x)
0050:             
0051:         ctx.save_for_backward(x)
0052:         ctx.save_arguments(negative_slope=negative_slope)
0053:         
0054:         return Tensor(np.where(x.data > 0, x.data, negative_slope * x.data))
0055:         
0056:     @staticmethod
0057:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0058:         x, = ctx.saved_tensors
0059:         negative_slope = ctx.saved_arguments['negative_slope']
0060:         
0061:         if x.requires_grad:
0062:             grad = grad_output * np.where(x.data > 0, 1.0, negative_slope)
0063:             grad_dict[id(x)] = grad
0064: 
0065: class ELU(Function):
0066:     """
0067:     Exponential Linear Unit activation function.
0068:     
0069:     Forward: f(x) = x if x > 0 else alpha * (exp(x) - 1)
0070:     Backward: f'(x) = 1 if x > 0 else alpha * exp(x)
0071:     
0072:     Args:
0073:         alpha: Controls the value to which an ELU saturates for negative inputs. Default: 1.0
0074:     """
0075:     
0076:     @staticmethod
0077:     def forward(ctx, x, alpha: float = 1.0):
0078:         if not isinstance(x, Tensor):
0079:             x = Tensor(x)
0080:             
0081:         ctx.save_for_backward(x)
0082:         ctx.save_arguments(alpha=alpha)
0083:         
0084:         return Tensor(np.where(x.data > 0, x.data, alpha * (np.exp(x.data) - 1)))
0085:         
0086:     @staticmethod
0087:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0088:         x, = ctx.saved_tensors
0089:         alpha = ctx.saved_arguments['alpha']
0090:         
0091:         if x.requires_grad:
0092:             grad = grad_output * np.where(x.data > 0, 1.0, alpha * np.exp(x.data))
0093:             grad_dict[id(x)] = grad
0094: 
0095: class GELU(Function):
0096:     """
0097:     Gaussian Error Linear Unit activation function.
0098:     
0099:     Forward: f(x) = x * Φ(x)
0100:     where Φ(x) is the Gaussian cumulative distribution function.
0101:     
0102:     This implementation uses the approximation:
0103:     f(x) ≈ 0.5x * (1 + tanh(√(2/π) * (x + 0.044715x³)))
0104:     """
0105:     
0106:     @staticmethod
0107:     def forward(ctx, x):
0108:         if not isinstance(x, Tensor):
0109:             x = Tensor(x)
0110:             
0111:         # Constants for the approximation
0112:         sqrt_2_over_pi = np.sqrt(2 / np.pi)
0113:         coeff = 0.044715
0114:         
0115:         # Compute intermediate values
0116:         x_cubed = x.data ** 3
0117:         inner = sqrt_2_over_pi * (x.data + coeff * x_cubed)
0118:         tanh_inner = np.tanh(inner)
0119:         
0120:         # Compute output
0121:         result = 0.5 * x.data * (1 + tanh_inner)
0122:         
0123:         # Save for backward pass
0124:         ctx.save_for_backward(x)
0125:         ctx.save_arguments(tanh_inner=tanh_inner)
0126:         
0127:         return Tensor(result)
0128:         
0129:     @staticmethod
0130:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0131:         x, = ctx.saved_tensors
0132:         tanh_inner = ctx.saved_arguments['tanh_inner']
0133:         
0134:         if x.requires_grad:
0135:             sqrt_2_over_pi = np.sqrt(2 / np.pi)
0136:             coeff = 0.044715
0137:             
0138:             # Compute derivative
0139:             x_cubed = x.data ** 3
0140:             inner = sqrt_2_over_pi * (x.data + coeff * x_cubed)
0141:             
0142:             # d/dx[GELU(x)] = 0.5 * (1 + tanh(inner)) + 
0143:             #                 0.5x * (1 - tanh²(inner)) * sqrt(2/π) * (1 + 3 * 0.044715x²)
0144:             grad = 0.5 * (1 + tanh_inner)
0145:             grad += 0.5 * x.data * (1 - tanh_inner ** 2) * sqrt_2_over_pi * (1 + 3 * coeff * x.data ** 2)
0146:             
0147:             grad_dict[id(x)] = grad_output * grad
0148: 
0149: class Sigmoid(Function):
0150:     """
0151:     Sigmoid activation function.
0152:     
0153:     Forward: f(x) = 1 / (1 + exp(-x))
0154:     Backward: f'(x) = f(x) * (1 - f(x))
0155:     """
0156:     
0157:     @staticmethod
0158:     def forward(ctx, x):
0159:         if not isinstance(x, Tensor):
0160:             x = Tensor(x)
0161:             
0162:         # Compute sigmoid with numerical stability
0163:         x_data = x.data
0164:         exp_neg_x = np.exp(-np.abs(x_data))
0165:         sigmoid_x = np.where(x_data >= 0, 
0166:                            1 / (1 + exp_neg_x),
0167:                            exp_neg_x / (1 + exp_neg_x))
0168:         
0169:         ctx.save_for_backward(x)
0170:         ctx.save_arguments(sigmoid_x=sigmoid_x)
0171:         return Tensor(sigmoid_x)
0172:         
0173:     @staticmethod
0174:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0175:         x, = ctx.saved_tensors
0176:         sigmoid_x = ctx.saved_arguments['sigmoid_x']
0177:         
0178:         if x.requires_grad:
0179:             grad = grad_output * sigmoid_x * (1 - sigmoid_x)
0180:             grad_dict[id(x)] = grad
0181: 
0182: class Tanh(Function):
0183:     """
0184:     Hyperbolic tangent activation function.
0185:     
0186:     Forward: f(x) = tanh(x)
0187:     Backward: f'(x) = 1 - tanh²(x)
0188:     """
0189:     
0190:     @staticmethod
0191:     def forward(ctx, x):
0192:         if not isinstance(x, Tensor):
0193:             x = Tensor(x)
0194:             
0195:         tanh_x = np.tanh(x.data)
0196:         ctx.save_for_backward(x)
0197:         ctx.save_arguments(tanh_x=tanh_x)
0198:         return Tensor(tanh_x)
0199:         
0200:     @staticmethod
0201:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0202:         x, = ctx.saved_tensors
0203:         tanh_x = ctx.saved_arguments['tanh_x']
0204:         
0205:         if x.requires_grad:
0206:             grad = grad_output * (1 - tanh_x ** 2)
0207:             grad_dict[id(x)] = grad
0208: 
0209: # Add convenience functions for each activation
0210: def relu(x: Tensor) -> Tensor:
0211:     """Applies ReLU activation function."""
0212:     return ReLU.apply(x)
0213: 
0214: def leaky_relu(x: Tensor, negative_slope: float = 0.01) -> Tensor:
0215:     """Applies Leaky ReLU activation function."""
0216:     return LeakyReLU.apply(x, negative_slope)
0217: 
0218: def elu(x: Tensor, alpha: float = 1.0) -> Tensor:
0219:     """Applies ELU activation function."""
0220:     return ELU.apply(x, alpha)
0221: 
0222: def gelu(x: Tensor) -> Tensor:
0223:     """Applies GELU activation function."""
0224:     return GELU.apply(x)
0225: 
0226: def sigmoid(x: Tensor) -> Tensor:
0227:     """Applies Sigmoid activation function."""
0228:     return Sigmoid.apply(x)
0229: 
0230: def tanh(x: Tensor) -> Tensor:
0231:     """Applies Tanh activation function."""
0232:     return Tanh.apply(x)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\conv2d.py
// ----------------------------------------
0001: from typing import Tuple, Optional, Union
0002: import numpy as np
0003: from ..core import Tensor
0004: from .modules import Module
0005: from ..ops.cnn import Conv2dFunction
0006: 
0007: def _pair(x: Union[int, Tuple[int, int]]) -> Tuple[int, int]:
0008:     """Convert input to a pair of values."""
0009:     if isinstance(x, tuple):
0010:         return x
0011:     return (x, x)
0012: 
0013: class Conv2d(Module):
0014:     """
0015:     Applies a 2D convolution over an input signal composed of several input planes.
0016:     
0017:     Args:
0018:         in_channels (int): Number of channels in the input image
0019:         out_channels (int): Number of channels produced by the convolution
0020:         kernel_size (int or tuple): Size of the convolving kernel
0021:         stride (int or tuple, optional): Stride of the convolution. Default: 1
0022:         padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0
0023:         dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
0024:         groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 
0025:         bias (bool, optional): If True, adds a learnable bias to the output. Default: True
0026:         
0027:     Shape:
0028:         - Input: (N, C_in, H, W)
0029:         - Output: (N, C_out, H_out, W_out)
0030:           where
0031:           H_out = (H + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1
0032:           W_out = (W + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1
0033:     """
0034:     
0035:     def __init__(
0036:         self,
0037:         in_channels: int,
0038:         out_channels: int,
0039:         kernel_size: Union[int, Tuple[int, int]],
0040:         stride: Union[int, Tuple[int, int]] = 1,
0041:         padding: Union[int, Tuple[int, int]] = 0,
0042:         dilation: Union[int, Tuple[int, int]] = 1,
0043:         groups: int = 1,
0044:         bias: bool = True
0045:     ):
0046:         super().__init__()
0047:         
0048:         if in_channels % groups != 0:
0049:             raise ValueError('in_channels must be divisible by groups')
0050:         if out_channels % groups != 0:
0051:             raise ValueError('out_channels must be divisible by groups')
0052:             
0053:         self.in_channels = in_channels
0054:         self.out_channels = out_channels
0055:         self.kernel_size = _pair(kernel_size)
0056:         self.stride = _pair(stride)
0057:         self.padding = _pair(padding)
0058:         self.dilation = _pair(dilation)
0059:         self.groups = groups
0060:         
0061:         # Initialize weights using He initialization
0062:         # Adjust fan_in to account for groups
0063:         fan_in = in_channels // groups * self.kernel_size[0] * self.kernel_size[1]
0064:         bound = np.sqrt(2.0 / fan_in)
0065:         weight_shape = (out_channels, in_channels // groups, *self.kernel_size)
0066:         weight = Tensor(
0067:             np.random.uniform(-bound, bound, weight_shape),
0068:             requires_grad=True
0069:         )
0070:         self.register_parameter('weight', weight)
0071:         
0072:         if bias:
0073:             # Initialize bias to zero
0074:             bias_data = np.zeros(out_channels)
0075:             self.register_parameter('bias', Tensor(bias_data, requires_grad=True))
0076:         else:
0077:             self.register_parameter('bias', None)
0078:             
0079:     def forward(self, x: Tensor) -> Tensor:
0080:         """
0081:         Forward pass of the convolution layer.
0082:         
0083:         Args:
0084:             x: Input tensor of shape (N, C_in, H, W)
0085:             
0086:         Returns:
0087:             Output tensor of shape (N, C_out, H_out, W_out)
0088:         """
0089:         return Conv2dFunction.apply(
0090:             x, self.weight, self.bias,
0091:             self.stride, self.padding,
0092:             self.dilation, self.groups
0093:         )
0094:         
0095:     def extra_repr(self) -> str:
0096:         """Returns a string with extra representation information."""
0097:         s = (f'{self.in_channels}, {self.out_channels}, '
0098:              f'kernel_size={self.kernel_size}')
0099:         
0100:         if self.stride != (1, 1):
0101:             s += f', stride={self.stride}'
0102:         if self.padding != (0, 0):
0103:             s += f', padding={self.padding}'
0104:         if self.dilation != (1, 1):
0105:             s += f', dilation={self.dilation}'
0106:         if self.groups != 1:
0107:             s += f', groups={self.groups}'
0108:         if self.bias is None:
0109:             s += ', bias=False'
0110:         return s
0111: 
0112:     @staticmethod
0113:     def calc_output_shape(
0114:         input_shape: Tuple[int, ...],
0115:         out_channels: int,
0116:         kernel_size: Tuple[int, int],
0117:         stride: Tuple[int, int],
0118:         padding: Tuple[int, int],
0119:         dilation: Tuple[int, int]
0120:     ) -> Tuple[int, int, int, int]:
0121:         """
0122:         Calculate the output shape of the convolution.
0123:         
0124:         Args:
0125:             input_shape: Input shape (N, C_in, H, W)
0126:             out_channels: Number of output channels
0127:             kernel_size: Size of the kernel
0128:             stride: Stride of the convolution
0129:             padding: Zero-padding added to both sides
0130:             dilation: Spacing between kernel elements
0131:             
0132:         Returns:
0133:             Output shape (N, C_out, H_out, W_out)
0134:         """
0135:         N, _, H, W = input_shape
0136:         
0137:         H_out = ((H + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) 
0138:                 // stride[0] + 1)
0139:         W_out = ((W + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) 
0140:                 // stride[1] + 1)
0141:         
0142:         return (N, out_channels, H_out, W_out)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\linear.py
// ----------------------------------------
0001: from typing import Optional, Dict
0002: import numpy as np
0003: from ..core import Tensor, Function
0004: from .modules import Module
0005: 
0006: class LinearFunction(Function):
0007:     @staticmethod
0008:     def forward(ctx, input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
0009:         # Save tensors needed for backward pass
0010:         ctx.save_for_backward(input, weight, bias)
0011:         
0012:         # Compute output: y = xW^T + b
0013:         output = input.data @ weight.data
0014:         if bias is not None:
0015:             output += bias.data
0016:             
0017:         return Tensor(output)
0018:     
0019:     @staticmethod
0020:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0021:         # Retrieve saved tensors
0022:         input, weight, bias = ctx.saved_tensors
0023:         
0024:         # Compute gradient with respect to input: dx = dout @ W
0025:         if input.requires_grad:
0026:             grad_dict[id(input)] = grad_output @ weight.data.T
0027:             
0028:         # Compute gradient with respect to weight: dW = x^T @ dout
0029:         if weight.requires_grad:
0030:             grad_dict[id(weight)] = input.data.T @ grad_output
0031:             
0032:         # Compute gradient with respect to bias: db = sum(dout, dim=0)
0033:         if bias is not None and bias.requires_grad:
0034:             grad_dict[id(bias)] = grad_output.sum(axis=0)
0035: 
0036: class Linear(Module):
0037:     """
0038:     Applies a linear transformation to the incoming data: y = xW^T + b
0039:     
0040:     Args:
0041:         in_features: size of each input sample
0042:         out_features: size of each output sample
0043:         bias: If set to False, the layer will not learn an additive bias
0044:         
0045:     Shape:
0046:         - Input: (batch_size, in_features)
0047:         - Output: (batch_size, out_features)
0048:         
0049:     Attributes:
0050:         weight: the learnable weights of shape (in_features, out_features)
0051:         bias: the learnable bias of shape (out_features,)
0052:     """
0053:     
0054:     def __init__(self, in_features: int, out_features: int, bias: bool = True):
0055:         super().__init__()
0056:         
0057:         self.in_features = in_features
0058:         self.out_features = out_features
0059:         
0060:         # Initialize weights using He initialization
0061:         bound = np.sqrt(2.0 / in_features)
0062:         weight = Tensor(
0063:             np.random.uniform(-bound, bound, (in_features, out_features)),
0064:             requires_grad=True
0065:         )
0066:         self.register_parameter('weight', weight)
0067:         
0068:         if bias:
0069:             bias = Tensor(np.zeros(out_features), requires_grad=True)
0070:             self.register_parameter('bias', bias)
0071:         else:
0072:             self.register_parameter('bias', None)
0073:             
0074:     def forward(self, input: Tensor) -> Tensor:
0075:         """Forward pass of the linear layer."""
0076:         return LinearFunction.apply(input, self.weight, self.bias)
0077:             
0078:     def extra_repr(self) -> str:
0079:         """Extra information to add to the string representation."""
0080:         return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\modules.py
// ----------------------------------------
0001: from typing import Iterator, Dict, Any, Optional, Union
0002: from collections import OrderedDict
0003: from ..core import Tensor
0004: 
0005: class Module:
0006:     """
0007:     Base class for all neural network modules.
0008:     
0009:     Your models should also subclass this class.
0010:     Modules can also contain other Modules, allowing to nest them in
0011:     a tree structure.
0012:     """
0013:     
0014:     def __init__(self):
0015:         """Initialize the module."""
0016:         # First set these directly to avoid triggering __setattr__
0017:         object.__setattr__(self, 'training', True)
0018:         object.__setattr__(self, '_parameters', OrderedDict())
0019:         object.__setattr__(self, '_buffers', OrderedDict())
0020:         object.__setattr__(self, '_modules', OrderedDict())
0021:         
0022:     def register_parameter(self, name: str, param: Optional[Tensor]) -> None:
0023:         """Add a parameter to the module.
0024:         
0025:         Args:
0026:             name: Name of the parameter
0027:             param: The parameter tensor to register
0028:         """
0029:         if '_parameters' not in self.__dict__:
0030:             raise TypeError(
0031:                 "cannot assign parameter before Module.__init__() call"
0032:             )
0033:             
0034:         if param is not None and not isinstance(param, Tensor):
0035:             raise TypeError(f"Parameter {name} must be a Tensor, not {type(param)}")
0036:             
0037:         self._parameters[name] = param
0038:         
0039:     def register_buffer(self, name: str, tensor: Optional[Tensor]) -> None:
0040:         """Add a persistent buffer to the module.
0041:         
0042:         Buffers are typically used for running statistics in modules like BatchNorm.
0043:         
0044:         Args:
0045:             name: Name of the buffer
0046:             tensor: The tensor to register as a buffer
0047:         """
0048:         if '_buffers' not in self.__dict__:
0049:             raise TypeError(
0050:                 "cannot assign buffer before Module.__init__() call"
0051:             )
0052:             
0053:         if tensor is not None and not isinstance(tensor, Tensor):
0054:             raise TypeError(f"Buffer {name} must be a Tensor, not {type(tensor)}")
0055:             
0056:         self._buffers[name] = tensor
0057:         
0058:     def add_module(self, name: str, module: Optional['Module']) -> None:
0059:         """Add a child module to the current module.
0060:         
0061:         Args:
0062:             name: Name of the child module
0063:             module: The module to add
0064:         """
0065:         if not isinstance(module, (Module, type(None))):
0066:             raise TypeError(f"{name} is not a Module subclass")
0067:             
0068:         if '_modules' not in self.__dict__:
0069:             raise TypeError(
0070:                 "cannot assign module before Module.__init__() call"
0071:             )
0072:             
0073:         self._modules[name] = module
0074:         
0075:     def __getattr__(self, name: str) -> Any:
0076:         """Custom getattr that looks through parameters, buffers, and modules."""
0077:         if '_parameters' in self.__dict__:
0078:             _parameters = self.__dict__['_parameters']
0079:             if name in _parameters:
0080:                 return _parameters[name]
0081:                 
0082:         if '_buffers' in self.__dict__:
0083:             _buffers = self.__dict__['_buffers']
0084:             if name in _buffers:
0085:                 return _buffers[name]
0086:                 
0087:         if '_modules' in self.__dict__:
0088:             modules = self.__dict__['_modules']
0089:             if name in modules:
0090:                 return modules[name]
0091:                 
0092:         raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
0093:         
0094:     def __setattr__(self, name: str, value: Any) -> None:
0095:         """Custom setattr that handles parameter registration."""
0096:         # Handle special module attributes first
0097:         if name in ['training']:
0098:             object.__setattr__(self, name, value)
0099:             return
0100:             
0101:         if isinstance(value, Tensor):
0102:             if not hasattr(self, '_parameters'):
0103:                 raise TypeError(
0104:                     "cannot assign parameters before Module.__init__() call"
0105:                 )
0106:             self.register_parameter(name, value)
0107:         elif isinstance(value, Module):
0108:             if not hasattr(self, '_modules'):
0109:                 raise TypeError(
0110:                     "cannot assign module before Module.__init__() call"
0111:                 )
0112:             self.add_module(name, value)
0113:         else:
0114:             object.__setattr__(self, name, value)
0115:             
0116:     def parameters(self) -> Iterator[Tensor]:
0117:         """Returns an iterator over module parameters."""
0118:         for param in self._parameters.values():
0119:             if param is not None:
0120:                 yield param
0121:         for module in self._modules.values():
0122:             if module is not None:
0123:                 yield from module.parameters()
0124:                 
0125:     def named_parameters(self) -> Iterator[tuple[str, Tensor]]:
0126:         """Returns an iterator over module parameters, yielding both the
0127:         name of the parameter as well as the parameter itself."""
0128:         for name, param in self._parameters.items():
0129:             if param is not None:
0130:                 yield name, param
0131:         for mname, module in self._modules.items():
0132:             if module is not None:
0133:                 for name, param in module.named_parameters():
0134:                     yield f"{mname}.{name}", param
0135:                     
0136:     def train(self, mode: bool = True) -> 'Module':
0137:         """Sets the module in training mode."""
0138:         self.training = mode
0139:         for module in self._modules.values():
0140:             if module is not None:
0141:                 module.train(mode)
0142:         return self
0143:         
0144:     def eval(self) -> 'Module':
0145:         """Sets the module in evaluation mode."""
0146:         return self.train(False)
0147:         
0148:     def __call__(self, *args, **kwargs):
0149:         return self.forward(*args, **kwargs)
0150:         
0151:     def forward(self, *args, **kwargs):
0152:         """Define the computation performed at every call."""
0153:         raise NotImplementedError
0154:         
0155:     def __repr__(self):
0156:         """Returns a string representation of the module."""
0157:         extra_lines = []
0158:         extra_repr = self.extra_repr()
0159:         if extra_repr:
0160:             extra_lines = extra_repr.split('\n')
0161:             
0162:         child_lines = []
0163:         for key, module in self._modules.items():
0164:             mod_str = repr(module)
0165:             mod_str = _addindent(mod_str, 2)
0166:             child_lines.append('(' + key + '): ' + mod_str)
0167:             
0168:         lines = extra_lines + child_lines
0169:         
0170:         main_str = self.__class__.__name__ + '('
0171:         if lines:
0172:             main_str += '\n  ' + '\n  '.join(lines) + '\n'
0173:         main_str += ')'
0174:         return main_str
0175:         
0176:     def extra_repr(self) -> str:
0177:         """Set the extra representation of the module."""
0178:         return ''
0179: 
0180: def _addindent(s_: str, numSpaces: int) -> str:
0181:     """Helper for indenting multiline strings."""
0182:     s = s_.split('\n')
0183:     if len(s) == 1:
0184:         return s_
0185:     first = s.pop(0)
0186:     s = [(numSpaces * ' ') + line for line in s]
0187:     return '\n'.join([first] + s)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\sequential.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\__init__.py
// ----------------------------------------
0001: """
0002: Operations module for DLpy.
0003: 
0004: This module contains all the mathematical operations that can be performed on tensors.
0005: """
0006: 
0007: from .basic import Add, Multiply
0008: from .reshape import Reshape
0009: from .power import Power, Divide
0010: from .elementwise import Log, Exp
0011: from .reduction import Sum, Mean, Max
0012: from .matrix import (
0013:     Transpose,
0014:     Greater,
0015:     GreaterEqual,
0016:     Less,
0017:     LessEqual,
0018:     Equal,
0019:     NotEqual
0020: )
0021: from .loss import (
0022:     MSELoss,
0023:     CrossEntropyLoss,
0024:     BinaryCrossEntropyLoss,
0025:     L1Loss,
0026:     HuberLoss,
0027:     KLDivLoss,
0028:     CosineSimilarityLoss,
0029:     HingeLoss,
0030:     FocalLoss
0031: )
0032: 
0033: from .cnn import Conv2dFunction
0034: 
0035: __all__ = [
0036:     # Basic operations
0037:     'Add',
0038:     'Multiply',
0039:     'Reshape',
0040:     
0041:     # Power operations
0042:     'Power',
0043:     'Divide',
0044:     
0045:     # Element-wise operations
0046:     'Log',
0047:     'Exp',
0048:     
0049:     # Reduction operations
0050:     'Sum',
0051:     'Mean',
0052:     'Max',
0053:     
0054:     # Matrix operations
0055:     'Transpose',
0056:     
0057:     # Comparison operations
0058:     'Greater',
0059:     'GreaterEqual',
0060:     'Less',
0061:     'LessEqual',
0062:     'Equal',
0063:     'NotEqual',
0064: 
0065:     # Loss functions
0066:     'MSELoss',
0067:     'CrossEntropyLoss',
0068:     'BinaryCrossEntropyLoss',
0069:     'L1Loss',
0070:     'HuberLoss',
0071:     'KLDivLoss',
0072:     'CosineSimilarityLoss',
0073:     'HingeLoss',
0074:     'FocalLoss',
0075: 
0076:     # CNN operations
0077:     'Conv2dFunction'
0078: ]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\basic.py
// ----------------------------------------
0001: from typing import Dict  # Add this import at the top
0002: from ..core.function import Function
0003: from ..core.tensor import Tensor
0004: import numpy as np
0005: 
0006: class Add(Function):
0007:     @staticmethod
0008:     def forward(ctx, a, b):
0009:         if not isinstance(a, Tensor):
0010:             a = Tensor(a)
0011:         if not isinstance(b, Tensor):
0012:             b = Tensor(b)
0013:             
0014:         shape_a = a.data.shape
0015:         shape_b = b.data.shape
0016: 
0017:         # Check valid broadcasting manually
0018:         if len(shape_a) == 2 and shape_a[0] == 1 and len(shape_b) == 1:
0019:             # Special case: (1,N) matrix with (M,) vector requires N==M
0020:             if shape_a[1] != shape_b[0]:
0021:                 raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
0022:         elif len(shape_a) == 1 and len(shape_b) == 2 and shape_b[0] == 1:
0023:             # Special case: (N,) vector with (1,M) matrix requires N==M
0024:             if shape_a[0] != shape_b[1]:
0025:                 raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
0026:                 
0027:         # Save tensors for backward pass
0028:         ctx.save_for_backward(a, b)
0029:         
0030:         # If we get here, try the operation
0031:         try:
0032:             result = a.data + b.data
0033:             return Tensor(result)
0034:         except ValueError:
0035:             raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
0036:         
0037:     @staticmethod
0038:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0039:         a, b = ctx.saved_tensors
0040: 
0041:         if a.requires_grad:
0042:             grad_a = grad_output
0043:             grad_a = Add._reduce_grad(grad_a, a.data.shape)
0044:             if id(a) not in grad_dict or grad_dict[id(a)] is None:
0045:                 grad_dict[id(a)] = grad_a
0046:             else:
0047:                 grad_dict[id(a)] += grad_a  # Accumulate gradients
0048: 
0049:         if b.requires_grad:
0050:             grad_b = grad_output
0051:             grad_b = Add._reduce_grad(grad_b, b.data.shape)
0052:             if id(b) not in grad_dict or grad_dict[id(b)] is None:
0053:                 grad_dict[id(b)] = grad_b
0054:             else:
0055:                 grad_dict[id(b)] += grad_b  # Accumulate gradients
0056: 
0057:     @staticmethod
0058:     def _reduce_grad(grad, target_shape):
0059:         """
0060:         Reduces the gradient to match the target shape by summing over broadcasted dimensions.
0061:         """
0062:         # Convert target_shape to a tuple if it's not
0063:         if not isinstance(target_shape, tuple):
0064:             target_shape = tuple(target_shape)
0065:         
0066:         # Align the dimensions by prepending 1s if necessary
0067:         grad_shape = grad.shape
0068:         target_shape = (1,) * (len(grad_shape) - len(target_shape)) + target_shape
0069:         for axis, (grad_dim, target_dim) in enumerate(zip(grad_shape, target_shape)):
0070:             if target_dim == 1 and grad_dim != 1:
0071:                 grad = grad.sum(axis=axis, keepdims=True)
0072:         return grad
0073: 
0074: class Multiply(Function):
0075:     @staticmethod
0076:     def forward(ctx, a, b):
0077:         if not isinstance(a, Tensor):
0078:             a = Tensor(a)
0079:         if not isinstance(b, Tensor):
0080:             b = Tensor(b)
0081:             
0082:         shape_a = a.data.shape
0083:         shape_b = b.data.shape
0084:         
0085:         # Check if shapes can be broadcast according to NumPy rules
0086:         try:
0087:             # Test broadcast compatibility without actually performing the operation
0088:             np.broadcast_shapes(shape_a, shape_b)
0089:             # If we get here, shapes are compatible
0090:             result = a.data * b.data
0091:             ctx.save_for_backward(a, b)
0092:             return Tensor(result)
0093:         except ValueError:
0094:             raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
0095:         
0096:     @staticmethod
0097:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0098:         a, b = ctx.saved_tensors
0099: 
0100:         if a.requires_grad:
0101:             grad_a = grad_output * b.data
0102:             grad_a = Multiply._reduce_grad(grad_a, a.data.shape)
0103:             if id(a) not in grad_dict or grad_dict[id(a)] is None:
0104:                 grad_dict[id(a)] = grad_a
0105:             else:
0106:                 grad_dict[id(a)] += grad_a  # Accumulate gradients
0107: 
0108:         if b.requires_grad:
0109:             grad_b = grad_output * a.data
0110:             grad_b = Multiply._reduce_grad(grad_b, b.data.shape)
0111:             if id(b) not in grad_dict or grad_dict[id(b)] is None:
0112:                 grad_dict[id(b)] = grad_b
0113:             else:
0114:                 grad_dict[id(b)] += grad_b  # Accumulate gradients
0115: 
0116:     @staticmethod
0117:     def _reduce_grad(grad, target_shape):
0118:         """
0119:         Reduces the gradient to match the target shape by summing over broadcasted dimensions.
0120:         """
0121:         # Convert target_shape to a tuple if it's not
0122:         if not isinstance(target_shape, tuple):
0123:             target_shape = tuple(target_shape)
0124:         
0125:         # Align the dimensions by prepending 1s if necessary
0126:         grad_shape = grad.shape
0127:         target_shape = (1,) * (len(grad_shape) - len(target_shape)) + target_shape
0128:         for axis, (grad_dim, target_dim) in enumerate(zip(grad_shape, target_shape)):
0129:             if target_dim == 1 and grad_dim != 1:
0130:                 grad = grad.sum(axis=axis, keepdims=True)
0131:         return grad

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\cnn.py
// ----------------------------------------
0001: from typing import Tuple, Dict, Optional, Union, List
0002: import numpy as np
0003: from ..core import Function, Tensor, Context
0004: 
0005: class ConvMode:
0006:     """Enumeration of convolution modes."""
0007:     STANDARD = "standard"
0008:     TRANSPOSED = "transposed"
0009:     DEFORMABLE = "deformable"
0010: 
0011: def _validate_conv_params(
0012:     x_shape: tuple,
0013:     weight_shape: tuple,
0014:     stride: tuple,
0015:     padding: tuple,
0016:     dilation: tuple,
0017:     groups: int,
0018:     mode: str = ConvMode.STANDARD,
0019:     offset: Optional[Tensor] = None,
0020:     weight: Optional[Tensor] = None, # Added weight parameter
0021:     mask: Optional[Tensor] = None
0022: ) -> None:
0023:     """Validates convolution parameters."""
0024:     N, C_in, H, W = x_shape
0025:     C_out, C_in_per_group, kH, kW = weight_shape
0026: 
0027:     if C_in % groups != 0:
0028:         raise ValueError(f"Input channels ({C_in}) must be divisible by groups ({groups})")
0029:         
0030:     if C_out % groups != 0:
0031:         raise ValueError(f"Output channels ({C_out}) must be divisible by groups ({groups})")
0032:         
0033:     if C_in_per_group != C_in // groups:
0034:         raise ValueError(f"Expected {C_in // groups} input channels per group, got {C_in_per_group}")
0035:         
0036:     if mode not in [ConvMode.STANDARD, ConvMode.TRANSPOSED, ConvMode.DEFORMABLE]:
0037:         raise ValueError(f"Invalid convolution mode: {mode}")
0038:         
0039:     if mode == ConvMode.DEFORMABLE:
0040:         # Check for offset either in direct parameter or weight.offset
0041:         offset_tensor = offset
0042:         if offset_tensor is None and weight is not None:
0043:             offset_tensor = getattr(weight, 'offset', None)
0044:             
0045:         if offset_tensor is None:
0046:             raise ValueError("Deformable convolution requires offset parameter")
0047:         
0048:         # Calculate output size
0049:         H_out = (H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0] + 1
0050:         W_out = (W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1] + 1
0051:         
0052:         # Check offset tensor shape
0053:         expected_offset_shape = (N, 2 * kH * kW, H_out, W_out)
0054:         if offset_tensor.shape != expected_offset_shape:
0055:             raise ValueError(f"Expected offset shape {expected_offset_shape}, got {offset_tensor.shape}")
0056:             
0057:         # Check mask tensor shape if provided
0058:         if mask is not None:
0059:             expected_mask_shape = (N, kH * kW, H_out, W_out)
0060:             if mask.shape != expected_mask_shape:
0061:                 raise ValueError(f"Expected mask shape {expected_mask_shape}, got {mask.shape}")
0062: 
0063: 
0064: def _pad_input(x: np.ndarray, padding: Tuple[int, int]) -> np.ndarray:
0065:     """
0066:     Pads input tensor with zeros.
0067:     
0068:     Args:
0069:         x: Input tensor
0070:         padding: (padding_height, padding_width)
0071:     """
0072:     if padding[0] == 0 and padding[1] == 0:
0073:         return x
0074:     pad_width = ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1]))
0075:     return np.pad(x, pad_width, mode='constant', constant_values=0)
0076: 
0077: def _get_output_shape(input_shape: Tuple[int, ...], kernel_size: Tuple[int, int],
0078:                     stride: Tuple[int, int], padding: Tuple[int, int],
0079:                     dilation: Tuple[int, int], mode: str = ConvMode.STANDARD) -> Tuple[int, int]:
0080:     """
0081:     Calculates output shape for different convolution types.
0082:     """
0083:     if mode == ConvMode.STANDARD:
0084:         H = ((input_shape[2] + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) 
0085:              // stride[0] + 1)
0086:         W = ((input_shape[3] + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) 
0087:              // stride[1] + 1)
0088:     elif mode == ConvMode.TRANSPOSED:
0089:         H = (input_shape[2] - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + 1
0090:         W = (input_shape[3] - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + 1
0091:     else:  # Deformable follows standard conv shape
0092:         H = ((input_shape[2] + 2 * padding[0] - kernel_size[0]) // stride[0] + 1)
0093:         W = ((input_shape[3] + 2 * padding[1] - kernel_size[1]) // stride[1] + 1)
0094:     return H, W
0095: 
0096: def _get_deformable_offsets(offset_tensor: np.ndarray, kernel_size: Tuple[int, int],
0097:                          input_shape: Tuple[int, ...]) -> np.ndarray:
0098:     """
0099:     Computes sampling locations for deformable convolution.
0100:     
0101:     Args:
0102:         offset_tensor: Offset values of shape (N, 2*kH*kW, H_out, W_out)
0103:         kernel_size: Size of the convolving kernel (kH, kW)
0104:         input_shape: Shape of input tensor (N, C, H, W)
0105:         
0106:     Returns:
0107:         Sampling locations of shape (N, H_out*W_out, kH*kW, 2)
0108:     """
0109:     N, C, H, W = input_shape
0110:     kH, kW = kernel_size
0111:     H_out, W_out = offset_tensor.shape[2:]
0112:     
0113:     # Convert memoryview to numpy array if needed
0114:     if isinstance(offset_tensor, memoryview):
0115:         offset_tensor = np.array(offset_tensor)
0116:     
0117:     # Generate base sampling grid for each output position
0118:     base_h = np.arange(kH)
0119:     base_w = np.arange(kW)
0120:     mesh_h, mesh_w = np.meshgrid(base_h, base_w, indexing='ij')
0121:     base_grid = np.stack([mesh_h, mesh_w], axis=-1)  # (kH, kW, 2)
0122:     
0123:     # Reshape for broadcasting
0124:     kHW = kH * kW
0125:     base_grid = base_grid.reshape(-1, 2).T  # (2, kH*kW)
0126:     base_grid = base_grid[None, None, :, :]  # (1, 1, 2, kH*kW)
0127:     
0128:     # Reshape offset tensor to (N, H_out*W_out, 2, kH*kW)
0129:     offset_tensor = offset_tensor.reshape(N, 2, kHW, H_out, W_out)
0130:     offset_tensor = offset_tensor.transpose(0, 3, 4, 1, 2)  # (N, H_out, W_out, 2, kH*kW)
0131:     offset_tensor = offset_tensor.reshape(N, H_out * W_out, 2, kHW)
0132:     
0133:     # Broadcast base grid to match offset tensor shape
0134:     base_grid = np.broadcast_to(base_grid, (N, H_out * W_out, 2, kHW))
0135:     
0136:     # Add offsets to base grid
0137:     sampling_locations = base_grid + offset_tensor
0138:     
0139:     # Ensure output is in correct format (N, H_out*W_out, kH*kW, 2)
0140:     return sampling_locations.transpose(0, 1, 3, 2)
0141: 
0142: def _bilinear_interpolate(input: np.ndarray, points: np.ndarray, align_corners: bool = True) -> np.ndarray:
0143:     """
0144:     Performs bilinear interpolation on the input tensor at specified points.
0145:     
0146:     Args:
0147:         input: Input tensor (N, C, H, W)
0148:         points: Points to sample (N, P, 2) in normalized coordinates [-1, 1]
0149:         align_corners: Whether to align corners in interpolation
0150:         
0151:     Returns:
0152:         Interpolated values (N, C, P)
0153:     """
0154:     N, C, H, W = input.shape
0155:     
0156:     # Ensure points is correct shape (N, P, 2)
0157:     if points.ndim == 4:
0158:         points = points.reshape(points.shape[0], -1, 2)
0159:     
0160:     _, P, _ = points.shape
0161:     
0162:     # Convert normalized coordinates to pixel coordinates
0163:     if align_corners:
0164:         x = (points[..., 0] + 1) * (W - 1) / 2
0165:         y = (points[..., 1] + 1) * (H - 1) / 2
0166:     else:
0167:         x = ((points[..., 0] + 1) * W - 1) / 2
0168:         y = ((points[..., 1] + 1) * H - 1) / 2
0169:     
0170:     # Get corner indices
0171:     x0 = np.floor(x).astype(np.int32)
0172:     x1 = x0 + 1
0173:     y0 = np.floor(y).astype(np.int32)
0174:     y1 = y0 + 1
0175:     
0176:     # Clip to image boundaries
0177:     x0 = np.clip(x0, 0, W - 1)
0178:     x1 = np.clip(x1, 0, W - 1)
0179:     y0 = np.clip(y0, 0, H - 1)
0180:     y1 = np.clip(y1, 0, H - 1)
0181:     
0182:     # Calculate interpolation weights
0183:     wa = (x1 - x) * (y1 - y)
0184:     wb = (x1 - x) * (y - y0)
0185:     wc = (x - x0) * (y1 - y)
0186:     wd = (x - x0) * (y - y0)
0187:     
0188:     # Reshape weights for broadcasting
0189:     wa = wa[..., None]
0190:     wb = wb[..., None]
0191:     wc = wc[..., None]
0192:     wd = wd[..., None]
0193:     
0194:     # Gather corner values and compute weighted sum
0195:     output = np.zeros((N, C, P))
0196:     for n in range(N):
0197:         output[n] = (wa[n] * input[n, :, y0[n], x0[n]] +
0198:                     wb[n] * input[n, :, y1[n], x0[n]] +
0199:                     wc[n] * input[n, :, y0[n], x1[n]] +
0200:                     wd[n] * input[n, :, y1[n], x1[n]])
0201:     
0202:     return output
0203: 
0204: def _im2col_dilated(x: np.ndarray, kernel_size: Tuple[int, int],
0205:                     stride: Tuple[int, int], dilation: Tuple[int, int],
0206:                     mode: str = ConvMode.STANDARD,
0207:                     sampling_locations: Optional[np.ndarray] = None) -> np.ndarray:
0208:     """
0209:     Rearranges dilated image blocks into columns with support for different convolution types.
0210:     """
0211:     N, C, H, W = x.shape
0212:     kH, kW = kernel_size
0213:     dH, dW = dilation
0214:     
0215:     # Calculate output size based on mode
0216:     out_h, out_w = _get_output_shape((N, C, H, W), kernel_size, stride, (0, 0), dilation, mode)
0217:     
0218:     # Initialize output array
0219:     cols = np.zeros((C * kH * kW, N * out_h * out_w))
0220:     
0221:     if mode == ConvMode.DEFORMABLE and sampling_locations is not None:
0222:         for n in range(N):
0223:             for h_out in range(out_h):
0224:                 for w_out in range(out_w):
0225:                     # Reshape to (1, kH*kW, 2) for bilinear interpolation
0226:                     loc = sampling_locations[n, h_out*out_w + w_out].reshape(1, -1, 2)
0227:                     
0228:                     # Sample using bilinear interpolation
0229:                     sampled_values = _bilinear_interpolate(
0230:                         x[n:n+1],
0231:                         loc
0232:                     )
0233:                     
0234:                     # Store in cols array
0235:                     idx = n * out_h * out_w + h_out * out_w + w_out
0236:                     cols[:, idx] = sampled_values.reshape(-1)
0237:     else:
0238:         # Standard or transposed convolution
0239:         for h_out in range(out_h):
0240:             for w_out in range(out_w):
0241:                 for c in range(C):
0242:                     for i in range(kH):
0243:                         for j in range(kW):
0244:                             if mode == ConvMode.STANDARD:
0245:                                 h_in = h_out * stride[0] + i * dH
0246:                                 w_in = w_out * stride[1] + j * dW
0247:                             else:  # TRANSPOSED
0248:                                 h_in = h_out * dH + i * stride[0]
0249:                                 w_in = w_out * dW + j * stride[1]
0250:                             
0251:                             col_idx = (c * kH * kW + i * kW + j)
0252:                             row_idx = h_out * out_w + w_out
0253:                             
0254:                             for n in range(N):
0255:                                 if 0 <= h_in < H and 0 <= w_in < W:
0256:                                     cols[col_idx, n * out_h * out_w + row_idx] = x[n, c, h_in, w_in]
0257:                                     
0258:     return cols
0259: 
0260: def _compute_conv_output_shape(input_size: int, kernel_size: int, stride: int,
0261:                              padding: int, dilation: int) -> int:
0262:     """Computes output dimension for a single axis."""
0263:     numerator = input_size + 2 * padding - dilation * (kernel_size - 1) - 1
0264:     return numerator // stride + 1
0265: 
0266: def _compute_conv_grad_input_padding(grad_output_size: int, input_size: int,
0267:                                    kernel_size: int, stride: int, padding: int,
0268:                                    dilation: int) -> Tuple[int, int]:
0269:     """Computes padding needed for gradient computation."""
0270:     grad_input_padding = kernel_size - 1 - padding
0271:     return grad_input_padding
0272: 
0273: def _compute_output_padding(input_size: int, output_size: int, kernel_size: int,
0274:                           stride: int, padding: int, dilation: int) -> int:
0275:     """Computes additional padding needed for transposed convolution."""
0276:     expected_output = (input_size - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + 1
0277:     return output_size - expected_output
0278: 
0279: def _unfold(input_tensor: np.ndarray,
0280:            kernel_size: Tuple[int, ...],
0281:            dilation: Tuple[int, ...],
0282:            padding: Tuple[int, ...],
0283:            stride: Tuple[int, ...]) -> np.ndarray:
0284:     """Extracts sliding local blocks from input tensor."""
0285:     N, C, H, W = input_tensor.shape
0286:     kH, kW = kernel_size
0287:     
0288:     # Apply padding if needed
0289:     if padding[0] > 0 or padding[1] > 0:
0290:         input_tensor = np.pad(input_tensor,
0291:                           ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])),
0292:                           mode='constant')
0293:     
0294:     # Calculate output dimensions
0295:     H_out = ((H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0296:     W_out = ((W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0297:     
0298:     # Initialize output array with correct shape
0299:     output = np.zeros((C * kH * kW, N * H_out * W_out))
0300:     
0301:     # Extract patches
0302:     for h in range(H_out):
0303:         for w in range(W_out):
0304:             for i in range(kH):
0305:                 for j in range(kW):
0306:                     h_start = h * stride[0] + i * dilation[0]
0307:                     w_start = w * stride[1] + j * dilation[1]
0308:                     
0309:                     # Extract patch for all channels and batches
0310:                     patch = input_tensor[:, :, h_start:h_start+1, w_start:w_start+1]
0311:                     
0312:                     # Place in output array
0313:                     row_idx = (i * kW + j) * C + np.arange(C)
0314:                     col_idx = h * W_out + w + np.arange(N) * H_out * W_out
0315:                     output[row_idx[:, None], col_idx] = patch.reshape(N, C).T
0316:     
0317:     return output
0318: 
0319: def _fold(input: np.ndarray,
0320:          output_size: Tuple[int, ...],
0321:          kernel_size: Tuple[int, ...],
0322:          dilation: Tuple[int, ...],
0323:          padding: Tuple[int, ...],
0324:          stride: Tuple[int, ...]) -> np.ndarray:
0325:     """Combines an array of sliding local blocks into a large tensor."""
0326:     H, W = output_size
0327:     kH, kW = kernel_size
0328:     C = input.shape[0] // (kH * kW)
0329:     N = input.shape[1] // ((H + 2 * padding[0] - kH + 1) * (W + 2 * padding[1] - kW + 1))
0330:     
0331:     # Initialize output tensor
0332:     output = np.zeros((N, C, H + 2 * padding[0], W + 2 * padding[1]))
0333:     divisor = np.zeros_like(output)  # For averaging overlapping values
0334:     
0335:     # Calculate output dimensions
0336:     H_out = ((H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0337:     W_out = ((W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0338:     
0339:     # Fold patches back
0340:     for h in range(H_out):
0341:         for w in range(W_out):
0342:             for i in range(kH):
0343:                 for j in range(kW):
0344:                     h_start = h * stride[0] + i * dilation[0]
0345:                     w_start = w * stride[1] + j * dilation[1]
0346:                     
0347:                     row_idx = (i * kW + j) * C + np.arange(C)
0348:                     col_idx = h * W_out + w + np.arange(N) * H_out * W_out
0349:                     
0350:                     patch = input[row_idx[:, None], col_idx].T.reshape(N, C, 1, 1)
0351:                     output[:, :, h_start:h_start+1, w_start:w_start+1] += patch
0352:                     divisor[:, :, h_start:h_start+1, w_start:w_start+1] += 1
0353:     
0354:     # Average overlapping values
0355:     output = np.divide(output, divisor, where=divisor != 0)
0356:     
0357:     # Remove padding if necessary
0358:     if padding[0] > 0 or padding[1] > 0:
0359:         output = output[:, :, padding[0]:-padding[0] if padding[0] > 0 else None,
0360:                        padding[1]:-padding[1] if padding[1] > 0 else None]
0361:     
0362:     return output
0363: 
0364: def _dilate(input: np.ndarray, dilation: Tuple[int, ...]) -> np.ndarray:
0365:     """
0366:     Dilates the input tensor by inserting zeros between elements.
0367:     
0368:     Args:
0369:         input: Input tensor
0370:         dilation: Dilation factors for each dimension
0371:         
0372:     Returns:
0373:         Dilated tensor
0374:     """
0375:     if all(d == 1 for d in dilation):
0376:         return input
0377: 
0378:     N, C, H, W = input.shape
0379:     dH, dW = dilation
0380:     
0381:     H_dilated = H + (H - 1) * (dH - 1)
0382:     W_dilated = W + (W - 1) * (dW - 1)
0383:     
0384:     output = np.zeros((N, C, H_dilated, W_dilated))
0385:     output[:, :, ::dH, ::dW] = input
0386:     
0387:     return output
0388: 
0389: def _bilinear_interpolate(input: np.ndarray,
0390:                        points: np.ndarray,
0391:                        align_corners: bool = True) -> np.ndarray:
0392:     """
0393:     Performs bilinear interpolation on the input tensor at specified points.
0394:     
0395:     Args:
0396:         input: Input tensor (N, C, H, W)
0397:         points: Points to sample (N, P, 2) in normalized coordinates [-1, 1]
0398:         align_corners: Whether to align corners in interpolation
0399:         
0400:     Returns:
0401:         Interpolated values (N, C, P)
0402:     """ 
0403:     N, C, H, W = input.shape
0404:     _, P, _ = points.shape
0405: 
0406:     # Convert normalized coordinates to pixel coordinates
0407:     if align_corners:
0408:         x = (points[..., 0] + 1) * (W - 1) / 2
0409:         y = (points[..., 1] + 1) * (H - 1) / 2
0410:     else:
0411:         x = ((points[..., 0] + 1) * W - 1) / 2
0412:         y = ((points[..., 1] + 1) * H - 1) / 2
0413: 
0414:     # Get corner indices
0415:     x0 = np.floor(x).astype(np.int32)
0416:     x1 = x0 + 1
0417:     y0 = np.floor(y).astype(np.int32)
0418:     y1 = y0 + 1
0419: 
0420:     # Clip to image boundaries
0421:     x0 = np.clip(x0, 0, W - 1)
0422:     x1 = np.clip(x1, 0, W - 1)
0423:     y0 = np.clip(y0, 0, H - 1)
0424:     y1 = np.clip(y1, 0, H - 1)
0425: 
0426:     # Calculate interpolation weights
0427:     wa = (x1 - x) * (y1 - y)
0428:     wb = (x1 - x) * (y - y0)
0429:     wc = (x - x0) * (y1 - y)
0430:     wd = (x - x0) * (y - y0)
0431: 
0432:     # Gather corner values
0433:     Ia = np.zeros((N, C, P))
0434:     Ib = np.zeros((N, C, P))
0435:     Ic = np.zeros((N, C, P))
0436:     Id = np.zeros((N, C, P))
0437: 
0438:     for n in range(N):
0439:         for p in range(P):
0440:             Ia[n, :, p] = input[n, :, y0[n, p], x0[n, p]]
0441:             Ib[n, :, p] = input[n, :, y1[n, p], x0[n, p]]
0442:             Ic[n, :, p] = input[n, :, y0[n, p], x1[n, p]]
0443:             Id[n, :, p] = input[n, :, y1[n, p], x1[n, p]]
0444: 
0445:     # Reshape weights for broadcasting
0446:     wa = wa.reshape(N, 1, P)
0447:     wb = wb.reshape(N, 1, P)
0448:     wc = wc.reshape(N, 1, P)
0449:     wd = wd.reshape(N, 1, P)
0450: 
0451:     # Interpolate
0452:     out = wa * Ia + wb * Ib + wc * Ic + wd * Id
0453:     return out
0454: 
0455: def _bilinear_interpolate_gradient(grad_output: np.ndarray,
0456:                                 points: np.ndarray,
0457:                                 input_size: Tuple[int, ...],
0458:                                 input_tensor: np.ndarray,
0459:                                 align_corners: bool = True) -> Tuple[np.ndarray, np.ndarray]:
0460:     """
0461:     Computes gradients for bilinear interpolation.
0462:     
0463:     Args:
0464:         grad_output: Gradient of loss with respect to interpolated values (can be any shape)
0465:         points: Points that were sampled (N, P, 2) in normalized coordinates [-1, 1]
0466:         input_size: Size of the input tensor (H, W)
0467:         input_tensor: The input tensor being sampled from (N, C, H, W)
0468:         align_corners: Whether corners were aligned in interpolation
0469:         
0470:     Returns:
0471:         Tuple of:
0472:             - grad_input: Gradient with respect to input tensor (N, C, H, W)
0473:             - grad_points: Gradient with respect to sampling points (N, P, 2)
0474:             
0475:     Raises:
0476:         ValueError: If input shapes are incompatible
0477:     """
0478:     # Ensure points is properly shaped first
0479:     if points.ndim == 2:
0480:         points = points.reshape(1, *points.shape)
0481:     
0482:     # Get number of points from points tensor
0483:     N, P, _ = points.shape
0484:     
0485:     # Ensure grad_output is properly shaped (N, C, P) to match points
0486:     if grad_output.ndim == 1:
0487:         grad_output = grad_output.reshape(1, 1, -1)
0488:     elif grad_output.ndim == 2:
0489:         grad_output = grad_output.reshape(1, -1, 1)
0490:         
0491:     # Broadcast grad_output to match number of points if necessary
0492:     if grad_output.shape[2] == 1:
0493:         grad_output = np.broadcast_to(grad_output, (grad_output.shape[0], grad_output.shape[1], P))
0494:     elif grad_output.shape[2] != P:
0495:         raise ValueError(f"Gradient shape {grad_output.shape} cannot be broadcast to number of points {P}")
0496:         
0497:     C = grad_output.shape[1]
0498:     H, W = input_size
0499: 
0500:     # Validate input_tensor shape
0501:     if input_tensor.shape[2:] != input_size:
0502:         raise ValueError(f"Input tensor spatial dimensions {input_tensor.shape[2:]} "
0503:                         f"don't match input_size {input_size}")
0504:     
0505:     # Convert normalized coordinates to pixel coordinates
0506:     if align_corners:
0507:         x = (points[..., 0] + 1) * (W - 1) / 2
0508:         y = (points[..., 1] + 1) * (H - 1) / 2
0509:     else:
0510:         x = ((points[..., 0] + 1) * W - 1) / 2
0511:         y = ((points[..., 1] + 1) * H - 1) / 2
0512:     
0513:     # Get corner indices
0514:     x0 = np.floor(x).astype(np.int32)
0515:     x1 = x0 + 1
0516:     y0 = np.floor(y).astype(np.int32)
0517:     y1 = y0 + 1
0518:     
0519:     # Clip to image boundaries
0520:     x0 = np.clip(x0, 0, W - 1)
0521:     x1 = np.clip(x1, 0, W - 1)
0522:     y0 = np.clip(y0, 0, H - 1)
0523:     y1 = np.clip(y1, 0, H - 1)
0524:     
0525:     # Compute weights for bilinear interpolation
0526:     wa = (x1 - x) * (y1 - y)
0527:     wb = (x1 - x) * (y - y0)
0528:     wc = (x - x0) * (y1 - y)
0529:     wd = (x - x0) * (y - y0)
0530:     
0531:     # Reshape weights for broadcasting with channel dimension
0532:     wa = wa[..., None]  # Shape: (N, P, 1)
0533:     wb = wb[..., None]
0534:     wc = wc[..., None]
0535:     wd = wd[..., None]
0536:     
0537:     # Initialize gradients
0538:     grad_input = np.zeros((N, C, H, W))
0539:     grad_points = np.zeros_like(points)  # Shape: (N, P, 2)
0540:     
0541:     # Compute gradients with respect to input
0542:     for n in range(N):
0543:         for c in range(C):
0544:             grad_chan = grad_output[n, c]  # Shape: (P,)
0545:             for p in range(P):
0546:                 grad = grad_chan[p]
0547:                 grad_input[n, c, y0[n, p], x0[n, p]] += grad * wa[n, p, 0]
0548:                 grad_input[n, c, y1[n, p], x0[n, p]] += grad * wb[n, p, 0]
0549:                 grad_input[n, c, y0[n, p], x1[n, p]] += grad * wc[n, p, 0]
0550:                 grad_input[n, c, y1[n, p], x1[n, p]] += grad * wd[n, p, 0]
0551:     
0552:     # Compute scaling factors for coordinate gradients
0553:     if align_corners:
0554:         dx = (W - 1) / 2
0555:         dy = (H - 1) / 2
0556:     else:
0557:         dx = W / 2
0558:         dy = H / 2
0559:     
0560:     # Compute gradients with respect to sampling points
0561:     for n in range(N):
0562:         for p in range(P):
0563:             grad = grad_output[n, :, p].sum()  # Sum over channels
0564:             
0565:             # Gradient with respect to x
0566:             gx = grad * (
0567:                 (y1[n, p] - y[n, p]) * (
0568:                     input_tensor[n, :, y0[n, p], x1[n, p]] - 
0569:                     input_tensor[n, :, y0[n, p], x0[n, p]]
0570:                 ).sum() +
0571:                 (y[n, p] - y0[n, p]) * (
0572:                     input_tensor[n, :, y1[n, p], x1[n, p]] - 
0573:                     input_tensor[n, :, y1[n, p], x0[n, p]]
0574:                 ).sum()
0575:             ) * dx
0576:             
0577:             # Gradient with respect to y
0578:             gy = grad * (
0579:                 (x1[n, p] - x[n, p]) * (
0580:                     input_tensor[n, :, y1[n, p], x0[n, p]] - 
0581:                     input_tensor[n, :, y0[n, p], x0[n, p]]
0582:                 ).sum() +
0583:                 (x[n, p] - x0[n, p]) * (
0584:                     input_tensor[n, :, y1[n, p], x1[n, p]] - 
0585:                     input_tensor[n, :, y0[n, p], x1[n, p]]
0586:                 ).sum()
0587:             ) * dy
0588:             
0589:             grad_points[n, p] = [gx, gy]
0590:     
0591:     return grad_input, grad_points
0592: 
0593: def _generate_grid(batch_size: int, height: int, width: int,
0594:                  align_corners: bool = True) -> np.ndarray:
0595:     """
0596:     Generates a coordinate grid for grid sampling.
0597:     
0598:     Args:
0599:         batch_size: Number of samples in batch
0600:         height: Height of the grid
0601:         width: Width of the grid
0602:         align_corners: Whether to align corners
0603:         
0604:     Returns:
0605:         Grid tensor of shape (N, H, W, 2) with normalized coordinates
0606:     """
0607:     if align_corners:
0608:         x = np.linspace(-1, 1, width)
0609:         y = np.linspace(-1, 1, height)
0610:     else:
0611:         x = np.linspace(-1 + (1/width), 1 - (1/width), width)
0612:         y = np.linspace(-1 + (1/height), 1 - (1/height), height)
0613: 
0614:     x_coords, y_coords = np.meshgrid(x, y)
0615:     grid = np.stack([x_coords, y_coords], axis=-1)
0616:     grid = np.tile(grid[None], (batch_size, 1, 1, 1))
0617:     
0618:     return grid
0619: 
0620: def _deform_grid(grid: np.ndarray, offset: np.ndarray) -> np.ndarray:
0621:     """
0622:     Deforms a regular grid using offset values.
0623:     
0624:     Args:
0625:         grid: Regular coordinate grid (N, H, W, 2)
0626:         offset: Offset values for deformation (N, 2, H, W)
0627:         
0628:     Returns:
0629:         Deformed grid (N, H, W, 2)
0630:     """
0631:     N, H, W, _ = grid.shape
0632:     
0633:     # Reshape offset to match grid shape
0634:     offset = offset.transpose(0, 2, 3, 1)
0635:     
0636:     # Add offset to grid
0637:     deformed_grid = grid + offset
0638:     
0639:     # Clamp values to [-1, 1] to ensure valid sampling
0640:     return np.clip(deformed_grid, -1, 1)
0641: 
0642: def _modulated_deform_grid(grid: np.ndarray, offset: np.ndarray, 
0643:                         mask: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
0644:     """
0645:     Deforms a regular grid using offset values and modulation mask.
0646:     Used in Deformable ConvNets v2.
0647:     
0648:     Args:
0649:         grid: Regular coordinate grid (N, H, W, 2)
0650:         offset: Offset values for deformation (N, 2, H, W)
0651:         mask: Modulation mask (N, 1, H, W)
0652:         
0653:     Returns:
0654:         Tuple of deformed grid and modulation mask
0655:     """
0656:     # Deform grid
0657:     deformed_grid = _deform_grid(grid, offset)
0658:     
0659:     # Reshape mask to match sampling points
0660:     mask = mask.transpose(0, 2, 3, 1)
0661:     
0662:     return deformed_grid, mask
0663: 
0664: def _compute_indices_weights(points: np.ndarray, size: Tuple[int, int]) -> Tuple[np.ndarray, ...]:
0665:     """
0666:     Computes indices and weights for bilinear interpolation.
0667:     
0668:     Args:
0669:         points: Sampling points (N, H, W, 2)
0670:         size: Size of the input feature map (H, W)
0671:         
0672:     Returns:
0673:         Tuple of indices and weights for bilinear interpolation
0674:     """
0675:     H, W = size
0676:     
0677:     # Convert points to pixel coordinates
0678:     x = (points[..., 0] + 1) * (W - 1) / 2
0679:     y = (points[..., 1] + 1) * (H - 1) / 2
0680:     
0681:     # Get corner indices
0682:     x0 = np.floor(x).astype(np.int32)
0683:     x1 = x0 + 1
0684:     y0 = np.floor(y).astype(np.int32)
0685:     y1 = y0 + 1
0686:     
0687:     # Clip to image boundaries
0688:     x0 = np.clip(x0, 0, W - 1)
0689:     x1 = np.clip(x1, 0, W - 1)
0690:     y0 = np.clip(y0, 0, H - 1)
0691:     y1 = np.clip(y1, 0, H - 1)
0692:     
0693:     # Compute weights
0694:     wa = (x1 - x) * (y1 - y)
0695:     wb = (x1 - x) * (y - y0)
0696:     wc = (x - x0) * (y1 - y)
0697:     wd = (x - x0) * (y - y0)
0698:     
0699:     return (x0, x1, y0, y1), (wa, wb, wc, wd)
0700: 
0701: def _apply_deform_conv(input: np.ndarray, weight: np.ndarray, offset: np.ndarray,
0702:                     stride: Tuple[int, int], padding: Tuple[int, int],
0703:                     dilation: Tuple[int, int], mask: Optional[np.ndarray] = None) -> np.ndarray:
0704:     """
0705:     Applies deformable convolution operation.
0706:     
0707:     Args:
0708:         input: Input feature map (N, C_in, H, W)
0709:         weight: Convolution weights (C_out, C_in, kH, kW)
0710:         offset: Sampling offsets (N, 2*kH*kW, H_out, W_out)
0711:         stride: Convolution stride
0712:         padding: Zero-padding size
0713:         dilation: Dilation rate
0714:         mask: Optional modulation mask for v2 (N, kH*kW, H_out, W_out)
0715:         
0716:     Returns:
0717:         Output feature map (N, C_out, H_out, W_out)
0718:     """
0719:     N, C_in, H, W = input.shape
0720:     C_out, _, kH, kW = weight.shape
0721:     H_out = (H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0] + 1
0722:     W_out = (W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1] + 1
0723:     
0724:     # Generate sampling grid
0725:     grid = _generate_grid(N, H_out, W_out)
0726:     
0727:     # Deform grid using offsets
0728:     if mask is not None:
0729:         deformed_grid, modulation = _modulated_deform_grid(grid, offset, mask)
0730:     else:
0731:         deformed_grid = _deform_grid(grid, offset)
0732:         modulation = None
0733:     
0734:     # Get sampling indices and weights
0735:     indices, weights = _compute_indices_weights(deformed_grid, (H, W))
0736:     x0, x1, y0, y1 = indices
0737:     wa, wb, wc, wd = weights
0738:     
0739:     # Initialize output
0740:     output = np.zeros((N, C_out, H_out, W_out))
0741:     
0742:     # Apply convolution with deformed sampling
0743:     for i in range(kH):
0744:         for j in range(kW):
0745:             # Get values from input feature map
0746:             values = (wa[..., None] * input[:, :, y0, x0] +
0747:                      wb[..., None] * input[:, :, y1, x0] +
0748:                      wc[..., None] * input[:, :, y0, x1] +
0749:                      wd[..., None] * input[:, :, y1, x1])
0750:                      
0751:             # Apply modulation if available
0752:             if modulation is not None:
0753:                 values = values * modulation[:, i*kW + j, ..., None]
0754:                 
0755:             # Accumulate weighted values
0756:             for cout in range(C_out):
0757:                 output[:, cout] += np.sum(values * weight[cout, :, i, j], axis=1)
0758:     
0759:     return output
0760:         
0761: class Conv2dFunction(Function):
0762:     """
0763:     Implements 2D convolution with support for:
0764:     - Asymmetric kernels
0765:     - Different strides for height and width
0766:     - Different padding for height and width
0767:     - Different dilation rates for height and width
0768:     - Transposed convolution
0769:     - Deformable convolution
0770:     - Grouped convolution
0771:     """
0772:     @staticmethod
0773:     def forward(ctx: Context, x: Tensor, weight: Tensor, bias: Optional[Tensor] = None,
0774:             stride: Tuple[int, int] = (1, 1), padding: Tuple[int, int] = (0, 0),
0775:             dilation: Tuple[int, int] = (1, 1), groups: int = 1,
0776:             mode: str = ConvMode.STANDARD, offset: Optional[Tensor] = None,
0777:             mask: Optional[Tensor] = None) -> Tensor:
0778:         """Forward pass of flexible 2D convolution."""
0779:         # Validate parameters
0780:         _validate_conv_params(x.shape, weight.shape, stride, padding, dilation, groups, 
0781:                             mode, offset, weight, mask)
0782:         
0783:         # Get required offset tensor and compute sampling locations for deformable conv
0784:         if mode == ConvMode.DEFORMABLE:
0785:             offset_tensor = offset if offset is not None else getattr(weight, 'offset', None)
0786:             sampling_locations = _get_deformable_offsets(
0787:                 offset_tensor.data,
0788:                 weight.shape[2:],
0789:                 x.shape
0790:             )
0791:             # Ensure sampling_locations has correct shape for _im2col_dilated
0792:             N, HW, kHW, _ = sampling_locations.shape
0793:             H_out = W_out = int(np.sqrt(HW))  # Assuming square output
0794:             sampling_locations = sampling_locations.reshape(N, H_out * W_out, kHW, 2)
0795:         else:
0796:             sampling_locations = None
0797:             offset_tensor = None  # Explicitly set to None if not deformable
0798:         
0799:         # Save tensors and info for backward pass
0800:         if mode == ConvMode.DEFORMABLE:
0801:             ctx.save_for_backward(x, weight, bias, offset_tensor)
0802:         else:
0803:             ctx.save_for_backward(x, weight, bias)
0804:             
0805:         ctx.save_arguments(stride=stride, padding=padding, dilation=dilation, 
0806:                         groups=groups, mode=mode, sampling_locations=sampling_locations)
0807:         
0808:         # Process each group
0809:         x_padded = _pad_input(x.data, padding)
0810:         C_in_per_group = x.shape[1] // groups
0811:         C_out_per_group = weight.shape[0] // groups
0812:         H_out, W_out = _get_output_shape(x.shape, weight.shape[2:], stride, padding, dilation, mode)
0813:         output = np.zeros((x.shape[0], weight.shape[0], H_out, W_out))
0814:         
0815:         for g in range(groups):
0816:             x_g = x_padded[:, g*C_in_per_group:(g+1)*C_in_per_group]
0817:             w_g = weight.data[g*C_out_per_group:(g+1)*C_out_per_group]
0818:             
0819:             # Convert input patches to columns
0820:             x_cols = _im2col_dilated(x_g, weight.shape[2:], stride, dilation, mode, 
0821:                                 sampling_locations)
0822:             
0823:             # Reshape weight for matrix multiplication
0824:             w_reshaped = w_g.reshape(C_out_per_group, -1)
0825:             
0826:             # Perform convolution
0827:             out = w_reshaped @ x_cols
0828:             
0829:             # Reshape output
0830:             output[:, g*C_out_per_group:(g+1)*C_out_per_group] = out.reshape(
0831:                 C_out_per_group, x.shape[0], *output.shape[2:]
0832:             ).transpose(1, 0, 2, 3)
0833:         
0834:         # Add bias if present
0835:         if bias is not None:
0836:             output += bias.data.reshape(1, -1, 1, 1)
0837:             
0838:         return Tensor(output)
0839:         
0840:     @staticmethod
0841:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0842:         """
0843:         Backward pass of 2D convolution.
0844:         
0845:         Computes gradients for:
0846:         - Input tensor (x)
0847:         - Weight tensor (w)
0848:         - Bias (if present)
0849:         - Offsets (for deformable convolution)
0850:         
0851:         Handles different convolution modes:
0852:         - Standard convolution
0853:         - Transposed convolution
0854:         - Deformable convolution
0855:         """
0856:         # Retrieve saved tensors and arguments
0857:         saved_tensors = ctx.saved_tensors
0858:         num_saved = len(saved_tensors)
0859: 
0860:         # Initialize mask_tensor to None
0861:         mask_tensor = None
0862: 
0863:         if num_saved == 4:  # Deformable case without mask
0864:             x, weight, bias, offset_tensor = saved_tensors
0865:         elif num_saved == 5:  # Deformable case with mask (if supported)
0866:             x, weight, bias, offset_tensor, mask_tensor = saved_tensors
0867:         else:  # Standard or other modes
0868:             x, weight, bias = saved_tensors
0869:             offset_tensor = None
0870: 
0871:         # Retrieve saved arguments
0872:         stride = ctx.saved_arguments['stride']
0873:         padding = ctx.saved_arguments['padding']
0874:         dilation = ctx.saved_arguments['dilation']
0875:         groups = ctx.saved_arguments['groups']
0876:         mode = ctx.saved_arguments['mode']
0877:         sampling_locations = ctx.saved_arguments.get('sampling_locations', None)
0878: 
0879:         # Retrieve mask if present
0880:         mask = ctx.saved_arguments.get('mask', None)
0881:         if mask_tensor is not None:
0882:             mask = mask_tensor.data
0883: 
0884:         # Determine dimensions
0885:         N, C_in, H, W = x.shape
0886:         C_out, _, kH, kW = weight.shape
0887:         C_in_per_group = C_in // groups
0888:         C_out_per_group = C_out // groups
0889:         H_out, W_out = _get_output_shape(x.shape, weight.shape[2:], stride, padding, dilation, mode)
0890: 
0891:         # Apply padding based on convolution mode
0892:         if mode in [ConvMode.STANDARD, ConvMode.DEFORMABLE]:
0893:             x_padded = _pad_input(x.data, padding)
0894:         elif mode == ConvMode.TRANSPOSED:
0895:             grad_output_padded = _pad_input(grad_output, padding)
0896: 
0897:         # Initialize gradients if required
0898:         if x.requires_grad:
0899:             if mode in [ConvMode.STANDARD, ConvMode.DEFORMABLE]:
0900:                 grad_x_padded = np.zeros_like(x_padded)
0901:             elif mode == ConvMode.TRANSPOSED:
0902:                 grad_x = np.zeros_like(x.data)
0903:         if weight.requires_grad:
0904:             grad_weight = np.zeros_like(weight.data)
0905:         if bias is not None and bias.requires_grad:
0906:             grad_bias = np.zeros_like(bias.data)
0907:         if mode == ConvMode.DEFORMABLE and offset_tensor is not None and offset_tensor.requires_grad:
0908:             grad_offset = np.zeros_like(offset_tensor.data)
0909:         if mask is not None and mode == ConvMode.DEFORMABLE and mask_tensor is not None and mask_tensor.requires_grad:
0910:             grad_mask = np.zeros_like(mask_tensor.data)
0911:         else:
0912:             grad_mask = None  # Initialize grad_mask to None if mask is not provided
0913: 
0914:         # Delegate backward computation based on mode
0915:         if mode == ConvMode.STANDARD:
0916:             Conv2dFunction._backward_standard(
0917:                 grad_output, grad_x_padded, grad_weight, grad_bias,
0918:                 x_padded, weight, bias, stride, padding, dilation, groups,
0919:                 N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
0920:             )
0921:         elif mode == ConvMode.TRANSPOSED:
0922:             Conv2dFunction._backward_transposed(
0923:                 grad_output_padded, grad_x, grad_weight, grad_bias,
0924:                 x, weight, bias, stride, padding, dilation, groups,
0925:                 N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
0926:             )
0927:         elif mode == ConvMode.DEFORMABLE:
0928:             Conv2dFunction._backward_deformable(
0929:                 grad_output, grad_x_padded, grad_weight, grad_bias, grad_offset, grad_mask,
0930:                 x_padded, weight, bias, offset_tensor, mask, stride, padding, dilation, groups, sampling_locations,
0931:                 N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
0932:             )
0933: 
0934:         # Assign gradients to grad_dict
0935:         if x.requires_grad:
0936:             if mode in [ConvMode.STANDARD, ConvMode.DEFORMABLE]:
0937:                 if mode == ConvMode.STANDARD:
0938:                     # Remove padding from grad_x_padded if necessary
0939:                     if padding[0] > 0 or padding[1] > 0:
0940:                         grad_x = grad_x_padded[:, :,
0941:                                             padding[0]:grad_x_padded.shape[2]-padding[0],
0942:                                             padding[1]:grad_x_padded.shape[3]-padding[1]]
0943:                     else:
0944:                         grad_x = grad_x_padded
0945:                 elif mode == ConvMode.DEFORMABLE:
0946:                     # Directly assign grad_x_padded to grad_x for deformable convolution
0947:                     grad_x = grad_x_padded
0948:                 grad_dict[id(x)] = grad_x
0949:             elif mode == ConvMode.TRANSPOSED:
0950:                 grad_dict[id(x)] = grad_x
0951: 
0952:         if weight.requires_grad:
0953:             grad_dict[id(weight)] = grad_weight
0954: 
0955:         if bias is not None and bias.requires_grad:
0956:             grad_dict[id(bias)] = grad_bias
0957: 
0958:         if mode == ConvMode.DEFORMABLE and offset_tensor is not None and offset_tensor.requires_grad:
0959:             grad_dict[id(offset_tensor)] = grad_offset
0960: 
0961:         if mask is not None and mode == ConvMode.DEFORMABLE and mask_tensor is not None and mask_tensor.requires_grad:
0962:             grad_dict[id(mask_tensor)] = grad_mask
0963: 
0964:     @staticmethod
0965:     def _backward_standard(
0966:         grad_output, grad_x_padded, grad_weight, grad_bias,
0967:         x_padded, weight, bias, stride, padding, dilation, groups,
0968:         N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
0969:     ):
0970:         """
0971:         Backward pass for standard convolution.
0972:         """
0973:         for g in range(groups):
0974:             # Slice weights and grad_output for the current group
0975:             w_g = weight.data[g * C_out_per_group:(g + 1) * C_out_per_group]
0976:             grad_out_g = grad_output[:, g * C_out_per_group:(g + 1) * C_out_per_group]
0977: 
0978:             # Reshape weights and grad_output for matrix multiplication
0979:             w_reshaped = w_g.reshape(C_out_per_group, -1).T  # Shape: (C_in_per_group * kH * kW, C_out_per_group)
0980:             grad_out_reshaped = grad_out_g.reshape(N, C_out_per_group, -1).transpose(1, 0, 2).reshape(C_out_per_group, -1)  # Shape: (C_out_per_group, N * H_out * W_out)
0981: 
0982:             # Compute gradient columns
0983:             grad_cols = w_reshaped @ grad_out_reshaped  # Shape: (C_in_per_group * kH * kW, N * H_out * W_out)
0984: 
0985:             # Convert columns back to image
0986:             grad_x_g = _col2im_dilated(
0987:                 grad_cols,
0988:                 x_padded[:, g * C_in_per_group:(g + 1) * C_in_per_group].shape,
0989:                 weight.shape[2:],
0990:                 stride,
0991:                 dilation
0992:             )
0993:             grad_x_padded[:, g * C_in_per_group:(g + 1) * C_in_per_group] += grad_x_g
0994: 
0995:             # Compute gradient for weights
0996:             x_cols = _im2col_dilated(
0997:                 x_padded[:, g * C_in_per_group:(g + 1) * C_in_per_group],
0998:                 weight.shape[2:], stride, dilation, ConvMode.STANDARD, sampling_locations=None
0999:             )
1000:             for n in range(N):
1001:                 grad_out_n = grad_out_g[n].reshape(C_out_per_group, -1)  # Shape: (C_out_per_group, H_out * W_out)
1002:                 grad_weight[g * C_out_per_group:(g + 1) * C_out_per_group] += \
1003:                     (grad_out_n @ x_cols[:, n * H_out * W_out:(n + 1) * H_out * W_out].T).reshape(
1004:                         C_out_per_group, C_in_per_group, kH, kW
1005:                     )
1006: 
1007:         # Compute gradient for bias
1008:         if bias is not None:
1009:             grad_bias += grad_output.sum(axis=(0, 2, 3))
1010: 
1011:     @staticmethod
1012:     def _backward_transposed(
1013:         grad_output_padded, grad_x, grad_weight, grad_bias,
1014:         x, weight, bias, stride, padding, dilation, groups,
1015:         N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
1016:     ):
1017:         """
1018:         Backward pass for transposed convolution.
1019:         """
1020:         for g in range(groups):
1021:             # Slice weights and grad_output for the current group
1022:             w_g = weight.data[g * C_out_per_group:(g + 1) * C_out_per_group]
1023:             grad_out_g = grad_output_padded[:, g * C_out_per_group:(g + 1) * C_out_per_group]
1024: 
1025:             # Reshape weights for gradient computation
1026:             w_flipped = np.flip(np.flip(w_g, 2), 3).transpose(1, 0, 2, 3)  # Flip kernel
1027:             w_reshaped = w_flipped.reshape(-1, C_out_per_group)  # Shape: (C_in_per_group * kH * kW, C_out_per_group)
1028: 
1029:             # Compute gradient columns with swapped stride and dilation
1030:             grad_cols = _im2col_dilated(
1031:                 grad_out_g,
1032:                 weight.shape[2:],
1033:                 dilation,  # Use dilation as stride
1034:                 stride,    # Use stride as dilation
1035:                 ConvMode.STANDARD
1036:             )
1037: 
1038:             # Compute gradient for input
1039:             grad_x[:, g * C_in_per_group:(g + 1) * C_in_per_group] += \
1040:                 (w_reshaped @ grad_cols).reshape(N, C_in_per_group, *x.shape[2:])
1041: 
1042:             # Compute gradient for weights
1043:             x_original = x.data[:, g * C_in_per_group:(g + 1) * C_in_per_group]
1044:             x_cols = _im2col_dilated(
1045:                 x_original,
1046:                 weight.shape[2:], stride, dilation, ConvMode.STANDARD, sampling_locations=None
1047:             )
1048:             for n in range(N):
1049:                 grad_out_n = grad_out_g[n].reshape(C_out_per_group, -1)  # Shape: (C_out_per_group, H_out * W_out)
1050:                 grad_weight[g * C_out_per_group:(g + 1) * C_out_per_group] += \
1051:                     (grad_out_n @ x_cols[:, n * H_out * W_out:(n + 1) * H_out * W_out].T).reshape(
1052:                         C_out_per_group, C_in_per_group, kH, kW
1053:                     )
1054: 
1055:         # Compute gradient for bias
1056:         if bias is not None:
1057:             grad_bias += grad_output_padded.sum(axis=(0, 2, 3))
1058: 
1059:     @staticmethod
1060:     def _backward_deformable(
1061:         grad_output, grad_x_padded, grad_weight, grad_bias, grad_offset, grad_mask,
1062:         x_padded, weight, bias, offset_tensor, mask, stride, padding, dilation, groups, sampling_locations,
1063:         N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
1064:     ):
1065:         """
1066:         Backward pass for deformable convolution.
1067:         """
1068:         for g in range(groups):
1069:             # Slice weights and grad_output for the current group
1070:             w_g = weight.data[g * C_out_per_group:(g + 1) * C_out_per_group]  # Shape: (C_out_per_group, C_in_per_group, kH, kW)
1071:             grad_out_g = grad_output[:, g * C_out_per_group:(g + 1) * C_out_per_group]  # Shape: (N, C_out_per_group, H_out, W_out)
1072: 
1073:             for n in range(N):
1074:                 for h in range(H_out):
1075:                     for w_ in range(W_out):
1076:                         # Flat index for the current spatial location
1077:                         i = h * W_out + w_
1078: 
1079:                         # Extract grad_out for current (n, g, h, w_)
1080:                         grad_out_slice = grad_out_g[n, :, h, w_]  # Shape: (C_out_per_group,)
1081: 
1082:                         for c_out in range(C_out_per_group):
1083:                             # Get the gradient for the current output channel
1084:                             grad = grad_out_slice[c_out]  # Scalar
1085: 
1086:                             for c_in in range(C_in_per_group):
1087:                                 for kh in range(kH):
1088:                                     for kw in range(kW):
1089:                                         # Index for the kernel position
1090:                                         k_idx = kh * kW + kw
1091: 
1092:                                         # Get the corresponding sampling location
1093:                                         loc = sampling_locations[n, i, k_idx, :]  # Shape: (2,)
1094:                                         y, x = loc  # y and x are the sampling locations in input space
1095: 
1096:                                         # Compute integer coordinates
1097:                                         y0 = int(np.floor(y))
1098:                                         x0 = int(np.floor(x))
1099:                                         y1 = y0 + 1
1100:                                         x1 = x0 + 1
1101: 
1102:                                         # Compute fractional parts
1103:                                         dy = y - y0
1104:                                         dx = x - x0
1105: 
1106:                                         # Compute interpolation weights
1107:                                         wa = (1 - dy) * (1 - dx)
1108:                                         wb = dy * (1 - dx)
1109:                                         wc = (1 - dy) * dx
1110:                                         wd = dy * dx
1111: 
1112:                                         # Compute input slice with batch and channel dimensions
1113:                                         input_slice = x_padded[n:n+1, g * C_in_per_group + c_in : g * C_in_per_group + c_in + 1, :, :]  # Shape: (1, 1, H, W)
1114: 
1115:                                         # Reshape loc to (N=1, P=1, 2)
1116:                                         points = loc.reshape(1, 1, 2)  # Shape: (1, 1, 2)
1117: 
1118:                                         # Perform bilinear interpolation
1119:                                         interpolated = _bilinear_interpolate(input_slice, points, align_corners=True)  # Shape: (1, 1, 1)
1120:                                         interpolated_sum = interpolated.sum()  # Scalar
1121: 
1122:                                         # Update grad_weight
1123:                                         grad_weight[g * C_out_per_group + c_out, c_in, kh, kw] += grad * interpolated_sum
1124: 
1125:                                         # Update grad_x_padded with interpolation weights
1126:                                         # Ensure that the indices are within bounds
1127:                                         if 0 <= y0 < x_padded.shape[2] and 0 <= x0 < x_padded.shape[3]:
1128:                                             grad_x_padded[n, g * C_in_per_group + c_in, y0, x0] += grad * w_g[c_out, c_in, kh, kw] * wa
1129:                                         if 0 <= y1 < x_padded.shape[2] and 0 <= x0 < x_padded.shape[3]:
1130:                                             grad_x_padded[n, g * C_in_per_group + c_in, y1, x0] += grad * w_g[c_out, c_in, kh, kw] * wb
1131:                                         if 0 <= y0 < x_padded.shape[2] and 0 <= x1 < x_padded.shape[3]:
1132:                                             grad_x_padded[n, g * C_in_per_group + c_in, y0, x1] += grad * w_g[c_out, c_in, kh, kw] * wc
1133:                                         if 0 <= y1 < x_padded.shape[2] and 0 <= x1 < x_padded.shape[3]:
1134:                                             grad_x_padded[n, g * C_in_per_group + c_in, y1, x1] += grad * w_g[c_out, c_in, kh, kw] * wd
1135: 
1136:                                         # Compute gradients w.r.t. offset if applicable
1137:                                         if offset_tensor.requires_grad:
1138:                                             # Placeholder for actual gradient computation w.r.t. offset
1139:                                             # This requires computing derivatives based on how offsets affect sampling locations
1140:                                             # For simplicity, this implementation does not compute these gradients
1141:                                             pass  # Replace with actual gradient computation for offsets
1142: 
1143:                                         # Compute gradients w.r.t. mask if applicable
1144:                                         if mask is not None and grad_mask is not None:
1145:                                             # Assuming mask modulates the convolution, compute its gradient
1146:                                             # The gradient is grad_output * weight * sampled_input
1147:                                             # First, perform bilinear interpolation to get sampled input
1148:                                             sampled_input = _bilinear_interpolate(input_slice, points, align_corners=True).squeeze(0).squeeze(0)  # Shape: (1,)
1149:                                             
1150:                                             # Compute grad_mask
1151:                                             grad_mask[g * kH * kW + kh * kW + kw, n, h, w_] += grad * w_g[c_out, c_in, kh, kw] * sampled_input
1152: 
1153:         # After accumulating all gradients, compute grad_bias if needed
1154:         if bias is not None and bias.requires_grad:
1155:             grad_bias += grad_output.sum(axis=(0, 2, 3))  # Sum over N, H_out, W_out

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\elementwise.py
// ----------------------------------------
0001: from typing import Dict
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class Log(Function):
0006:     """
0007:     Natural logarithm operation.
0008:     
0009:     Forward: f(x) = ln(x)
0010:     Backward: f'(x) = 1/x
0011:     """
0012:     @staticmethod
0013:     def forward(ctx, x):
0014:         if not isinstance(x, Tensor):
0015:             x = Tensor(x)
0016:             
0017:         # Check for negative values
0018:         if np.any(x.data <= 0):
0019:             raise ValueError("Log of negative numbers or zero is undefined")
0020:             
0021:         ctx.save_for_backward(x)
0022:         return Tensor(np.log(x.data))
0023:         
0024:     @staticmethod
0025:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0026:         x, = ctx.saved_tensors
0027:         if x.requires_grad:
0028:             # d/dx(log(x)) = 1/x
0029:             grad_dict[id(x)] = grad_output / x.data
0030: 
0031: class Exp(Function):
0032:     """
0033:     Exponential operation.
0034:     
0035:     Forward: f(x) = exp(x)
0036:     Backward: f'(x) = exp(x)
0037:     """
0038:     @staticmethod
0039:     def forward(ctx, x):
0040:         if not isinstance(x, Tensor):
0041:             x = Tensor(x)
0042:             
0043:         result = np.exp(x.data)
0044:         ctx.save_for_backward(x)  # Save x for backward pass
0045:         ctx.save_arguments(exp_x=result)  # Save exp(x) as argument
0046:         return Tensor(result)
0047:         
0048:     @staticmethod
0049:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0050:         x, = ctx.saved_tensors
0051:         exp_x = ctx.saved_arguments['exp_x']
0052:         
0053:         if x.requires_grad:
0054:             # d/dx(exp(x)) = exp(x)
0055:             grad_dict[id(x)] = grad_output * exp_x

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\loss.py
// ----------------------------------------
0001: from typing import Dict, Optional
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class MSELoss(Function):
0006:     """
0007:     Mean Squared Error Loss: L = 1/N * Σ(y - ŷ)²
0008:     
0009:     Args:
0010:         reduction (str): Specifies the reduction to apply to the output:
0011:             'mean' (default) | 'sum' | 'none'
0012:     """
0013:     
0014:     @staticmethod
0015:     def forward(ctx, predictions, targets, reduction='mean'):
0016:         if not isinstance(predictions, Tensor):
0017:             predictions = Tensor(predictions)
0018:         if not isinstance(targets, Tensor):
0019:             targets = Tensor(targets)
0020:             
0021:         if predictions.shape != targets.shape:
0022:             raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
0023:             
0024:         diff = predictions.data - targets.data
0025:         squared_diff = diff * diff
0026:         
0027:         if reduction == 'none':
0028:             result = squared_diff
0029:         elif reduction == 'sum':
0030:             result = np.sum(squared_diff)
0031:         elif reduction == 'mean':
0032:             result = np.mean(squared_diff)
0033:         else:
0034:             raise ValueError(f"Invalid reduction method: {reduction}")
0035:             
0036:         ctx.save_for_backward(predictions, targets)
0037:         ctx.save_arguments(reduction=reduction)
0038:         return Tensor(result)
0039:         
0040:     @staticmethod
0041:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0042:         predictions, targets = ctx.saved_tensors
0043:         reduction = ctx.saved_arguments['reduction']
0044:         
0045:         diff = predictions.data - targets.data
0046:         
0047:         if reduction == 'mean':
0048:             grad = grad_output * 2 * diff / np.prod(diff.shape)
0049:         elif reduction == 'sum':
0050:             grad = grad_output * 2 * diff
0051:         else:  # 'none'
0052:             grad = grad_output * 2 * diff
0053:             
0054:         if predictions.requires_grad:
0055:             grad_dict[id(predictions)] = grad
0056: 
0057: class CrossEntropyLoss(Function):
0058:     """
0059:     Cross Entropy Loss with built-in LogSoftmax: L = -Σ y_true * log(softmax(y_pred))
0060:     
0061:     Args:
0062:         reduction (str): Specifies the reduction to apply to the output:
0063:             'mean' (default) | 'sum' | 'none'
0064:     """
0065:     
0066:     @staticmethod
0067:     def _log_softmax(x):
0068:         # Compute log(softmax(x)) in a numerically stable way
0069:         max_x = np.max(x, axis=1, keepdims=True)
0070:         exp_x = np.exp(x - max_x)
0071:         sum_exp_x = np.sum(exp_x, axis=1, keepdims=True)
0072:         return (x - max_x) - np.log(sum_exp_x)
0073:         
0074:     @staticmethod
0075:     def forward(ctx, predictions, targets, reduction='mean'):
0076:         if not isinstance(predictions, Tensor):
0077:             predictions = Tensor(predictions)
0078:         if not isinstance(targets, Tensor):
0079:             targets = Tensor(targets)
0080:             
0081:         log_softmax = CrossEntropyLoss._log_softmax(predictions.data)
0082:         nll_loss = -np.sum(targets.data * log_softmax, axis=1)
0083:         
0084:         if reduction == 'none':
0085:             result = nll_loss
0086:         elif reduction == 'sum':
0087:             result = np.sum(nll_loss)
0088:         elif reduction == 'mean':
0089:             result = np.mean(nll_loss)
0090:         else:
0091:             raise ValueError(f"Invalid reduction method: {reduction}")
0092:             
0093:         ctx.save_for_backward(predictions, targets)
0094:         ctx.save_arguments(reduction=reduction, log_softmax=log_softmax)
0095:         return Tensor(result)
0096:         
0097:     @staticmethod
0098:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0099:         predictions, targets = ctx.saved_tensors
0100:         reduction = ctx.saved_arguments['reduction']
0101:         log_softmax = ctx.saved_arguments['log_softmax']
0102:         
0103:         grad_output = np.array(grad_output)
0104:         if reduction == 'mean':
0105:             grad_output = grad_output / len(targets.data)
0106:         
0107:         softmax = np.exp(log_softmax)
0108:         grad = grad_output.reshape(-1, 1) * (softmax - targets.data)
0109:             
0110:         if predictions.requires_grad:
0111:             grad_dict[id(predictions)] = grad
0112: 
0113: class BinaryCrossEntropyLoss(Function):
0114:     """
0115:     Binary Cross Entropy Loss: L = -Σ (y * log(p) + (1-y) * log(1-p))
0116:     
0117:     Args:
0118:         reduction (str): Specifies the reduction to apply to the output:
0119:             'mean' (default) | 'sum' | 'none'
0120:         eps (float): Small value for numerical stability
0121:     """
0122:     
0123:     @staticmethod
0124:     def forward(ctx, predictions, targets, reduction='mean', eps=1e-7):
0125:         if not isinstance(predictions, Tensor):
0126:             predictions = Tensor(predictions)
0127:         if not isinstance(targets, Tensor):
0128:             targets = Tensor(targets)
0129: 
0130:         # Check valid probability values
0131:         if np.any(predictions.data < 0) or np.any(predictions.data > 1):
0132:             raise ValueError("Predictions must be in range [0, 1]")
0133:             
0134:         # Clip predictions to prevent log(0)
0135:         predictions_clipped = np.clip(predictions.data, eps, 1 - eps)
0136:         
0137:         loss = -(targets.data * np.log(predictions_clipped) + 
0138:                 (1 - targets.data) * np.log(1 - predictions_clipped))
0139:                 
0140:         if reduction == 'none':
0141:             result = loss
0142:         elif reduction == 'sum':
0143:             result = float(np.sum(loss))  # Convert to scalar
0144:         elif reduction == 'mean':
0145:             result = float(np.mean(loss))  # Convert to scalar
0146:         else:
0147:             raise ValueError(f"Invalid reduction method: {reduction}")
0148:             
0149:         ctx.save_for_backward(predictions, targets)
0150:         ctx.save_arguments(reduction=reduction, eps=eps)
0151:         return Tensor(result)
0152:         
0153:     @staticmethod
0154:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0155:         predictions, targets = ctx.saved_tensors
0156:         reduction = ctx.saved_arguments['reduction']
0157:         eps = ctx.saved_arguments['eps']
0158:         
0159:         predictions_clipped = np.clip(predictions.data, eps, 1 - eps)
0160:         
0161:         grad = grad_output * (predictions_clipped - targets.data) / (
0162:             predictions_clipped * (1 - predictions_clipped))
0163:             
0164:         if reduction == 'mean':
0165:             grad = grad / np.prod(targets.shape)
0166:             
0167:         if predictions.requires_grad:
0168:             grad_dict[id(predictions)] = grad
0169: 
0170: class L1Loss(Function):
0171:     """
0172:     L1 Loss (Mean Absolute Error): L = |y - ŷ|
0173:     
0174:     Args:
0175:         reduction (str): Specifies the reduction to apply to the output:
0176:             'mean' (default) | 'sum' | 'none'
0177:     """
0178:     
0179:     @staticmethod
0180:     def forward(ctx, predictions, targets, reduction='mean'):
0181:         if not isinstance(predictions, Tensor):
0182:             predictions = Tensor(predictions)
0183:         if not isinstance(targets, Tensor):
0184:             targets = Tensor(targets)
0185:             
0186:         if predictions.shape != targets.shape:
0187:             raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
0188:             
0189:         diff = predictions.data - targets.data
0190:         abs_diff = np.abs(diff)
0191:         
0192:         if reduction == 'none':
0193:             result = abs_diff
0194:         elif reduction == 'sum':
0195:             result = np.sum(abs_diff)
0196:         elif reduction == 'mean':
0197:             result = np.mean(abs_diff)
0198:         else:
0199:             raise ValueError(f"Invalid reduction method: {reduction}")
0200:             
0201:         ctx.save_for_backward(predictions, targets)
0202:         ctx.save_arguments(reduction=reduction)
0203:         return Tensor(result)
0204:         
0205:     @staticmethod
0206:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0207:         predictions, targets = ctx.saved_tensors
0208:         reduction = ctx.saved_arguments['reduction']
0209:         
0210:         diff = predictions.data - targets.data
0211:         grad = np.sign(diff)
0212:         
0213:         if reduction == 'mean':
0214:             grad = grad * grad_output / np.prod(diff.shape)
0215:         else:  # 'sum' or 'none'
0216:             grad = grad * grad_output
0217:             
0218:         if predictions.requires_grad:
0219:             grad_dict[id(predictions)] = grad
0220: 
0221: class KLDivLoss(Function):
0222:     """
0223:     Kullback-Leibler Divergence Loss.
0224:     KL divergence measures the relative entropy between two probability distributions.
0225:     
0226:     Args:
0227:         reduction (str): Specifies the reduction to apply to the output:
0228:             'mean' (default) | 'sum' | 'none'
0229:         log_target (bool): If True, target is expected to be log-probabilities
0230:     """
0231:     
0232:     @staticmethod
0233:     def forward(ctx, predictions, targets, reduction='mean', log_target=False):
0234:         if not isinstance(predictions, Tensor):
0235:             predictions = Tensor(predictions)
0236:         if not isinstance(targets, Tensor):
0237:             targets = Tensor(targets)
0238:             
0239:         if not log_target:
0240:             targets_log = np.log(np.clip(targets.data, 1e-7, 1.0))
0241:         else:
0242:             targets_log = targets.data
0243:             
0244:         # KL divergence formula: KL(P||Q) = P * (log(P) - log(Q))
0245:         loss = np.exp(targets_log) * (targets_log - predictions.data)
0246:         loss = -loss  # Correct the sign to make it positive
0247:         
0248:         if reduction == 'none':
0249:             result = loss
0250:         elif reduction == 'sum':
0251:             result = np.sum(loss)
0252:         elif reduction == 'mean':
0253:             result = np.mean(loss)
0254:         else:
0255:             raise ValueError(f"Invalid reduction method: {reduction}")
0256:             
0257:         ctx.save_for_backward(predictions, targets)
0258:         ctx.save_arguments(reduction=reduction, log_target=log_target)
0259:         return Tensor(result)
0260:         
0261:     @staticmethod
0262:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0263:         predictions, targets = ctx.saved_tensors
0264:         reduction = ctx.saved_arguments['reduction']
0265:         log_target = ctx.saved_arguments['log_target']
0266:         
0267:         if not log_target:
0268:             targets_log = np.log(np.clip(targets.data, 1e-7, 1.0))
0269:         else:
0270:             targets_log = targets.data
0271:             
0272:         grad = -np.exp(targets_log) * grad_output
0273:         
0274:         if reduction == 'mean':
0275:             grad = grad / np.prod(predictions.shape)
0276:             
0277:         if predictions.requires_grad:
0278:             grad_dict[id(predictions)] = grad
0279: 
0280: class CosineSimilarityLoss(Function):
0281:     """
0282:     Cosine Similarity Loss.
0283:     Measures the cosine similarity between two vectors.
0284:     
0285:     Args:
0286:         dim (int): Dimension along which cosine similarity is computed
0287:         eps (float): Small value to avoid division by zero
0288:         reduction (str): Specifies the reduction to apply to the output
0289:     """
0290:     
0291:     @staticmethod
0292:     def forward(ctx, x1, x2, dim=1, eps=1e-8, reduction='mean'):
0293:         if not isinstance(x1, Tensor):
0294:             x1 = Tensor(x1)
0295:         if not isinstance(x2, Tensor):
0296:             x2 = Tensor(x2)
0297:             
0298:         # Compute norms
0299:         norm1 = np.sqrt(np.sum(x1.data * x1.data, axis=dim, keepdims=True))
0300:         norm2 = np.sqrt(np.sum(x2.data * x2.data, axis=dim, keepdims=True))
0301:         
0302:         # Normalize vectors
0303:         x1_normalized = x1.data / np.maximum(norm1, eps)
0304:         x2_normalized = x2.data / np.maximum(norm2, eps)
0305:         
0306:         # Compute cosine similarity
0307:         cos_sim = np.sum(x1_normalized * x2_normalized, axis=dim)
0308:         
0309:         # For orthogonal vectors, cos_sim = 0, we want loss = 1
0310:         # For identical vectors, cos_sim = 1, we want loss = 0
0311:         # Therefore, loss = 1 - cos_sim
0312:         if reduction == 'none':
0313:             result = 1 - cos_sim
0314:         elif reduction == 'sum':
0315:             result = float(np.sum(1 - cos_sim))
0316:         elif reduction == 'mean':
0317:             result = float(np.mean(1 - cos_sim))
0318:         else:
0319:             raise ValueError(f"Invalid reduction method: {reduction}")
0320:             
0321:         ctx.save_for_backward(x1, x2)
0322:         ctx.save_arguments(dim=dim, eps=eps, reduction=reduction,
0323:                          x1_normalized=x1_normalized,
0324:                          x2_normalized=x2_normalized)
0325:         return Tensor(result)
0326:         
0327:     @staticmethod
0328:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0329:         x1, x2 = ctx.saved_tensors
0330:         dim = ctx.saved_arguments['dim']
0331:         eps = ctx.saved_arguments['eps']
0332:         reduction = ctx.saved_arguments['reduction']
0333:         x1_normalized = ctx.saved_arguments['x1_normalized']
0334:         x2_normalized = ctx.saved_arguments['x2_normalized']
0335:         
0336:         if reduction == 'mean':
0337:             grad_output = grad_output / x1.shape[0]
0338:         
0339:         # Gradient with respect to x1
0340:         if x1.requires_grad:
0341:             grad_x1 = -grad_output[..., None] * x2_normalized
0342:             grad_dict[id(x1)] = grad_x1
0343:             
0344:         # Gradient with respect to x2
0345:         if x2.requires_grad:
0346:             grad_x2 = -grad_output[..., None] * x1_normalized
0347:             grad_dict[id(x2)] = grad_x2
0348: 
0349: class HingeLoss(Function):
0350:     """
0351:     Hinge Loss (max-margin loss).
0352:     Commonly used for SVM training.
0353:     L = max(0, margin - y * f(x))
0354:     
0355:     Args:
0356:         margin (float): Margin in the hinge loss
0357:         reduction (str): Specifies the reduction to apply to the output
0358:     """
0359:     
0360:     @staticmethod
0361:     def forward(ctx, predictions, targets, margin=1.0, reduction='mean'):
0362:         if not isinstance(predictions, Tensor):
0363:             predictions = Tensor(predictions)
0364:         if not isinstance(targets, Tensor):
0365:             targets = Tensor(targets)
0366:             
0367:         # Convert targets to ±1
0368:         signed_targets = 2.0 * targets.data - 1.0
0369:         
0370:         # Compute raw hinge loss
0371:         loss = np.maximum(0, margin - signed_targets * predictions.data)
0372:         
0373:         if reduction == 'none':
0374:             result = loss
0375:         elif reduction == 'sum':
0376:             result = np.sum(loss)
0377:         elif reduction == 'mean':
0378:             result = np.mean(loss)
0379:         else:
0380:             raise ValueError(f"Invalid reduction method: {reduction}")
0381:             
0382:         ctx.save_for_backward(predictions, targets)
0383:         ctx.save_arguments(margin=margin, reduction=reduction,
0384:                          signed_targets=signed_targets)
0385:         return Tensor(result)
0386:         
0387:     @staticmethod
0388:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0389:         predictions, targets = ctx.saved_tensors
0390:         margin = ctx.saved_arguments['margin']
0391:         reduction = ctx.saved_arguments['reduction']
0392:         signed_targets = ctx.saved_arguments['signed_targets']
0393:         
0394:         # Gradient is -y when margin - y*f(x) > 0, 0 otherwise
0395:         mask = (margin - signed_targets * predictions.data) > 0
0396:         grad = -signed_targets * mask * grad_output
0397:         
0398:         if reduction == 'mean':
0399:             grad = grad / np.prod(predictions.shape)
0400:             
0401:         if predictions.requires_grad:
0402:             grad_dict[id(predictions)] = grad
0403: 
0404: class FocalLoss(Function):
0405:     """
0406:     Focal Loss.
0407:     Addresses class imbalance by down-weighting easily classified examples.
0408:     FL(p) = -alpha * (1-p)^gamma * log(p)
0409:     
0410:     Args:
0411:         alpha (float): Weighting factor for rare classes
0412:         gamma (float): Focusing parameter
0413:         reduction (str): Specifies the reduction to apply to the output
0414:     """
0415:     
0416:     @staticmethod
0417:     def forward(ctx, predictions, targets, alpha=0.25, gamma=2.0, reduction='mean'):
0418:         if not isinstance(predictions, Tensor):
0419:             predictions = Tensor(predictions)
0420:         if not isinstance(targets, Tensor):
0421:             targets = Tensor(targets)
0422:             
0423:         # Clip predictions for numerical stability
0424:         eps = 1e-7
0425:         predictions_clipped = np.clip(predictions.data, eps, 1 - eps)
0426:         
0427:         # Compute pt (probability of target class)
0428:         pt = predictions_clipped * targets.data + (1 - predictions_clipped) * (1 - targets.data)
0429:         
0430:         # Compute focal weight
0431:         focal_weight = alpha * ((1 - pt) ** gamma)
0432:         
0433:         # Compute binary cross entropy
0434:         bce = -(targets.data * np.log(predictions_clipped) + 
0435:                 (1 - targets.data) * np.log(1 - predictions_clipped))
0436:         
0437:         # Apply focal weight
0438:         loss = focal_weight * bce
0439:         
0440:         if reduction == 'none':
0441:             result = loss
0442:         elif reduction == 'sum':
0443:             result = np.sum(loss)
0444:         elif reduction == 'mean':
0445:             result = np.mean(loss)
0446:         else:
0447:             raise ValueError(f"Invalid reduction method: {reduction}")
0448:             
0449:         ctx.save_for_backward(predictions, targets)
0450:         ctx.save_arguments(alpha=alpha, gamma=gamma, reduction=reduction,
0451:                          pt=pt, focal_weight=focal_weight)
0452:         return Tensor(result)
0453:         
0454:     @staticmethod
0455:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0456:         predictions, targets = ctx.saved_tensors
0457:         alpha = ctx.saved_arguments['alpha']
0458:         gamma = ctx.saved_arguments['gamma']
0459:         reduction = ctx.saved_arguments['reduction']
0460:         pt = ctx.saved_arguments['pt']
0461:         focal_weight = ctx.saved_arguments['focal_weight']
0462:         
0463:         # Compute gradient
0464:         grad = grad_output * focal_weight * (
0465:             gamma * pt * np.log(pt) + pt - targets.data
0466:         )
0467:         
0468:         if reduction == 'mean':
0469:             grad = grad / np.prod(predictions.shape)
0470:             
0471:         if predictions.requires_grad:
0472:             grad_dict[id(predictions)] = grad
0473: 
0474: class HuberLoss(Function):
0475:     """
0476:     Huber Loss: L = 0.5 * (y - ŷ)² if |y - ŷ| <= delta else delta * |y - ŷ| - 0.5 * delta²
0477:     
0478:     This loss combines the best properties of MSE and L1 loss.
0479:     For small errors it behaves like MSE, for large errors it behaves like L1.
0480:     
0481:     Args:
0482:         delta (float): Threshold where loss transitions from squared to linear
0483:         reduction (str): Specifies the reduction to apply to the output:
0484:             'mean' (default) | 'sum' | 'none'
0485:     """
0486:     
0487:     @staticmethod
0488:     def forward(ctx, predictions, targets, delta=1.0, reduction='mean'):
0489:         if not isinstance(predictions, Tensor):
0490:             predictions = Tensor(predictions)
0491:         if not isinstance(targets, Tensor):
0492:             targets = Tensor(targets)
0493:             
0494:         if predictions.shape != targets.shape:
0495:             raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
0496:             
0497:         diff = predictions.data - targets.data
0498:         abs_diff = np.abs(diff)
0499:         quadratic = np.minimum(abs_diff, delta)
0500:         linear = abs_diff - quadratic
0501:         loss = 0.5 * quadratic ** 2 + delta * linear
0502:         
0503:         if reduction == 'none':
0504:             result = loss
0505:         elif reduction == 'sum':
0506:             result = np.sum(loss)
0507:         elif reduction == 'mean':
0508:             result = np.mean(loss)
0509:         else:
0510:             raise ValueError(f"Invalid reduction method: {reduction}")
0511:             
0512:         ctx.save_for_backward(predictions, targets)
0513:         ctx.save_arguments(delta=delta, reduction=reduction)
0514:         return Tensor(result)
0515:         
0516:     @staticmethod
0517:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0518:         predictions, targets = ctx.saved_tensors
0519:         delta = ctx.saved_arguments['delta']
0520:         reduction = ctx.saved_arguments['reduction']
0521:         
0522:         diff = predictions.data - targets.data
0523:         abs_diff = np.abs(diff)
0524:         
0525:         # Gradient is diff/|diff| * min(|diff|, delta)
0526:         grad = np.sign(diff) * np.minimum(abs_diff, delta)
0527:         
0528:         if reduction == 'mean':
0529:             grad = grad * grad_output / np.prod(diff.shape)
0530:         else:  # 'sum' or 'none'
0531:             grad = grad * grad_output
0532:             
0533:         if predictions.requires_grad:
0534:             grad_dict[id(predictions)] = grad

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\matrix.py
// ----------------------------------------
0001: from typing import Dict, Optional, Union, Tuple
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class Transpose(Function):
0006:     @staticmethod
0007:     def forward(ctx, x, axes: Optional[Tuple[int, ...]] = None):
0008:         if not isinstance(x, Tensor):
0009:             x = Tensor(x)
0010:             
0011:         ctx.save_for_backward(x)
0012:         ctx.save_arguments(axes=axes)
0013:         
0014:         if axes is None:
0015:             return Tensor(np.transpose(x.data))
0016:         return Tensor(np.transpose(x.data, axes))
0017:         
0018:     @staticmethod
0019:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0020:         x, = ctx.saved_tensors
0021:         axes = ctx.saved_arguments['axes']
0022:         
0023:         if x.requires_grad:
0024:             if axes is None:
0025:                 # For standard transpose, just transpose the gradient
0026:                 grad_dict[id(x)] = np.transpose(grad_output)
0027:             else:
0028:                 # For specific axes, need to invert the permutation
0029:                 inverse_axes = np.argsort(axes)
0030:                 grad_dict[id(x)] = np.transpose(grad_output, inverse_axes)
0031: 
0032: class Compare(Function):
0033:     """Base class for comparison operations"""
0034:     @staticmethod
0035:     def _compare(op, x1, x2):
0036:         if not isinstance(x1, Tensor):
0037:             x1 = Tensor(x1)
0038:         if not isinstance(x2, Tensor):
0039:             x2 = Tensor(x2)
0040:             
0041:         return Tensor(op(x1.data, x2.data))
0042:         
0043:     @staticmethod
0044:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0045:         # Comparison operations have no gradient
0046:         pass
0047: 
0048: class Greater(Compare):
0049:     @staticmethod
0050:     def forward(ctx, x1, x2):
0051:         return Compare._compare(np.greater, x1, x2)
0052: 
0053: class GreaterEqual(Compare):
0054:     @staticmethod
0055:     def forward(ctx, x1, x2):
0056:         return Compare._compare(np.greater_equal, x1, x2)
0057: 
0058: class Less(Compare):
0059:     @staticmethod
0060:     def forward(ctx, x1, x2):
0061:         return Compare._compare(np.less, x1, x2)
0062: 
0063: class LessEqual(Compare):
0064:     @staticmethod
0065:     def forward(ctx, x1, x2):
0066:         return Compare._compare(np.less_equal, x1, x2)
0067: 
0068: class Equal(Compare):
0069:     @staticmethod
0070:     def forward(ctx, x1, x2):
0071:         return Compare._compare(np.equal, x1, x2)
0072: 
0073: class NotEqual(Compare):
0074:     @staticmethod
0075:     def forward(ctx, x1, x2):
0076:         return Compare._compare(np.not_equal, x1, x2)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\nn.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\power.py
// ----------------------------------------
0001: from typing import Dict
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class Power(Function):
0006:     @staticmethod
0007:     def forward(ctx, base, exponent):
0008:         if not isinstance(base, Tensor):
0009:             base = Tensor(base)
0010:         if not isinstance(exponent, (Tensor, int, float)):
0011:             raise TypeError("Exponent must be a Tensor, int, or float")
0012:             
0013:         # Convert Tensor exponent to scalar if possible
0014:         if isinstance(exponent, Tensor):
0015:             if exponent.data.size == 1:
0016:                 exponent = float(exponent.data)
0017:             else:
0018:                 raise ValueError("Only scalar exponents are supported")
0019:                 
0020:         ctx.save_for_backward(base)
0021:         ctx.save_arguments(exponent=exponent)
0022:         
0023:         return Tensor(np.power(base.data, exponent))
0024:         
0025:     @staticmethod
0026:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0027:         base, = ctx.saved_tensors
0028:         exponent = ctx.saved_arguments['exponent']
0029:         
0030:         if base.requires_grad:
0031:             # d/dx(x^n) = nx^(n-1)
0032:             grad = grad_output * exponent * np.power(base.data, exponent - 1)
0033:             grad_dict[id(base)] = grad
0034: 
0035: class Divide(Function):
0036:     @staticmethod
0037:     def forward(ctx, numerator, denominator):
0038:         if not isinstance(numerator, Tensor):
0039:             numerator = Tensor(numerator)
0040:         if not isinstance(denominator, Tensor):
0041:             denominator = Tensor(denominator)
0042:             
0043:         # Check for division by zero
0044:         if np.any(denominator.data == 0):
0045:             raise ValueError("Division by zero encountered")
0046:             
0047:         ctx.save_for_backward(numerator, denominator)
0048:         return Tensor(numerator.data / denominator.data)
0049:         
0050:     @staticmethod
0051:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0052:         numerator, denominator = ctx.saved_tensors
0053:         
0054:         if numerator.requires_grad:
0055:             # d/dx(x/y) = 1/y
0056:             grad_dict[id(numerator)] = grad_output / denominator.data
0057:             
0058:         if denominator.requires_grad:
0059:             # d/dy(x/y) = -x/y^2
0060:             grad_dict[id(denominator)] = -grad_output * numerator.data / (denominator.data ** 2)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\reduction.py
// ----------------------------------------
0001: from typing import Dict, Optional, Union, Tuple
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class Sum(Function):
0006:     @staticmethod
0007:     def forward(ctx, x, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False):
0008:         if not isinstance(x, Tensor):
0009:             x = Tensor(x)
0010:             
0011:         ctx.save_for_backward(x)
0012:         ctx.save_arguments(axis=axis, keepdims=keepdims, input_shape=x.shape)
0013:         
0014:         return Tensor(np.sum(x.data, axis=axis, keepdims=keepdims))
0015:         
0016:     @staticmethod
0017:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0018:         x, = ctx.saved_tensors
0019:         axis = ctx.saved_arguments['axis']
0020:         keepdims = ctx.saved_arguments['keepdims']
0021:         input_shape = ctx.saved_arguments['input_shape']
0022:         
0023:         if x.requires_grad:
0024:             # If not keeping dims, need to reshape grad_output to match broadcast
0025:             if not keepdims and axis is not None:
0026:                 grad_output = np.expand_dims(grad_output, axis=axis)
0027:                 
0028:             # Broadcast gradient to match input shape
0029:             grad = np.broadcast_to(grad_output, input_shape)
0030:             grad_dict[id(x)] = grad
0031: 
0032: class Mean(Function):
0033:     @staticmethod
0034:     def forward(ctx, x, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False):
0035:         if not isinstance(x, Tensor):
0036:             x = Tensor(x)
0037:             
0038:         ctx.save_for_backward(x)
0039:         ctx.save_arguments(axis=axis, keepdims=keepdims, input_shape=x.shape)
0040:         
0041:         return Tensor(np.mean(x.data, axis=axis, keepdims=keepdims))
0042:         
0043:     @staticmethod
0044:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0045:         x, = ctx.saved_tensors
0046:         axis = ctx.saved_arguments['axis']
0047:         keepdims = ctx.saved_arguments['keepdims']
0048:         input_shape = ctx.saved_arguments['input_shape']
0049:         
0050:         if x.requires_grad:
0051:             # If not keeping dims, need to reshape grad_output to match broadcast
0052:             if not keepdims and axis is not None:
0053:                 grad_output = np.expand_dims(grad_output, axis=axis)
0054:                 
0055:             # Calculate number of elements we're taking mean over
0056:             if axis is None:
0057:                 n = np.prod(input_shape)
0058:             else:
0059:                 n = np.prod([input_shape[i] for i in (axis,) if i < len(input_shape)])
0060:                 
0061:             # Broadcast gradient to match input shape and divide by n
0062:             grad = np.broadcast_to(grad_output, input_shape) / n
0063:             grad_dict[id(x)] = grad
0064: 
0065: class Max(Function):
0066:     @staticmethod
0067:     def forward(ctx, x, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False):
0068:         if not isinstance(x, Tensor):
0069:             x = Tensor(x)
0070:             
0071:         result = np.amax(x.data, axis=axis, keepdims=True)
0072:         ctx.save_for_backward(x)
0073:         ctx.save_arguments(axis=axis, keepdims=keepdims, max_vals=result)
0074:         
0075:         if not keepdims:
0076:             result = np.squeeze(result, axis=axis)
0077:             
0078:         return Tensor(result)
0079:         
0080:     @staticmethod
0081:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0082:         x, = ctx.saved_tensors
0083:         axis = ctx.saved_arguments['axis']
0084:         keepdims = ctx.saved_arguments['keepdims']
0085:         max_vals = ctx.saved_arguments['max_vals']
0086:         
0087:         if x.requires_grad:
0088:             # If not keeping dims, need to reshape grad_output
0089:             if not keepdims and axis is not None:
0090:                 grad_output = np.expand_dims(grad_output, axis=axis)
0091:                 
0092:             # Create gradient mask (1 where x equals max, 0 elsewhere)
0093:             mask = (x.data == max_vals)
0094:             
0095:             # In case of multiple maxima, distribute gradient equally
0096:             mask = mask / np.sum(mask, axis=axis, keepdims=True)
0097:             
0098:             grad_dict[id(x)] = grad_output * mask

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\reshape.py
// ----------------------------------------
0001: # DLpy/ops/reshape.py
0002: from ..core.function import Function
0003: from ..core.tensor import Tensor
0004: import numpy as np
0005: from typing import Dict
0006: 
0007: class Reshape(Function):
0008:     @staticmethod
0009:     def forward(ctx, tensor, shape):
0010:         # Save both the input tensor and the target shape
0011:         ctx.save_for_backward(tensor)
0012:         ctx.save_arguments(target_shape=shape)
0013:         # Create and return a new tensor with the reshaped data
0014:         return Tensor(tensor.data.reshape(shape))
0015:         
0016:     @staticmethod
0017:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0018:         # Get the original tensor and reshape the gradient back to its shape
0019:         original_tensor, = ctx.saved_tensors
0020:         if original_tensor.requires_grad:
0021:             # Reshape gradient back to the original tensor's shape
0022:             grad_dict[id(original_tensor)] = grad_output.reshape(original_tensor.shape)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\__init__.py
// ----------------------------------------
0001: """
0002: Optimization algorithms for DLpy.
0003: 
0004: This module implements various optimization algorithms used in deep learning.
0005: """
0006: 
0007: from .optimizer import Optimizer
0008: from .sgd import SGD
0009: from .adam import Adam
0010: from .rmsprop import RMSprop
0011: from .adagrad import AdaGrad
0012: 
0013: __all__ = [
0014:     'Optimizer',
0015:     'SGD',
0016:     'Adam',
0017:     'RMSprop',
0018:     'AdaGrad'
0019: ]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\adagrad.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Dict, Iterator, Optional
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class AdaGrad(Optimizer):
0007:     """
0008:     Implements AdaGrad algorithm.
0009:     
0010:     AdaGrad is an optimizer with parameter-specific learning rates,
0011:     which are adapted based on historical gradient information.
0012:     
0013:     Args:
0014:         params: Iterable of parameters to optimize
0015:         lr (float): Learning rate (default: 1e-2)
0016:         lr_decay (float): Learning rate decay (default: 0)
0017:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0018:         eps (float): Term added to denominator to improve numerical stability (default: 1e-10)
0019:         initial_accumulator_value (float): Initial value for accumulator (default: 0)
0020:     """
0021:     
0022:     def __init__(self, params, lr: float = 1e-2, lr_decay: float = 0,
0023:                  weight_decay: float = 0, initial_accumulator_value: float = 0,
0024:                  eps: float = 1e-10):
0025:         if not 0.0 <= lr:
0026:             raise ValueError(f"Invalid learning rate: {lr}")
0027:         if not 0.0 <= lr_decay:
0028:             raise ValueError(f"Invalid lr_decay value: {lr_decay}")
0029:         if not 0.0 <= weight_decay:
0030:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0031:         if not 0.0 <= initial_accumulator_value:
0032:             raise ValueError(f"Invalid initial_accumulator_value value: {initial_accumulator_value}")
0033:         if not 0.0 <= eps:
0034:             raise ValueError(f"Invalid epsilon value: {eps}")
0035:             
0036:         defaults = dict(lr=lr, lr_decay=lr_decay, eps=eps, 
0037:                        weight_decay=weight_decay,
0038:                        initial_accumulator_value=initial_accumulator_value)
0039:         super().__init__(params, defaults)
0040: 
0041:         for group in self._params:
0042:             state = self.state[id(group)]
0043:             state['step'] = 0
0044:             state['sum'] = np.full_like(group.data, initial_accumulator_value, dtype=np.float64)
0045: 
0046:     def step(self) -> None:
0047:         """
0048:         Performs a single optimization step.
0049:         
0050:         For each parameter p, accumulates the square of the gradient and then
0051:         updates the parameter using the formula:
0052:         p = p - lr * g / (sqrt(accumulator) + eps)
0053:         where g is the gradient.
0054:         """
0055:         for p in self._params:
0056:             if p.grad is None:
0057:                 continue
0058:                 
0059:             grad = p.grad
0060:             state = self.state[id(p)]
0061: 
0062:             state['step'] += 1
0063: 
0064:             if self.defaults['weight_decay'] != 0:
0065:                 grad = grad + self.defaults['weight_decay'] * p.data
0066: 
0067:             # Update accumulator with squared gradient
0068:             state['sum'] += grad * grad
0069: 
0070:             # Compute the adaptive learning rate
0071:             std = np.sqrt(state['sum'])
0072:             
0073:             # Add epsilon for numerical stability before division
0074:             denom = std + self.defaults['eps']
0075: 
0076:             # Apply learning rate decay if specified
0077:             if self.defaults['lr_decay'] != 0:
0078:                 lr = self.defaults['lr'] / (1 + (state['step'] - 1) * self.defaults['lr_decay'])
0079:             else:
0080:                 lr = self.defaults['lr']
0081: 
0082:             # Update parameters
0083:             p.data -= lr * grad / denom
0084: 
0085:     def reset_state(self) -> None:
0086:         """
0087:         Resets the state of the optimizer.
0088:         
0089:         This can be useful when you want to restart training or when you want to 
0090:         reset the accumulated gradients without creating a new optimizer instance.
0091:         """
0092:         initial_accumulator_value = self.defaults['initial_accumulator_value']
0093:         
0094:         for group in self._params:
0095:             state = self.state[id(group)]
0096:             state['step'] = 0
0097:             state['sum'] = np.full_like(group.data, initial_accumulator_value, dtype=np.float64)
0098: 
0099:     def state_dict(self) -> Dict:
0100:         """
0101:         Returns the state of the optimizer as a Dict.
0102:         
0103:         The returned state dict contains two entries:
0104:             * state - a dict holding current optimization state. Its content
0105:                 differs between optimizer classes.
0106:             * param_groups - a dict containing all parameter groups
0107:         """
0108:         return {
0109:             'state': self.state,
0110:             'defaults': self.defaults
0111:         }
0112: 
0113:     def load_state_dict(self, state_dict: Dict) -> None:
0114:         """
0115:         Loads the optimizer state.
0116:         
0117:         Args:
0118:             state_dict (dict): Optimizer state. Should be an object returned
0119:                 from a call to state_dict().
0120:         """
0121:         self.state = state_dict['state']
0122:         self.defaults = state_dict['defaults']

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\adam.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Dict, Iterator, Optional
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class Adam(Optimizer):
0007:     """
0008:     Implements Adam algorithm.
0009:     
0010:     Args:
0011:         params: Iterable of parameters to optimize
0012:         lr (float): Learning rate (default: 0.001)
0013:         betas (tuple): Coefficients for computing running averages of gradient and its square
0014:             (default: (0.9, 0.999))
0015:         eps (float): Term added to denominator to improve numerical stability (default: 1e-8)
0016:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0017:         amsgrad (bool): Whether to use the AMSGrad variant (default: False)
0018:     """
0019:     
0020:     def __init__(self, params, lr: float = 0.001, betas: tuple = (0.9, 0.999),
0021:                  eps: float = 1e-8, weight_decay: float = 0, amsgrad: bool = False):
0022:         if not 0.0 <= lr:
0023:             raise ValueError(f"Invalid learning rate: {lr}")
0024:         if not 0.0 <= eps:
0025:             raise ValueError(f"Invalid epsilon value: {eps}")
0026:         if not 0.0 <= betas[0] < 1.0:
0027:             raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
0028:         if not 0.0 <= betas[1] < 1.0:
0029:             raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")
0030:         if not 0.0 <= weight_decay:
0031:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0032:             
0033:         defaults = dict(lr=lr, betas=betas, eps=eps,
0034:                        weight_decay=weight_decay, amsgrad=amsgrad)
0035:         super().__init__(params, defaults)
0036:         
0037:     def step(self) -> None:
0038:         """Performs a single optimization step."""
0039:         for p in self._params:
0040:             if p.grad is None:
0041:                 continue
0042:                 
0043:             grad = p.grad
0044:             
0045:             # Get optimizer state
0046:             state = self.state[id(p)]
0047:             
0048:             # State initialization
0049:             if len(state) == 0:
0050:                 state['step'] = 0
0051:                 # Exponential moving average of gradient values
0052:                 state['exp_avg'] = np.zeros_like(p.data)
0053:                 # Exponential moving average of squared gradient values
0054:                 state['exp_avg_sq'] = np.zeros_like(p.data)
0055:                 if self.defaults['amsgrad']:
0056:                     # Maintains max of all exp. moving avg. of sq. grad. values
0057:                     state['max_exp_avg_sq'] = np.zeros_like(p.data)
0058:                     
0059:             exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
0060:             if self.defaults['amsgrad']:
0061:                 max_exp_avg_sq = state['max_exp_avg_sq']
0062:             beta1, beta2 = self.defaults['betas']
0063:             
0064:             state['step'] += 1
0065:             bias_correction1 = 1 - beta1 ** state['step']
0066:             bias_correction2 = 1 - beta2 ** state['step']
0067:             
0068:             if self.defaults['weight_decay'] != 0:
0069:                 grad = grad + self.defaults['weight_decay'] * p.data
0070:                 
0071:             # Decay the first and second moment running average coefficient
0072:             exp_avg = beta1 * exp_avg + (1 - beta1) * grad
0073:             exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * grad * grad
0074:             
0075:             if self.defaults['amsgrad']:
0076:                 # Maintains the maximum of all 2nd moment running avg. till now
0077:                 max_exp_avg_sq = np.maximum(max_exp_avg_sq, exp_avg_sq)
0078:                 # Use the max. for normalizing running avg. of gradient
0079:                 denom = (np.sqrt(max_exp_avg_sq) / np.sqrt(bias_correction2)) + self.defaults['eps']
0080:             else:
0081:                 denom = (np.sqrt(exp_avg_sq) / np.sqrt(bias_correction2)) + self.defaults['eps']
0082:                 
0083:             step_size = self.defaults['lr'] / bias_correction1
0084:             
0085:             p.data -= step_size * exp_avg / denom
0086:             
0087:             # Save state
0088:             state['exp_avg'] = exp_avg
0089:             state['exp_avg_sq'] = exp_avg_sq
0090:             if self.defaults['amsgrad']:
0091:                 state['max_exp_avg_sq'] = max_exp_avg_sq

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\optimizer.py
// ----------------------------------------
0001: from typing import Dict, Iterator, Optional
0002: from ..core import Tensor
0003: 
0004: class Optimizer:
0005:     """
0006:     Base class for all optimizers.
0007:     
0008:     Args:
0009:         params: An iterable of parameters to optimize or a dict of parameter groups
0010:         defaults: Dictionary of default hyperparameter values
0011:     """
0012:     
0013:     def __init__(self, params, defaults: Dict):
0014:         self.defaults = defaults
0015:         self._params = list(params)  # Convert iterator to list
0016:         self.state: Dict = {}  # State dict for optimizer states
0017:         
0018:         # Initialize state for each parameter
0019:         for p in self._params:
0020:             self.state[id(p)] = {}
0021:             
0022:     def zero_grad(self) -> None:
0023:         """Clears the gradients of all optimized parameters."""
0024:         for p in self._params:
0025:             if p.grad is not None:
0026:                 p.grad.fill(0)
0027:                 
0028:     def step(self) -> None:
0029:         """Performs a single optimization step.
0030:         
0031:         This method should be overridden by all optimizers.
0032:         """
0033:         raise NotImplementedError
0034:         
0035:     def add_param_group(self, param_group: Dict) -> None:
0036:         """Add a param group to the optimizer's param groups.
0037:         
0038:         Args:
0039:             param_group (dict): Specifies parameters and parameter-specific options
0040:         """
0041:         params = param_group['params']
0042:         if isinstance(params, Tensor):
0043:             param_group['params'] = [params]
0044:         elif isinstance(params, set):
0045:             param_group['params'] = list(params)
0046:             
0047:         for param in param_group['params']:
0048:             if id(param) not in self.state:
0049:                 self.state[id(param)] = {}
0050:             self._params.append(param)
0051:             
0052:     def load_state_dict(self, state_dict: Dict) -> None:
0053:         """Loads the optimizer state.
0054:         
0055:         Args:
0056:             state_dict (dict): Optimizer state dict
0057:         """
0058:         self.state = state_dict['state']
0059:         
0060:     def state_dict(self) -> Dict:
0061:         """Returns the state of the optimizer as a dict.
0062:         
0063:         Returns:
0064:             dict: The state of the optimizer
0065:         """
0066:         return {
0067:             'state': self.state,
0068:         }

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\rmsprop.py
// ----------------------------------------
0001: import numpy as np 
0002: from typing import Dict, Iterator, Optional
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class RMSprop(Optimizer):
0007:     """
0008:     Implements RMSprop algorithm.
0009:     
0010:     Args:
0011:         params: Iterable of parameters to optimize
0012:         lr (float): Learning rate (default: 0.01)
0013:         alpha (float): Smoothing constant (default: 0.99)
0014:         eps (float): Term added to denominator to improve numerical stability (default: 1e-8)
0015:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0016:         momentum (float): Momentum factor (default: 0)
0017:         centered (bool): If True, compute centered RMSprop, gradients normalized by their variance
0018:     """
0019:     
0020:     def __init__(self, params, lr: float = 0.01, alpha: float = 0.99,
0021:                  eps: float = 1e-8, weight_decay: float = 0,
0022:                  momentum: float = 0, centered: bool = False):
0023:         if not 0.0 <= lr:
0024:             raise ValueError(f"Invalid learning rate: {lr}")
0025:         if not 0.0 <= eps:
0026:             raise ValueError(f"Invalid epsilon value: {eps}")
0027:         if not 0.0 <= momentum:
0028:             raise ValueError(f"Invalid momentum value: {momentum}")
0029:         if not 0.0 <= alpha:
0030:             raise ValueError(f"Invalid alpha value: {alpha}")
0031:         if not 0.0 <= weight_decay:
0032:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0033:             
0034:         defaults = dict(lr=lr, alpha=alpha, eps=eps, weight_decay=weight_decay,
0035:                        momentum=momentum, centered=centered)
0036:         super().__init__(params, defaults)
0037:         
0038:     def step(self) -> None:
0039:         """Performs a single optimization step."""
0040:         for p in self._params:
0041:             if p.grad is None:
0042:                 continue
0043:                 
0044:             grad = p.grad
0045:             state = self.state[id(p)]
0046:             
0047:             # State initialization
0048:             if len(state) == 0:
0049:                 state['step'] = 0
0050:                 state['square_avg'] = np.zeros_like(p.data)
0051:                 if self.defaults['momentum'] > 0:
0052:                     state['momentum_buffer'] = np.zeros_like(p.data)
0053:                 if self.defaults['centered']:
0054:                     state['grad_avg'] = np.zeros_like(p.data)
0055:                     
0056:             square_avg = state['square_avg']
0057:             alpha = self.defaults['alpha']
0058:             
0059:             state['step'] += 1
0060:             
0061:             if self.defaults['weight_decay'] != 0:
0062:                 grad = grad + self.defaults['weight_decay'] * p.data
0063:                 
0064:             # Update squared average
0065:             square_avg = alpha * square_avg + (1 - alpha) * grad * grad
0066:             
0067:             if self.defaults['centered']:
0068:                 grad_avg = state['grad_avg']
0069:                 grad_avg = alpha * grad_avg + (1 - alpha) * grad
0070:                 avg = square_avg - grad_avg * grad_avg
0071:                 state['grad_avg'] = grad_avg
0072:             else:
0073:                 avg = square_avg
0074:                 
0075:             # Apply momentum if enabled
0076:             if self.defaults['momentum'] > 0:
0077:                 buf = state.get('momentum_buffer', np.zeros_like(grad))
0078:                 buf = self.defaults['momentum'] * buf + grad / (np.sqrt(avg) + self.defaults['eps'])
0079:                 state['momentum_buffer'] = buf
0080:                 p.data -= self.defaults['lr'] * buf
0081:             else:
0082:                 p.data -= self.defaults['lr'] * grad / (np.sqrt(avg) + self.defaults['eps'])
0083:                 
0084:             # Save state
0085:             state['square_avg'] = square_avg

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\sgd.py
// ----------------------------------------
0001: from typing import Dict, Iterator, Optional
0002: import numpy as np
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class SGD(Optimizer):
0007:     """
0008:     Implements stochastic gradient descent with momentum.
0009:     
0010:     Args:
0011:         params: Iterable of parameters to optimize
0012:         lr (float): Learning rate (default: 0.1)
0013:         momentum (float): Momentum factor (default: 0)
0014:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0015:         dampening (float): Dampening for momentum (default: 0)
0016:         nesterov (bool): Enables Nesterov momentum (default: False)
0017:     """
0018:     
0019:     def __init__(self, params, lr: float = 0.1, momentum: float = 0.0,
0020:                  weight_decay: float = 0.0, dampening: float = 0.0,
0021:                  nesterov: bool = False):
0022:         if lr < 0.0:
0023:             raise ValueError(f"Invalid learning rate: {lr}")
0024:         if momentum < 0.0:
0025:             raise ValueError(f"Invalid momentum value: {momentum}")
0026:         if weight_decay < 0.0:
0027:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0028:             
0029:         defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay,
0030:                        dampening=dampening, nesterov=nesterov)
0031:         super().__init__(params, defaults)
0032:         
0033:     def step(self) -> None:
0034:         """Performs a single optimization step."""
0035:         
0036:         for p in self._params:
0037:             if p.grad is None:
0038:                 continue
0039:                 
0040:             grad = p.grad
0041:             
0042:             # Apply weight decay
0043:             if self.defaults['weight_decay'] != 0:
0044:                 grad = grad + self.defaults['weight_decay'] * p.data
0045:                 
0046:             # Get or initialize momentum buffer
0047:             if 'momentum_buffer' not in self.state[id(p)]:
0048:                 buf = self.state[id(p)]['momentum_buffer'] = np.zeros_like(p.data)
0049:             else:
0050:                 buf = self.state[id(p)]['momentum_buffer']
0051:                 
0052:             # Update momentum buffer
0053:             if self.defaults['momentum'] != 0:
0054:                 buf *= self.defaults['momentum']
0055:                 if self.defaults['dampening'] != 0:
0056:                     grad *= 1 - self.defaults['dampening']
0057:                 buf += grad
0058:             else:
0059:                 buf = grad
0060:                 
0061:             # Nesterov momentum
0062:             if self.defaults['nesterov']:
0063:                 grad += self.defaults['momentum'] * buf
0064:             else:
0065:                 grad = buf
0066:                 
0067:             # Update parameters
0068:             p.data -= self.defaults['lr'] * grad
0069:             
0070:             # Store updated momentum buffer
0071:             self.state[id(p)]['momentum_buffer'] = buf

// File: C:\Users\aluja\Desktop\DLpy\examples\basic_autograd.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\examples\neural_network.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\setup.py
// ----------------------------------------
0001: from setuptools import setup, find_packages
0002: 
0003: setup(
0004:     name="DLpy",  # Changed from DLpy to DLpy
0005:     version="0.1.0",
0006:     packages=find_packages(include=["DLpy", "DLpy.*"]),  # Changed from DLpy to DLpy
0007:     install_requires=[
0008:         "numpy>=1.20.0",
0009:     ],
0010:     extras_require={
0011:         "dev": [
0012:             "pytest>=7.0.0",
0013:             "pytest-cov>=4.0.0",
0014:             "pytest-xdist>=3.0.0",
0015:             "black>=22.0.0",
0016:             "isort>=5.0.0",
0017:             "mypy>=1.0.0",
0018:         ],
0019:     },
0020:     python_requires=">=3.8",
0021: )

// File: C:\Users\aluja\Desktop\DLpy\tests\__init__.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\tests\test_activations.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: from DLpy.nn.activations import (
0005:     relu, leaky_relu, elu, gelu, sigmoid, tanh,
0006:     ReLU, LeakyReLU, ELU, GELU, Sigmoid, Tanh
0007: )
0008: 
0009: class TestActivations:
0010:     """Tests for activation functions"""
0011:     
0012:     def test_relu(self):
0013:         """Test ReLU activation"""
0014:         x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
0015:         y = relu(x)
0016:         y.backward(np.ones_like(x.data))
0017:         
0018:         assert np.array_equal(y.data, [0.0, 0.0, 0.0, 1.0, 2.0])
0019:         assert np.array_equal(x.grad, [0.0, 0.0, 0.0, 1.0, 1.0])
0020:         
0021:     def test_leaky_relu(self):
0022:         """Test Leaky ReLU activation"""
0023:         x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
0024:         slope = 0.1
0025:         y = leaky_relu(x, negative_slope=slope)
0026:         y.backward(np.ones_like(x.data))
0027:         
0028:         expected_forward = [-0.2, -0.1, 0.0, 1.0, 2.0]
0029:         expected_backward = [slope, slope, slope, 1.0, 1.0]
0030:         
0031:         assert np.allclose(y.data, expected_forward)
0032:         assert np.allclose(x.grad, expected_backward)
0033:         
0034:     def test_elu(self):
0035:         """Test ELU activation"""
0036:         x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
0037:         alpha = 1.0
0038:         y = elu(x, alpha=alpha)
0039:         y.backward(np.ones_like(x.data))
0040:         
0041:         expected_forward = [alpha * (np.exp(-2.0) - 1), alpha * (np.exp(-1.0) - 1), 0.0, 1.0, 2.0]
0042:         expected_backward = [alpha * np.exp(-2.0), alpha * np.exp(-1.0), alpha * 1.0, 1.0, 1.0]
0043:         
0044:         assert np.allclose(y.data, expected_forward)
0045:         assert np.allclose(x.grad, expected_backward)
0046:         
0047:     def test_gelu(self):
0048:         """Test GELU activation"""
0049:         x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
0050:         y = gelu(x)
0051:         y.backward(np.ones_like(x.data))
0052:         
0053:         # Values should be finite and have correct shape
0054:         assert not np.any(np.isnan(y.data))
0055:         assert not np.any(np.isinf(y.data))
0056:         assert y.data.shape == x.data.shape
0057:         assert not np.any(np.isnan(x.grad))
0058:         assert not np.any(np.isinf(x.grad))
0059:         
0060:         # Test specific known values
0061:         assert np.allclose(y.data[2], 0.0)  # GELU(0) = 0
0062:         assert y.data[3] > 0.8  # GELU(1) ≈ 0.841
0063:         assert y.data[1] < 0  # GELU(-1) is negative
0064:         
0065:     def test_sigmoid(self):
0066:         """Test Sigmoid activation"""
0067:         x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
0068:         y = sigmoid(x)
0069:         y.backward(np.ones_like(x.data))
0070:         
0071:         sigmoid_x = 1 / (1 + np.exp(-x.data))
0072:         expected_backward = sigmoid_x * (1 - sigmoid_x)
0073:         
0074:         assert np.allclose(y.data, sigmoid_x)
0075:         assert np.allclose(x.grad, expected_backward)
0076:         
0077:         # Test special values
0078:         assert np.allclose(y.data[2], 0.5)  # sigmoid(0) = 0.5
0079:         assert y.data[0] < 0.5  # sigmoid(-2) < 0.5
0080:         assert y.data[4] > 0.5  # sigmoid(2) > 0.5
0081:         
0082:     def test_tanh(self):
0083:         """Test Tanh activation"""
0084:         x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
0085:         y = tanh(x)
0086:         y.backward(np.ones_like(x.data))
0087:         
0088:         expected_forward = np.tanh(x.data)
0089:         expected_backward = 1 - np.tanh(x.data) ** 2
0090:         
0091:         assert np.allclose(y.data, expected_forward)
0092:         assert np.allclose(x.grad, expected_backward)
0093:         
0094:         # Test special values
0095:         assert np.allclose(y.data[2], 0.0)  # tanh(0) = 0
0096:         assert y.data[0] < -0.9  # tanh(-2) ≈ -0.964
0097:         assert y.data[4] > 0.9   # tanh(2) ≈ 0.964
0098: 
0099: class TestNumericalStability:
0100:     """Tests for numerical stability of activation functions"""
0101:     
0102:     def test_sigmoid_stability(self):
0103:         """Test Sigmoid with large inputs"""
0104:         x = Tensor([1000.0, -1000.0], requires_grad=True)
0105:         y = sigmoid(x)
0106:         y.backward(np.ones_like(x.data))
0107:         
0108:         # Check that values are properly clamped
0109:         assert np.allclose(y.data[0], 1.0)
0110:         assert np.allclose(y.data[1], 0.0)
0111:         assert not np.any(np.isnan(y.data))
0112:         assert not np.any(np.isnan(x.grad))
0113:         
0114:     def test_elu_stability(self):
0115:         """Test ELU with large negative inputs"""
0116:         x = Tensor([-1000.0], requires_grad=True)
0117:         y = elu(x)
0118:         y.backward(np.ones_like(x.data))
0119:         
0120:         # Should be close to -1.0 for large negative values
0121:         assert np.allclose(y.data[0], -1.0, rtol=1e-3)
0122:         assert not np.any(np.isnan(y.data))
0123:         assert not np.any(np.isnan(x.grad))
0124:         
0125:     def test_gelu_stability(self):
0126:         """Test GELU with large inputs"""
0127:         x = Tensor([1000.0, -1000.0], requires_grad=True)
0128:         y = gelu(x)
0129:         y.backward(np.ones_like(x.data))
0130:         
0131:         # Check that outputs are finite
0132:         assert not np.any(np.isnan(y.data))
0133:         assert not np.any(np.isinf(y.data))
0134:         assert not np.any(np.isnan(x.grad))
0135:         assert not np.any(np.isinf(x.grad))
0136: 
0137: class TestGradientFlow:
0138:     """Tests for gradient flow through activation functions"""
0139:     
0140:     def test_relu_dead_neurons(self):
0141:         """Test ReLU gradient flow for negative inputs"""
0142:         x = Tensor([-1.0], requires_grad=True)
0143:         y = relu(x)
0144:         y.backward(np.array([1.0]))
0145:         
0146:         assert x.grad[0] == 0.0  # Gradient should be zero for negative input
0147:         
0148:     def test_leaky_relu_gradient_flow(self):
0149:         """Test Leaky ReLU gradient flow"""
0150:         x = Tensor([-1.0], requires_grad=True)
0151:         slope = 0.01
0152:         y = leaky_relu(x, negative_slope=slope)
0153:         y.backward(np.array([1.0]))
0154:         
0155:         assert x.grad[0] == slope  # Should have small but non-zero gradient
0156:         
0157:     def test_elu_gradient_flow(self):
0158:         """Test ELU gradient flow"""
0159:         x = Tensor([-1.0, 1.0], requires_grad=True)
0160:         y = elu(x)
0161:         y.backward(np.ones_like(x.data))
0162:         
0163:         assert x.grad[0] > 0.0  # Should have positive gradient for negative input
0164:         assert x.grad[1] == 1.0  # Should have gradient 1 for positive input
0165: 
0166: class TestShapes:
0167:     """Tests for handling different input shapes"""
0168:     
0169:     def test_batch_input(self):
0170:         """Test activations with batched input"""
0171:         batch_size, features = 32, 10
0172:         x = Tensor(np.random.randn(batch_size, features), requires_grad=True)
0173:         
0174:         # Test all activations with batched input
0175:         activations = [relu, leaky_relu, elu, gelu, sigmoid, tanh]
0176:         for activation in activations:
0177:             y = activation(x)
0178:             assert y.shape == x.shape
0179:             y.backward(np.ones_like(x.data))
0180:             assert x.grad.shape == x.shape
0181:             
0182:     def test_scalar_input_single(self):
0183:         """Test scalar input handling for a single activation"""
0184:         x = Tensor(2.0, requires_grad=True)
0185:         y = relu(x)
0186:         
0187:         # Check forward pass maintains scalar nature
0188:         assert y.data.ndim == 0
0189:         assert isinstance(y.data, np.ndarray)
0190:         assert y.data.shape == ()
0191:         
0192:         # Check backward pass (gradient should be size 1 array as in PyTorch)
0193:         y.backward(np.array(1.0))
0194:         assert x.grad.size == 1
0195:         assert isinstance(x.grad, np.ndarray)
0196: 
0197:     def test_scalar_input(self):
0198:         """Test activations with scalar input"""
0199:         x = Tensor(2.0, requires_grad=True)
0200:         
0201:         # Test all activations with scalar input
0202:         activations = [relu, leaky_relu, elu, gelu, sigmoid, tanh]
0203:         for activation in activations:
0204:             y = activation(x)
0205:             assert y.data.ndim == 0  # Should preserve scalar nature
0206:             y.backward(np.array(1.0))
0207:             assert x.grad.size == 1  # Gradient should be size 1 array (matching PyTorch behavior)
0208: 
0209: class TestCustomGradients:
0210:     """Tests for custom gradient computations"""
0211:     
0212:     def test_relu_custom_gradient(self):
0213:         """Test ReLU with custom gradient"""
0214:         x = Tensor([1.0, -1.0], requires_grad=True)
0215:         y = relu(x)
0216:         y.backward(np.array([2.0, 2.0]))  # Custom gradient values
0217:         
0218:         assert np.array_equal(x.grad, [2.0, 0.0])  # Should scale gradient for positive input
0219:         
0220:     def test_sigmoid_custom_gradient(self):
0221:         """Test Sigmoid with custom gradient"""
0222:         x = Tensor([0.0], requires_grad=True)
0223:         y = sigmoid(x)
0224:         y.backward(np.array([2.0]))  # Custom gradient value
0225:         
0226:         expected_grad = 2.0 * 0.25  # 2.0 * sigmoid(0) * (1 - sigmoid(0))
0227:         assert np.allclose(x.grad, expected_grad)

// File: C:\Users\aluja\Desktop\DLpy\tests\test_autograd.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import (
0004:     Tensor,
0005:     get_autograd_engine
0006: )
0007: from DLpy.ops import Add, Multiply
0008: from DLpy.core.autograd import Edge
0009: 
0010: class TestAutogradEngine:
0011:     """Tests for the autograd engine's core functionality."""
0012:     
0013:     def setup_method(self):
0014:         """Setup method run before each test."""
0015:         self.engine = get_autograd_engine()
0016:         self.engine.clear()
0017: 
0018:     def test_register_tensor(self):
0019:         """Test registering a tensor with the autograd engine."""
0020:         tensor = Tensor([1.0], requires_grad=True)
0021:         self.engine.register_tensor(tensor)
0022:         assert id(tensor) in self.engine._nodes
0023: 
0024:     def test_add_edge(self):
0025:         """Test adding edges between tensors in the computational graph."""
0026:         t1 = Tensor([1.0], requires_grad=True)
0027:         t2 = Tensor([2.0], requires_grad=True)
0028:         
0029:         self.engine.register_tensor(t1)
0030:         self.engine.register_tensor(t2)
0031:         self.engine.add_edge(t1, t2)
0032:         
0033:         node1 = self.engine._nodes[id(t1)]
0034:         node2 = self.engine._nodes[id(t2)]
0035:         
0036:         assert len(node1.out_edges) == 1
0037:         assert len(node2.in_edges) == 1
0038:         assert node1.out_edges[0].dst == node2
0039: 
0040: class TestGradientComputation:
0041:     """Tests for gradient computation in different graph structures."""
0042:     
0043:     def setup_method(self):
0044:         self.engine = get_autograd_engine()
0045:         self.engine.clear()
0046: 
0047:     def test_linear_graph(self):
0048:         """Test gradient computation in a linear graph."""
0049:         # Create a simple linear computation: z = 2x + y
0050:         x = Tensor([2.0], requires_grad=True)
0051:         y = Tensor([3.0], requires_grad=True)
0052:         z = Add.apply(x, y)
0053:         self.engine.backward(z, np.array([1.0]))
0054:         
0055:         # Check gradients
0056:         assert np.allclose(x.grad, [1.0])
0057:         assert np.allclose(y.grad, [1.0])
0058: 
0059:     def test_branching_graph(self):
0060:         """Test gradient computation in a graph with multiple paths."""
0061: 
0062:         # The test creates a computation graph shaped like:
0063:         #     x
0064:         #   /   \  
0065:         #  y1   y2
0066:         #   \   /
0067:         #     z
0068: 
0069:         # This tests whether gradients properly flow and accumulate through 
0070:         # multiple paths back to the same input.
0071:         x = Tensor([2.0], requires_grad=True)
0072:         y1 = Multiply.apply(x, Tensor([2.0]))  # y1 = 2x
0073:         y2 = Multiply.apply(x, Tensor([3.0]))  # y2 = 3x
0074:         z = Add.apply(y1, y2)  # z = y1 + y2 = 5x
0075: 
0076:         self.engine.backward(z, np.array([1.0]))
0077:         # Gradient should be 5.0 (sum of both paths: 2 + 3)
0078:         assert np.allclose(x.grad, [5.0])
0079: 
0080:     def test_diamond_graph(self):
0081:         """Test gradient computation in a diamond-shaped graph."""
0082:         # Create a diamond computation:
0083:         #     x
0084:         #    / \
0085:         #   h1  h2
0086:         #    \ /
0087:         #     y
0088:         x = Tensor([1.0], requires_grad=True)
0089:         w1 = Tensor([2.0], requires_grad=True)
0090:         w2 = Tensor([3.0], requires_grad=True)
0091:         
0092:         h1 = Multiply.apply(x, w1)
0093:         h2 = Multiply.apply(x, w2)
0094:         y = Add.apply(h1, h2)
0095:         
0096:         self.engine.backward(y, np.array([1.0]))
0097:         
0098:         # x's gradient should include effects from both paths
0099:         assert np.allclose(x.grad, [5.0])  # 2 + 3
0100:         assert np.allclose(w1.grad, [1.0])
0101:         assert np.allclose(w2.grad, [1.0])
0102: 
0103: class TestGradientAccumulation:
0104:     """Tests for correct gradient accumulation behavior."""
0105: 
0106:     def setup_method(self):
0107:         self.engine = get_autograd_engine()
0108:         self.engine.clear()
0109: 
0110:     def test_reused_variable(self):
0111:         """Test gradient accumulation when a variable is used multiple times."""
0112:         x = Tensor([2.0], requires_grad=True)
0113:         
0114:         # Use x in three separate computations
0115:         y1 = Multiply.apply(x, Tensor([2.0]))
0116:         y2 = Multiply.apply(x, Tensor([3.0]))
0117:         y3 = Multiply.apply(x, Tensor([4.0]))
0118:         
0119:         # Backward on all three outputs
0120:         self.engine.backward(y1, np.array([1.0]))
0121:         self.engine.backward(y2, np.array([1.0]))
0122:         self.engine.backward(y3, np.array([1.0]))
0123:         
0124:         # Gradient should accumulate: 2 + 3 + 4 = 9
0125:         assert np.allclose(x.grad, [9.0])
0126: 
0127:     def test_shared_structure(self):
0128:         """Test gradient computation with shared subgraphs."""
0129:         # Create a computation where the same subgraph is used multiple times
0130:         x = Tensor([1.0], requires_grad=True)
0131:         y = Tensor([2.0], requires_grad=True)
0132:         
0133:         # Shared computation
0134:         shared = Multiply.apply(x, y)
0135:         
0136:         # Use shared result multiple times
0137:         out1 = Multiply.apply(shared, Tensor([2.0]))
0138:         out2 = Multiply.apply(shared, Tensor([3.0]))
0139:         
0140:         # Final sum
0141:         final = Add.apply(out1, out2)
0142:         
0143:         self.engine.backward(final, np.array([1.0]))
0144:         
0145:         # Verify gradients include effects from all paths
0146:         assert x.grad is not None
0147:         assert y.grad is not None
0148: 
0149: class TestAdvancedAutogradFeatures:
0150:     """Tests for advanced AutogradEngine features and edge cases"""
0151:     
0152:     def setup_method(self):
0153:         self.engine = get_autograd_engine()
0154:         self.engine.clear()
0155: 
0156:     def test_validate_graph(self):
0157:         """Test graph validation functionality"""
0158:         # Create a disconnected subgraph
0159:         x = Tensor([1.0], requires_grad=True)
0160:         y = Tensor([2.0], requires_grad=True)
0161:         _ = Add.apply(x, y)
0162:         
0163:         # Add an isolated node
0164:         z = Tensor([3.0], requires_grad=True)
0165:         self.engine.register_tensor(z)
0166:         
0167:         warnings = self.engine.validate_graph()
0168:         assert len(warnings) > 0
0169:         assert "isolated nodes" in warnings[0]  
0170: 
0171:     def test_nested_gradient_computation(self):
0172:         """Test detection of nested gradient computations"""
0173:         x = Tensor([1.0], requires_grad=True)
0174:         y = Add.apply(x, Tensor([2.0]))
0175:         
0176:         # Simulate nested gradient computation
0177:         self.engine._currently_computing_gradients = True
0178:         with pytest.raises(RuntimeError, match="Nested gradient computation detected"):
0179:             self.engine.backward(y)
0180:         self.engine._currently_computing_gradients = False
0181: 
0182:     def test_cyclic_graph_detection(self):
0183:         """Test detection of cycles in computational graph"""
0184:         x = Tensor([1.0], requires_grad=True)
0185:         y = Tensor([2.0], requires_grad=True)
0186:         
0187:         # Manually create a cycle in the graph
0188:         node_x = self.engine._nodes[id(x)]
0189:         node_y = self.engine._nodes[id(y)]
0190:         
0191:         edge1 = Edge(node_x, node_y)
0192:         edge2 = Edge(node_y, node_x)
0193:         
0194:         node_x.out_edges.append(edge1)
0195:         node_y.in_edges.append(edge1)
0196:         node_y.out_edges.append(edge2)
0197:         node_x.in_edges.append(edge2)
0198:         
0199:         with pytest.raises(RuntimeError, match="Cycle detected in computation graph"):
0200:             self.engine.backward(x)
0201: 
0202:     def test_gradient_shape_mismatch(self):
0203:         """Test detection of gradient shape mismatches"""
0204:         x = Tensor([[1.0]], requires_grad=True)  # Shape (1, 1)
0205:         y = Tensor([2.0], requires_grad=True)    # Shape (1,)
0206:         
0207:         # Create edge with obviously wrong shape
0208:         node_x = self.engine._nodes[id(x)]
0209:         node_y = self.engine._nodes[id(y)]
0210:         
0211:         edge = Edge(node_x, node_y)
0212:         edge.grad = np.array([[1.0, 2.0]])  # Wrong shape (1, 2)
0213:         
0214:         # Add the edge to both nodes and the engine
0215:         node_x.out_edges.append(edge)
0216:         node_y.in_edges.append(edge)
0217:         self.engine._edges.add(edge)
0218:         
0219:         warnings = self.engine.validate_graph()
0220:         assert any("shape mismatch" in w for w in warnings), "Should detect shape mismatch"

// File: C:\Users\aluja\Desktop\DLpy\tests\test_cnn.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: from DLpy.nn import Conv2d
0005: from DLpy.ops.cnn import (
0006:     Conv2dFunction, ConvMode,
0007:     _compute_conv_output_shape,
0008:     _unfold,
0009:     _fold,
0010:     _bilinear_interpolate,
0011:     _generate_grid,
0012:     _deform_grid
0013: )
0014: 
0015: class TestConv2d:
0016:     """Tests for Conv2d module."""
0017:     
0018:     # [Previous tests remain the same...]
0019: 
0020:     def test_conv2d_asymmetric_kernel(self):
0021:         """Test Conv2d with asymmetric kernel."""
0022:         batch_size = 2
0023:         in_channels = 3
0024:         out_channels = 16
0025:         height = width = 32
0026:         kernel_size = (3, 5)  # Asymmetric kernel
0027:         
0028:         conv = Conv2d(in_channels, out_channels, kernel_size)
0029:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0030:         output = conv(x)
0031:         
0032:         # Check output shape
0033:         expected_height = height - kernel_size[0] + 1
0034:         expected_width = width - kernel_size[1] + 1
0035:         assert output.shape == (batch_size, out_channels, expected_height, expected_width)
0036: 
0037:     def test_conv2d_asymmetric_stride(self):
0038:         """Test Conv2d with different strides for height and width."""
0039:         batch_size = 2
0040:         in_channels = 3
0041:         out_channels = 16
0042:         height = width = 32
0043:         kernel_size = 3
0044:         stride = (2, 3)  # Different strides
0045:         
0046:         conv = Conv2d(in_channels, out_channels, kernel_size, stride=stride)
0047:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0048:         output = conv(x)
0049:         
0050:         expected_height = (height - kernel_size) // stride[0] + 1
0051:         expected_width = (width - kernel_size) // stride[1] + 1
0052:         assert output.shape == (batch_size, out_channels, expected_height, expected_width)
0053: 
0054:     def test_conv2d_asymmetric_padding(self):
0055:         """Test Conv2d with different padding for height and width."""
0056:         batch_size = 2
0057:         in_channels = 3
0058:         out_channels = 16
0059:         height = width = 32
0060:         kernel_size = 3
0061:         padding = (1, 2)  # Different padding
0062:         
0063:         conv = Conv2d(in_channels, out_channels, kernel_size, padding=padding)
0064:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0065:         output = conv(x)
0066:         
0067:         expected_height = height + 2*padding[0] - kernel_size + 1
0068:         expected_width = width + 2*padding[1] - kernel_size + 1
0069:         assert output.shape == (batch_size, out_channels, expected_height, expected_width)
0070: 
0071:     def test_conv2d_dilated(self):
0072:         """Test dilated convolution."""
0073:         batch_size = 2
0074:         in_channels = 3
0075:         out_channels = 16
0076:         height = width = 32
0077:         kernel_size = 3
0078:         dilation = 2
0079:         
0080:         conv = Conv2d(in_channels, out_channels, kernel_size, dilation=dilation)
0081:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0082:         output = conv(x)
0083:         
0084:         effective_kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)
0085:         expected_height = height - effective_kernel_size + 1
0086:         expected_width = width - effective_kernel_size + 1
0087:         assert output.shape == (batch_size, out_channels, expected_height, expected_width)
0088: 
0089:     def test_conv2d_groups(self):
0090:         """Test grouped convolution."""
0091:         batch_size = 2
0092:         in_channels = 4
0093:         out_channels = 4
0094:         height = width = 32
0095:         kernel_size = 3
0096:         groups = 2
0097:         
0098:         conv = Conv2d(in_channels, out_channels, kernel_size, groups=groups)
0099:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0100:         output = conv(x)
0101:         
0102:         expected_height = height - kernel_size + 1
0103:         expected_width = width - kernel_size + 1
0104:         assert output.shape == (batch_size, out_channels, expected_height, expected_width)
0105: 
0106: class TestConv2dFunction:
0107:     """Tests for Conv2dFunction and related helper functions."""
0108:     
0109:     def test_deformable_conv_forward(self):
0110:         """Test forward pass of deformable convolution."""
0111:         batch_size = 2
0112:         in_channels = 3
0113:         out_channels = 16
0114:         height = width = 8
0115:         kernel_size = 3
0116:         
0117:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0118:         weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size))
0119:         offset = Tensor(np.random.randn(batch_size, 2*kernel_size*kernel_size, height-2, width-2))
0120:         bias = Tensor(np.random.randn(out_channels))
0121:         
0122:         output = Conv2dFunction.apply(x, weight, bias, (1, 1), (0, 0), (1, 1), 1,
0123:                                     ConvMode.DEFORMABLE, offset)
0124: 
0125:     def test_modulated_deform_conv(self):
0126:         """Test modulated deformable convolution."""
0127:         batch_size = 2
0128:         in_channels = 3
0129:         out_channels = 16
0130:         height = width = 8
0131:         kernel_size = 3
0132:         
0133:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0134:         weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size))
0135:         offset = Tensor(np.random.randn(batch_size, 2*kernel_size*kernel_size, height-2, width-2))
0136:         mask = Tensor(np.random.randn(batch_size, kernel_size*kernel_size, height-2, width-2))
0137:         bias = Tensor(np.random.randn(out_channels))
0138:         
0139:         output = Conv2dFunction.apply(x, weight, bias, (1, 1), (0, 0), (1, 1), 1,
0140:                                     ConvMode.DEFORMABLE, offset, mask)
0141: 
0142: class TestHelperFunctions:
0143:     """Tests for CNN helper functions."""
0144:     
0145:     def test_compute_output_shape(self):
0146:         """Test output shape computation."""
0147:         input_size = 32
0148:         kernel_size = 3
0149:         stride = 1
0150:         padding = 0
0151:         dilation = 1
0152:         
0153:         output_size = _compute_conv_output_shape(
0154:             input_size, kernel_size, stride, padding, dilation
0155:         )
0156:         assert output_size == 30  # 32 - 3 + 1
0157: 
0158:     def test_unfold_operation(self):
0159:         """Test im2col (unfold) operation."""
0160:         batch_size = 2
0161:         channels = 3
0162:         height = width = 8
0163:         kernel_size = (3, 3)
0164:         
0165:         input_tensor = np.random.randn(batch_size, channels, height, width)
0166:         unfolded = _unfold(input_tensor, kernel_size, (1, 1), (0, 0), (1, 1))
0167:         
0168:         # Check unfolded shape
0169:         expected_unfold_shape = (channels * kernel_size[0] * kernel_size[1],
0170:                                batch_size * (height - kernel_size[0] + 1) * 
0171:                                (width - kernel_size[1] + 1))
0172:         assert unfolded.shape == expected_unfold_shape
0173: 
0174:     def test_fold_operation(self):
0175:         """Test col2im (fold) operation."""
0176:         batch_size = 2
0177:         channels = 3
0178:         height = width = 8
0179:         kernel_size = (3, 3)
0180:         
0181:         # Create random input and unfold it
0182:         input_tensor = np.random.randn(batch_size, channels, height, width)
0183:         unfolded = _unfold(input_tensor, kernel_size, (1, 1), (0, 0), (1, 1))
0184:         
0185:         # Fold back
0186:         folded = _fold(unfolded, (height, width), kernel_size, (1, 1), (0, 0), (1, 1))
0187:         
0188:         # Check folded shape
0189:         assert folded.shape == input_tensor.shape
0190: 
0191:     def test_bilinear_interpolation(self):
0192:         """Test bilinear interpolation."""
0193:         batch_size = 2
0194:         channels = 3
0195:         height = width = 8
0196:         
0197:         input_tensor = np.random.randn(batch_size, channels, height, width)
0198:         points = np.random.uniform(-1, 1, (batch_size, 4, 2))  # Sample 4 points
0199:         
0200:         interpolated = _bilinear_interpolate(input_tensor, points)
0201:         assert interpolated.shape == (batch_size, channels, 4)
0202: 
0203:     def test_generate_grid(self):
0204:         """Test sampling grid generation."""
0205:         batch_size = 2
0206:         height = 8
0207:         width = 8
0208:         
0209:         grid = _generate_grid(batch_size, height, width)
0210:         assert grid.shape == (batch_size, height, width, 2)
0211:         assert np.all(grid >= -1) and np.all(grid <= 1)
0212: 
0213:     def test_deform_grid(self):
0214:         """Test grid deformation."""
0215:         batch_size = 2
0216:         height = 8
0217:         width = 8
0218:         
0219:         grid = _generate_grid(batch_size, height, width)
0220:         offset = np.random.randn(batch_size, 2, height, width) * 0.1
0221:         
0222:         deformed = _deform_grid(grid, offset)
0223:         assert deformed.shape == (batch_size, height, width, 2)
0224:         assert np.all(deformed >= -1) and np.all(deformed <= 1)
0225: 
0226:     def test_numerical_gradient_deformable(self):
0227:         """Test numerical gradient computation for deformable convolution."""
0228:         batch_size = 2
0229:         in_channels = 2
0230:         out_channels = 3
0231:         height = width = 5
0232:         kernel_size = 3
0233:         
0234:         x = Tensor(np.random.randn(batch_size, in_channels, height, width), requires_grad=True)
0235:         weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size), 
0236:                        requires_grad=True)
0237:         offset = Tensor(np.random.randn(batch_size, 2*kernel_size*kernel_size, height-2, width-2), 
0238:                        requires_grad=True)
0239:         setattr(weight, 'offset', offset)
0240:         bias = Tensor(np.random.randn(out_channels), requires_grad=True)
0241:         
0242:         def compute_loss(x, w, b):
0243:             return np.sum(Conv2dFunction.apply(x, w, b, (1, 1), (0, 0), (1, 1), 1, 
0244:                                              ConvMode.DEFORMABLE).data)
0245:         
0246:         # Compute analytical gradients
0247:         output = Conv2dFunction.apply(x, weight, bias, (1, 1), (0, 0), (1, 1), 1, 
0248:                                     ConvMode.DEFORMABLE)
0249:         output.backward(np.ones_like(output.data))
0250:         
0251:         # Verify offset gradients exist and have correct shape
0252:         assert offset.grad is not None
0253:         assert offset.grad.shape == offset.shape

// File: C:\Users\aluja\Desktop\DLpy\tests\test_context.py
// ----------------------------------------
0001: import pytest
0002: from DLpy.core import Context, Tensor
0003: import numpy as np
0004: 
0005: class TestContext:
0006:     """Tests for Context class functionality"""
0007:     
0008:     def test_save_and_retrieve_tensors(self):
0009:         """Test saving and retrieving tensors"""
0010:         ctx = Context()
0011:         tensor1 = Tensor([1.0])
0012:         tensor2 = Tensor([2.0])
0013:         
0014:         ctx.save_for_backward(tensor1, tensor2)
0015:         saved = ctx.saved_tensors
0016:         
0017:         assert len(saved) == 2
0018:         assert np.array_equal(saved[0].data, tensor1.data)
0019:         assert np.array_equal(saved[1].data, tensor2.data)
0020: 
0021:     def test_save_and_retrieve_arguments(self):
0022:         """Test saving and retrieving non-tensor arguments"""
0023:         ctx = Context()
0024:         ctx.save_arguments(arg1="test", arg2=42)
0025:         
0026:         args = ctx.saved_arguments
0027:         assert args["arg1"] == "test"
0028:         assert args["arg2"] == 42
0029:         assert isinstance(args, dict)
0030: 
0031:     def test_intermediate_values(self):
0032:         """Test storing and retrieving intermediate values"""
0033:         ctx = Context()
0034:         
0035:         # Store various types of values
0036:         ctx.store_intermediate("scalar", 42)
0037:         ctx.store_intermediate("list", [1, 2, 3])
0038:         ctx.store_intermediate("tensor", Tensor([1.0]))
0039:         
0040:         # Retrieve and verify values
0041:         assert ctx.get_intermediate("scalar") == 42
0042:         assert ctx.get_intermediate("list") == [1, 2, 3]
0043:         assert isinstance(ctx.get_intermediate("tensor"), Tensor)
0044:         
0045:         # Test retrieving non-existent key
0046:         with pytest.raises(KeyError):
0047:             ctx.get_intermediate("nonexistent")
0048: 
0049:     def test_clear_functionality(self):
0050:         """Test clearing all stored data"""
0051:         ctx = Context()
0052:         
0053:         # Store various types of data
0054:         ctx.save_for_backward(Tensor([1.0]))
0055:         ctx.save_arguments(arg1="test")
0056:         ctx.store_intermediate("key", "value")
0057:         
0058:         # Clear all data
0059:         ctx.clear()
0060:         
0061:         # Verify everything is cleared
0062:         assert len(ctx._saved_tensors) == 0
0063:         assert len(ctx._non_tensor_args) == 0
0064:         assert len(ctx._intermediate_values) == 0

// File: C:\Users\aluja\Desktop\DLpy\tests\test_function.py
// ----------------------------------------
0001: import pytest
0002: from DLpy.core import Function, Tensor
0003: import numpy as np
0004: 
0005: class TestFunction:
0006:     """Tests for Function base class and utilities"""
0007:     
0008:     class TestFunction(Function):
0009:         """Simple test function implementation"""
0010:         
0011:         @staticmethod
0012:         def forward(ctx, x, y=None):
0013:             ctx.save_for_backward(x)
0014:             ctx.save_arguments(y=y)
0015:             return Tensor(x.data * 2)
0016:             
0017:         @staticmethod
0018:         def backward(ctx, grad_output, grad_dict):
0019:             x, = ctx.saved_tensors
0020:             y = ctx.saved_arguments["y"]
0021:             
0022:             if x.requires_grad:
0023:                 grad_dict[id(x)] = grad_output * 2
0024: 
0025:     def test_function_application(self):
0026:         """Test applying a function to inputs"""
0027:         x = Tensor([1.0], requires_grad=True)
0028:         result = self.TestFunction.apply(x, y=2.0)
0029:         
0030:         assert isinstance(result, Tensor)
0031:         assert np.array_equal(result.data, [2.0])
0032:         assert result.requires_grad
0033:         assert result._backward_fn is not None
0034: 
0035:     def test_verify_backward(self):
0036:         """Test gradient verification utility"""
0037:         def forward_fn(x):
0038:             return x * 2
0039:             
0040:         def correct_backward_fn(ctx, grad_output):
0041:             return grad_output * 2
0042:             
0043:         def incorrect_backward_fn(ctx, grad_output):
0044:             return grad_output * 3
0045:         
0046:         # Test with correct gradients
0047:         x = np.array([1.0])
0048:         assert Function.verify_backward(forward_fn, correct_backward_fn, (x,))
0049:         
0050:         # Test with incorrect gradients
0051:         assert not Function.verify_backward(forward_fn, incorrect_backward_fn, (x,))
0052: 
0053:     def test_abstract_methods(self):
0054:         """Test that abstract methods raise NotImplementedError"""
0055:         
0056:         class IncompleteFunction(Function):
0057:             pass
0058:             
0059:         with pytest.raises(TypeError):
0060:             IncompleteFunction()

// File: C:\Users\aluja\Desktop\DLpy\tests\test_loss.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: from DLpy.ops.loss import (
0005:     MSELoss,
0006:     CrossEntropyLoss,
0007:     BinaryCrossEntropyLoss,
0008:     L1Loss,
0009:     HuberLoss,
0010:     KLDivLoss,
0011:     CosineSimilarityLoss,
0012:     HingeLoss,
0013:     FocalLoss
0014: )
0015: 
0016: class TestMSELoss:
0017:     """Tests for Mean Squared Error Loss"""
0018:     
0019:     def test_forward(self):
0020:         """Test forward pass of MSE loss"""
0021:         predictions = Tensor([[1.0, 2.0], [3.0, 4.0]])
0022:         targets = Tensor([[2.0, 1.0], [4.0, 3.0]])
0023:         
0024:         # Test mean reduction
0025:         loss = MSELoss.apply(predictions, targets, 'mean')
0026:         expected = np.mean((predictions.data - targets.data) ** 2)
0027:         assert np.allclose(loss.data, expected)
0028:         
0029:         # Test sum reduction
0030:         loss = MSELoss.apply(predictions, targets, 'sum')
0031:         expected = np.sum((predictions.data - targets.data) ** 2)
0032:         assert np.allclose(loss.data, expected)
0033:         
0034:     def test_backward(self):
0035:         """Test backward pass of MSE loss"""
0036:         predictions = Tensor([[1.0]], requires_grad=True)
0037:         targets = Tensor([[2.0]])
0038:         
0039:         loss = MSELoss.apply(predictions, targets, 'mean')
0040:         loss.backward()
0041:         
0042:         # For MSE, gradient should be 2(pred - target)/N
0043:         expected_grad = 2 * (predictions.data - targets.data) / np.prod(predictions.shape)
0044:         assert np.allclose(predictions.grad, expected_grad)
0045: 
0046: class TestCrossEntropyLoss:
0047:     """Tests for Cross Entropy Loss"""
0048:     
0049:     def test_forward(self):
0050:         """Test forward pass of cross entropy loss"""
0051:         predictions = Tensor([[1.0, 0.0], [0.0, 1.0]])
0052:         targets = Tensor([[1.0, 0.0], [0.0, 1.0]])  # One-hot encoded
0053:         
0054:         loss = CrossEntropyLoss.apply(predictions, targets)
0055:         assert loss.data >= 0  # Loss should be non-negative
0056:         
0057:     def test_numerical_stability(self):
0058:         """Test numerical stability with large inputs"""
0059:         predictions = Tensor([[1000., -1000.], [-1000., 1000.]])
0060:         targets = Tensor([[1., 0.], [0., 1.]])
0061:         
0062:         loss = CrossEntropyLoss.apply(predictions, targets)
0063:         assert not np.isnan(loss.data)
0064:         assert not np.isinf(loss.data)
0065:         
0066:     def test_gradient(self):
0067:         """Test gradient computation"""
0068:         predictions = Tensor([[1.0, 0.0]], requires_grad=True)
0069:         targets = Tensor([[1.0, 0.0]])
0070:         
0071:         loss = CrossEntropyLoss.apply(predictions, targets)
0072:         loss.backward()
0073:         
0074:         assert predictions.grad is not None
0075:         assert not np.isnan(predictions.grad).any()
0076:         assert not np.isinf(predictions.grad).any()
0077: 
0078: class TestBinaryCrossEntropyLoss:
0079:     """Tests for Binary Cross Entropy Loss"""
0080:     
0081:     def test_forward(self):
0082:         """Test forward pass of binary cross entropy loss"""
0083:         predictions = Tensor([0.7, 0.3])
0084:         targets = Tensor([1.0, 0.0])
0085:         
0086:         loss = BinaryCrossEntropyLoss.apply(predictions, targets)
0087:         assert loss.data >= 0  # Loss should be non-negative
0088:         
0089:     def test_gradient(self):
0090:         """Test gradient computation"""
0091:         predictions = Tensor([0.7], requires_grad=True)
0092:         targets = Tensor([1.0])
0093:         
0094:         loss = BinaryCrossEntropyLoss.apply(predictions, targets)
0095:         loss.backward()
0096:         
0097:         assert predictions.grad is not None
0098:         assert not np.isnan(predictions.grad).any()
0099:         
0100:     def test_reductions(self):
0101:         """Test different reduction methods"""
0102:         predictions = Tensor([[0.7, 0.3], [0.2, 0.8]])
0103:         targets = Tensor([[1.0, 0.0], [0.0, 1.0]])
0104:         
0105:         loss_none = BinaryCrossEntropyLoss.apply(predictions, targets, 'none')
0106:         loss_mean = BinaryCrossEntropyLoss.apply(predictions, targets, 'mean')
0107:         loss_sum = BinaryCrossEntropyLoss.apply(predictions, targets, 'sum')
0108:         
0109:         assert loss_none.shape == predictions.shape
0110:         # Check if scalar by ensuring it's a 0-dimensional array or float
0111:         assert loss_mean.data.ndim == 0 or isinstance(loss_mean.data, float)
0112:         assert loss_sum.data.ndim == 0 or isinstance(loss_sum.data, float)
0113: 
0114: class TestL1Loss:
0115:     """Tests for L1 Loss"""
0116:     
0117:     def test_forward(self):
0118:         """Test forward pass of L1 loss"""
0119:         predictions = Tensor([[1.0, 2.0], [3.0, 4.0]])
0120:         targets = Tensor([[2.0, 1.0], [4.0, 3.0]])
0121:         
0122:         loss = L1Loss.apply(predictions, targets)
0123:         expected = np.mean(np.abs(predictions.data - targets.data))
0124:         assert np.allclose(loss.data, expected)
0125:         
0126:     def test_backward(self):
0127:         """Test backward pass of L1 loss"""
0128:         predictions = Tensor([1.0], requires_grad=True)
0129:         targets = Tensor([2.0])
0130:         
0131:         loss = L1Loss.apply(predictions, targets)
0132:         loss.backward()
0133:         
0134:         # Gradient should be sign(pred - target)
0135:         expected_grad = np.sign(predictions.data - targets.data)
0136:         assert np.allclose(predictions.grad, expected_grad)
0137: 
0138: class TestHuberLoss:
0139:     """Tests for Huber Loss"""
0140:     
0141:     def test_forward(self):
0142:         """Test forward pass of Huber loss"""
0143:         predictions = Tensor([1.0, 2.0])
0144:         targets = Tensor([0.0, 4.0])
0145:         delta = 1.0
0146:         
0147:         loss = HuberLoss.apply(predictions, targets, delta)
0148:         
0149:         # Manually calculate expected loss
0150:         diff = predictions.data - targets.data
0151:         expected = np.mean(np.where(np.abs(diff) <= delta,
0152:                                   0.5 * diff ** 2,
0153:                                   delta * np.abs(diff) - 0.5 * delta ** 2))
0154:         
0155:         assert np.allclose(loss.data, expected)
0156:         
0157:     def test_backward(self):
0158:         """Test backward pass of Huber loss"""
0159:         predictions = Tensor([0.0], requires_grad=True)
0160:         targets = Tensor([2.0])
0161:         delta = 1.0
0162:         
0163:         loss = HuberLoss.apply(predictions, targets, delta)
0164:         loss.backward()
0165:         
0166:         assert predictions.grad is not None
0167:         assert not np.isnan(predictions.grad).any()
0168: 
0169: class TestKLDivLoss:
0170:     """Tests for KL Divergence Loss"""
0171:     
0172:     def test_forward(self):
0173:         """Test forward pass of KL divergence loss"""
0174:         predictions = Tensor([[0.5, 0.5]])
0175:         targets = Tensor([[0.8, 0.2]])
0176:         
0177:         loss = KLDivLoss.apply(predictions, targets)
0178:         assert loss.data >= 0  # KL divergence is always non-negative
0179:         
0180:     def test_numerical_stability(self):
0181:         """Test numerical stability with small probabilities"""
0182:         predictions = Tensor([[0.999, 0.001]])
0183:         targets = Tensor([[0.001, 0.999]])
0184:         
0185:         loss = KLDivLoss.apply(predictions, targets)
0186:         assert not np.isnan(loss.data)
0187:         assert not np.isinf(loss.data)
0188: 
0189: class TestCosineSimilarityLoss:
0190:     """Tests for Cosine Similarity Loss"""
0191:     
0192:     def test_forward(self):
0193:         """Test forward pass of cosine similarity loss"""
0194:         x1 = Tensor([[1.0, 0.0]])
0195:         x2 = Tensor([[0.0, 1.0]])
0196:         
0197:         loss = CosineSimilarityLoss.apply(x1, x2)
0198:         # Orthogonal vectors should have cos_sim = 0, so loss = 1 - 0 = 1
0199:         assert np.allclose(loss.data, 1.0), f"Expected 1.0, got {loss.data}"
0200:         
0201:     def test_identical_vectors(self):
0202:         """Test with identical vectors"""
0203:         x = Tensor([[1.0, 1.0]])
0204:         loss = CosineSimilarityLoss.apply(x, x)
0205:         # For identical vectors, cosine similarity is 1, so loss = 1 - 1 = 0
0206:         assert np.allclose(loss.data, 0.0, atol=1e-7)
0207:         
0208:     def test_numerical_stability(self):
0209:         """Test numerical stability with zero vectors"""
0210:         x1 = Tensor([[0.0, 0.0]])
0211:         x2 = Tensor([[1.0, 1.0]])
0212:         
0213:         loss = CosineSimilarityLoss.apply(x1, x2)
0214:         assert not np.isnan(loss.data)
0215: 
0216: class TestHingeLoss:
0217:     """Tests for Hinge Loss"""
0218:     
0219:     def test_forward(self):
0220:         """Test forward pass of hinge loss"""
0221:         predictions = Tensor([0.5, -0.5])
0222:         targets = Tensor([1.0, 0.0])
0223:         
0224:         loss = HingeLoss.apply(predictions, targets)
0225:         assert loss.data >= 0  # Hinge loss is non-negative
0226:         
0227:     def test_perfect_prediction(self):
0228:         """Test with perfect predictions"""
0229:         predictions = Tensor([1.0])
0230:         targets = Tensor([1.0])
0231:         
0232:         loss = HingeLoss.apply(predictions, targets)
0233:         assert np.allclose(loss.data, 0.0)  # Loss should be zero
0234:         
0235:     def test_margin(self):
0236:         """Test different margin values"""
0237:         predictions = Tensor([0.5])
0238:         targets = Tensor([1.0])
0239:         
0240:         loss1 = HingeLoss.apply(predictions, targets, margin=1.0)
0241:         loss2 = HingeLoss.apply(predictions, targets, margin=2.0)
0242:         assert loss2.data > loss1.data  # Larger margin should give larger loss
0243: 
0244: class TestFocalLoss:
0245:     """Tests for Focal Loss"""
0246:     
0247:     def test_forward(self):
0248:         """Test forward pass of focal loss"""
0249:         predictions = Tensor([0.7, 0.3])
0250:         targets = Tensor([1.0, 0.0])
0251:         
0252:         loss = FocalLoss.apply(predictions, targets)
0253:         assert loss.data >= 0  # Focal loss is non-negative
0254:         
0255:     def test_gamma_effect(self):
0256:         """Test effect of gamma parameter"""
0257:         predictions = Tensor([0.7])
0258:         targets = Tensor([1.0])
0259:         
0260:         loss1 = FocalLoss.apply(predictions, targets, gamma=0.0)  # Equivalent to BCE
0261:         loss2 = FocalLoss.apply(predictions, targets, gamma=2.0)  # Standard focal loss
0262:         
0263:         # Focal loss should be smaller than BCE for easy examples
0264:         assert loss2.data < loss1.data
0265:         
0266:     def test_numerical_stability(self):
0267:         """Test numerical stability with extreme probabilities"""
0268:         predictions = Tensor([0.999, 0.001])
0269:         targets = Tensor([1.0, 0.0])
0270:         
0271:         loss = FocalLoss.apply(predictions, targets)
0272:         assert not np.isnan(loss.data)
0273:         assert not np.isinf(loss.data)
0274: 
0275: class TestEdgeCases:
0276:     """Tests for edge cases and error conditions"""
0277:     
0278:     def test_shape_mismatch(self):
0279:         """Test shape mismatch handling"""
0280:         predictions = Tensor([[1.0, 2.0]])
0281:         targets = Tensor([1.0])
0282:         
0283:         with pytest.raises(ValueError):
0284:             MSELoss.apply(predictions, targets)
0285:             
0286:     def test_invalid_reduction(self):
0287:         """Test invalid reduction method"""
0288:         predictions = Tensor([1.0])
0289:         targets = Tensor([1.0])
0290:         
0291:         with pytest.raises(ValueError):
0292:             MSELoss.apply(predictions, targets, reduction='invalid')
0293:             
0294:     def test_negative_probabilities(self):
0295:         """Test handling of negative probabilities"""
0296:         predictions = Tensor([-0.1, 1.1])
0297:         targets = Tensor([0.0, 1.0])
0298:         
0299:         with pytest.raises(ValueError):
0300:             BinaryCrossEntropyLoss.apply(predictions, targets)

// File: C:\Users\aluja\Desktop\DLpy\tests\test_modules.py
// ----------------------------------------
0001: import pytest
0002: from DLpy.nn.modules import Module
0003: from DLpy.core import Tensor
0004: import numpy as np
0005: from DLpy.nn.linear import Linear  
0006: 
0007: class TestModuleEdgeCases:
0008:     """Tests for edge cases in Module functionality"""
0009:     
0010:     def test_premature_parameter_registration(self):
0011:         """Test parameter registration before initialization"""
0012:         with pytest.raises(TypeError):
0013:             class BadModule(Module):
0014:                 def __init__(self):
0015:                     self.param = Tensor([1.0])  # Before super().__init__()
0016:             BadModule()
0017: 
0018:     def test_invalid_module_addition(self):
0019:         """Test adding invalid modules"""
0020:         module = Module()
0021:         
0022:         # Test adding None module
0023:         module.add_module('none_module', None)
0024:         assert module._modules['none_module'] is None
0025:         
0026:         # Test adding invalid type
0027:         with pytest.raises(TypeError):
0028:             module.add_module('invalid', "not a module")
0029:             
0030:         # Test adding before initialization
0031:         with pytest.raises(TypeError):
0032:             class BadModule(Module):
0033:                 def __init__(self):
0034:                     self.add_module('test', Module())  # Before super().__init__()
0035:             BadModule()
0036: 
0037:     def test_attribute_access(self):
0038:         """Test attribute access edge cases"""
0039:         # Test accessing non-existent attribute
0040:         module = Module()
0041:         with pytest.raises(AttributeError):
0042:             _ = module.nonexistent_attr
0043:         
0044:         # Test accessing training attribute before initialization
0045:         class BadModule(Module):
0046:             def __init__(self):
0047:                 # Access training before super().__init__()
0048:                 try:
0049:                     _ = self._parameters
0050:                 except AttributeError:
0051:                     pass  # Expected
0052:                     
0053:                 # Now try to get the training attribute which should fail
0054:                 _ = self.training
0055:                 super().__init__()
0056:                 
0057:         with pytest.raises(AttributeError):
0058:             BadModule()
0059: 
0060:     def test_module_buffer_operations(self):
0061:         """Test buffer operations in detail"""
0062:         class TestModule(Module):
0063:             def __init__(self):
0064:                 super().__init__()
0065:                 self.register_buffer('running_mean', Tensor([0.0]))
0066:                 self.register_buffer('running_var', None)
0067:                 
0068:         module = TestModule()
0069:         assert 'running_mean' in module._buffers
0070:         assert module._buffers['running_var'] is None
0071:         
0072:         # Test buffer replacement
0073:         module.register_buffer('running_mean', Tensor([1.0]))
0074:         assert np.array_equal(module._buffers['running_mean'].data, [1.0])
0075: 
0076:     def test_module_state_dict(self):
0077:         """Test state dict functionality"""
0078:         class ComplexModule(Module):
0079:             def __init__(self):
0080:                 super().__init__()
0081:                 self.linear = Linear(2, 2)
0082:                 self.register_buffer('running_stats', Tensor([0.0]))
0083:                 
0084:         module = ComplexModule()
0085:         # Test parameter access
0086:         params = dict(module.named_parameters())
0087:         assert 'linear.weight' in params
0088:         assert 'linear.bias' in params
0089: 
0090:     def test_nested_module_training(self):
0091:         """Test training mode propagation in nested modules"""
0092:         class NestedModule(Module):
0093:             def __init__(self):
0094:                 super().__init__()
0095:                 self.sub1 = Linear(2, 2)
0096:                 self.sub2 = Linear(2, 2)
0097:                 
0098:         module = NestedModule()
0099:         module.train(False)
0100:         assert not module.training
0101:         assert not module.sub1.training
0102:         assert not module.sub2.training

// File: C:\Users\aluja\Desktop\DLpy\tests\test_nn.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: from DLpy.nn.linear import Linear
0005: from DLpy.nn.modules import Module
0006: 
0007: 
0008: class TestLinearLayer:
0009:     """Test suite for the Linear layer implementation."""
0010:     
0011:     def test_linear_layer_creation(self):
0012:         """Test that linear layers are created with correct shapes and initialization."""
0013:         in_features, out_features = 5, 3
0014:         layer = Linear(in_features, out_features)
0015:         
0016:         # Test weight dimensions
0017:         assert layer.weight.shape == (in_features, out_features)
0018:         assert layer.weight.requires_grad
0019:         
0020:         # Test bias dimensions
0021:         assert layer.bias is not None
0022:         assert layer.bias.shape == (out_features,)
0023:         assert layer.bias.requires_grad
0024:         
0025:         # Test layer without bias
0026:         layer_no_bias = Linear(in_features, out_features, bias=False)
0027:         assert layer_no_bias.bias is None
0028:         
0029:     def test_linear_forward(self):
0030:         """Test the forward pass of the linear layer."""
0031:         # Create a simple linear layer with known weights for testing
0032:         layer = Linear(2, 3)
0033:         layer.weight.data = np.array([[1., 2., 3.], [4., 5., 6.]])
0034:         layer.bias.data = np.array([0.1, 0.2, 0.3])
0035:         
0036:         # Create input tensor
0037:         x = Tensor([[1., 2.]])  # Batch size 1, 2 features
0038:         
0039:         # Compute expected output manually
0040:         expected_output = np.array([[9.1, 12.2, 15.3]])  # (1×2) @ (2×3) + bias
0041:         
0042:         # Get actual output
0043:         output = layer(x)
0044:         
0045:         # Compare results
0046:         assert isinstance(output, Tensor)
0047:         assert output.shape == (1, 3)
0048:         assert np.allclose(output.data, expected_output)
0049:         
0050:     def test_linear_backward(self):
0051:         """Test the backward pass and gradient computation of the linear layer."""
0052:         # Create a layer with specific weights for testing
0053:         layer = Linear(2, 1)
0054:         layer.weight.data = np.array([[1.], [2.]])
0055:         layer.bias.data = np.array([0.])
0056:         
0057:         # Forward pass
0058:         x = Tensor([[1., 2.]], requires_grad=True)
0059:         output = layer(x)
0060:         
0061:         # Backward pass
0062:         output.backward(np.array([[1.]]))
0063:         
0064:         # Check input gradients
0065:         expected_input_grad = np.array([[1., 2.]])  # Gradient w.r.t input
0066:         assert np.allclose(x.grad, expected_input_grad)
0067:         
0068:         # Check weight gradients
0069:         expected_weight_grad = np.array([[1.], [2.]])  # Gradient w.r.t weights
0070:         assert np.allclose(layer.weight.grad, expected_weight_grad)
0071:         
0072:         # Check bias gradients
0073:         expected_bias_grad = np.array([1.])  # Gradient w.r.t bias
0074:         assert np.allclose(layer.bias.grad, expected_bias_grad)
0075:         
0076:     def test_linear_batch_processing(self):
0077:         """Test that the linear layer correctly handles batched inputs."""
0078:         layer = Linear(3, 2)
0079:         batch_size = 4
0080:         x = Tensor(np.random.randn(batch_size, 3))
0081:         
0082:         output = layer(x)
0083:         assert output.shape == (batch_size, 2)
0084:         
0085:     def test_weight_initialization(self):
0086:         """Test that weights are properly initialized using He initialization."""
0087:         in_features, out_features = 100, 100
0088:         layer = Linear(in_features, out_features)
0089:         
0090:         # Check if weights follow He initialization statistics
0091:         weights = layer.weight.data
0092:         mean = np.mean(weights)
0093:         std = np.std(weights)
0094:         
0095:         # He initialization should have mean ≈ 0 and std ≈ sqrt(2/in_features)
0096:         expected_std = np.sqrt(2.0 / in_features)
0097:         assert abs(mean) < 0.1  # Mean should be close to 0
0098:         assert abs(std - expected_std) < 0.1  # Std should be close to expected
0099: 
0100: 
0101: class TestModule:
0102:     """Test suite for the base Module class."""
0103:     
0104:     class SimpleModule(Module):
0105:         """A simple module for testing purposes."""
0106:         def __init__(self):
0107:             super().__init__()
0108:             self.linear1 = Linear(2, 3)
0109:             self.linear2 = Linear(3, 1)
0110:             self.register_buffer('running_mean', Tensor(np.zeros(3)))
0111:             
0112:         def forward(self, x):
0113:             x = self.linear1(x)
0114:             return self.linear2(x)
0115:     
0116:     def test_module_parameter_registration(self):
0117:         """Test that parameters are correctly registered and tracked."""
0118:         model = self.SimpleModule()
0119:         
0120:         # Count parameters
0121:         params = list(model.parameters())
0122:         assert len(params) == 4  # 2 weights + 2 biases
0123:         
0124:         # Check named parameters
0125:         named_params = dict(model.named_parameters())
0126:         assert 'linear1.weight' in named_params
0127:         assert 'linear1.bias' in named_params
0128:         assert 'linear2.weight' in named_params
0129:         assert 'linear2.bias' in named_params
0130:         
0131:     def test_module_buffer_registration(self):
0132:         """Test that buffers are correctly registered."""
0133:         model = self.SimpleModule()
0134:         assert 'running_mean' in model._buffers
0135:         assert isinstance(model._buffers['running_mean'], Tensor)
0136:         
0137:     def test_module_train_eval_modes(self):
0138:         """Test switching between training and evaluation modes."""
0139:         model = self.SimpleModule()
0140:         
0141:         # Test train mode
0142:         model.train()
0143:         assert model.training
0144:         assert model.linear1.training
0145:         assert model.linear2.training
0146:         
0147:         # Test eval mode
0148:         model.eval()
0149:         assert not model.training
0150:         assert not model.linear1.training
0151:         assert not model.linear2.training
0152:         
0153:     def test_module_repr(self):
0154:         """Test the string representation of modules."""
0155:         model = self.SimpleModule()
0156:         repr_str = repr(model)
0157:         
0158:         # Check that repr includes important information
0159:         assert 'SimpleModule' in repr_str
0160:         assert 'linear1' in repr_str
0161:         assert 'linear2' in repr_str
0162: 
0163: 
0164: class TestEndToEnd:
0165:     """End-to-end tests for neural network components."""
0166:     
0167:     def test_simple_network(self):
0168:         """Test a simple network with multiple layers."""
0169:         # Create a simple network
0170:         class SimpleNet(Module):
0171:             def __init__(self):
0172:                 super().__init__()
0173:                 self.linear1 = Linear(2, 3)
0174:                 self.linear2 = Linear(3, 1)
0175:                 
0176:             def forward(self, x):
0177:                 x = self.linear1(x)
0178:                 return self.linear2(x)
0179:         
0180:         # Create model and input
0181:         model = SimpleNet()
0182:         x = Tensor([[1., 2.]], requires_grad=True)
0183:         
0184:         # Forward pass
0185:         output = model(x)
0186:         assert output.shape == (1, 1)
0187:         
0188:         # Backward pass
0189:         output.backward(np.array([[1.]]))
0190:         
0191:         # Check that all parameters have gradients
0192:         for param in model.parameters():
0193:             assert param.grad is not None

// File: C:\Users\aluja\Desktop\DLpy\tests\test_ops.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: from DLpy.ops import (
0005:     Add, Multiply, Power, Divide, Log, Exp, Sum, Mean, Max, Transpose,
0006:     Greater, GreaterEqual, Less, LessEqual, Equal, NotEqual
0007: )
0008: 
0009: class TestBasicOps:
0010:     """Tests for basic arithmetic operations"""
0011:     
0012:     def test_add_edge_cases(self):
0013:         """Test edge cases for Add operation"""
0014:         # Test broadcasting
0015:         x = Tensor([[1.0]], requires_grad=True)
0016:         y = Tensor([1.0, 2.0], requires_grad=True)
0017:         
0018:         with pytest.raises(ValueError):
0019:             _ = Add.apply(x, y)
0020:         
0021:         # Test gradient accumulation
0022:         x = Tensor([1.0], requires_grad=True)
0023:         y = Tensor([2.0], requires_grad=True)
0024:         z = Add.apply(x, y)
0025:         z.backward()
0026:         
0027:         assert np.array_equal(x.grad, [1.0])
0028:         assert np.array_equal(y.grad, [1.0])
0029: 
0030:     def test_multiply_edge_cases(self):
0031:         """Test edge cases for Multiply operation"""
0032:         # Test scalar multiplication
0033:         x = Tensor([1.0], requires_grad=True)
0034:         y = Tensor(2.0, requires_grad=True)
0035:         z = Multiply.apply(x, y)
0036:         z.backward()
0037:         
0038:         assert np.array_equal(x.grad, [2.0])
0039:         assert np.array_equal(y.grad, [1.0])
0040:     
0041:     def test_add_broadcasting_complex(self):
0042:         """Test complex broadcasting scenarios in Add operation"""
0043:         # Test broadcasting with different dimensions
0044:         x = Tensor([[1.0]], requires_grad=True)
0045:         y = Tensor([1.0, 2.0, 3.0], requires_grad=True)
0046:         with pytest.raises(ValueError):
0047:             _ = x + y  # Incompatible shapes
0048:             
0049:         # Test broadcasting with scalar
0050:         x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
0051:         y = Tensor(2.0, requires_grad=True)
0052:         z = x + y
0053:         z.backward(np.ones_like(x.data))
0054:         assert np.sum(y.grad) == np.prod(x.shape)  # Sum of gradients equals number of elements
0055: 
0056:     def test_multiply_broadcasting_complex(self):
0057:         """Test complex broadcasting scenarios in Multiply operation"""
0058:         # Test scalar multiplication with matrix
0059:         x = Tensor(2.0, requires_grad=True)
0060:         y = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
0061:         z = x * y
0062:         z.backward(np.ones_like(y.data))
0063: 
0064:         # Check scalar gradient - should be sum of all elements in y
0065:         assert x.grad.shape == (1,)
0066:         assert np.allclose(x.grad, [10.0])  # sum([1,2,3,4])
0067: 
0068:         # Check matrix gradient - should be uniformly scaled by x
0069:         assert y.grad.shape == y.data.shape
0070:         assert np.allclose(y.grad, np.full_like(y.data, 2.0))
0071: 
0072:         # Test broadcasting matrix with different shapes
0073:         a = Tensor([[1.0], [2.0]], requires_grad=True)  # Shape: (2,1)
0074:         b = Tensor([[1.0, 2.0]], requires_grad=True)    # Shape: (1,2)
0075:         c = a * b  # Should broadcast to shape (2,2)
0076: 
0077:         assert c.data.shape == (2, 2)
0078:         expected = np.array([[1.0, 2.0], [2.0, 4.0]])
0079:         assert np.allclose(c.data, expected)
0080: 
0081:         # Test gradient propagation with broadcasting
0082:         c.backward(np.ones_like(c.data))
0083:         assert a.grad.shape == (2, 1)
0084:         assert b.grad.shape == (1, 2)
0085:         # Correct expected gradients
0086:         assert np.allclose(a.grad, np.array([[3.0], [3.0]]))  # Correct sum of gradients for each row
0087:         assert np.allclose(b.grad, np.array([[3.0, 3.0]]))    # Correct sum of gradients for each column
0088: 
0089:     def test_add_empty_tensors(self):
0090:         x = Tensor([], requires_grad=True)
0091:         y = Tensor([], requires_grad=True)
0092:         z = Add.apply(x, y)
0093:         assert z.shape == (0,)
0094:     
0095:     def test_multiply_empty_tensors(self):
0096:         x = Tensor([], requires_grad=True)
0097:         y = Tensor([], requires_grad=True)
0098:         z = Multiply.apply(x, y)
0099:         assert z.shape == (0,)
0100: 
0101: class TestPowerOperations:
0102:     """Tests for power and division operations"""
0103:     
0104:     def test_power_scalar(self):
0105:         """Test power operation with scalar exponent"""
0106:         x = Tensor([2.0, 3.0], requires_grad=True)
0107:         y = x ** 2
0108:         y.backward(np.array([1.0, 1.0]))
0109:         
0110:         assert np.allclose(y.data, [4.0, 9.0])
0111:         assert np.allclose(x.grad, [4.0, 6.0])  # d/dx(x^2) = 2x
0112:         
0113:     def test_power_negative(self):
0114:         """Test power operation with negative exponent"""
0115:         x = Tensor([2.0, 4.0], requires_grad=True)
0116:         y = x ** (-1)
0117:         y.backward(np.array([1.0, 1.0]))
0118:         
0119:         assert np.allclose(y.data, [0.5, 0.25])
0120:         assert np.allclose(x.grad, [-0.25, -0.0625])  # d/dx(x^-1) = -x^-2
0121:         
0122:     def test_division(self):
0123:         """Test division operation"""
0124:         x = Tensor([6.0, 8.0], requires_grad=True)
0125:         y = Tensor([2.0, 4.0], requires_grad=True)
0126:         z = x / y
0127:         z.backward(np.array([1.0, 1.0]))
0128:         
0129:         assert np.allclose(z.data, [3.0, 2.0])
0130:         assert np.allclose(x.grad, [0.5, 0.25])  # d/dx(x/y) = 1/y
0131:         assert np.allclose(y.grad, [-1.5, -0.5])  # d/dy(x/y) = -x/y^2
0132:         
0133:     def test_division_by_zero(self):
0134:         """Test division by zero raises error"""
0135:         x = Tensor([1.0, 2.0])
0136:         y = Tensor([1.0, 0.0])
0137:         with pytest.raises(ValueError):
0138:             _ = x / y
0139: 
0140: class TestElementwiseOperations:
0141:     """Tests for element-wise operations"""
0142:     
0143:     def test_log(self):
0144:         """Test natural logarithm"""
0145:         x = Tensor([1.0, np.e], requires_grad=True)
0146:         y = x.log()
0147:         y.backward(np.array([1.0, 1.0]))
0148:         
0149:         assert np.allclose(y.data, [0.0, 1.0])
0150:         assert np.allclose(x.grad, [1.0, 1/np.e])  # d/dx(log(x)) = 1/x
0151:         
0152:     def test_exp(self):
0153:         """Test exponential function"""
0154:         x = Tensor([0.0, 1.0], requires_grad=True)
0155:         y = x.exp()
0156:         y.backward(np.array([1.0, 1.0]))
0157:         
0158:         assert np.allclose(y.data, [1.0, np.e])
0159:         assert np.allclose(y.data, x.grad)  # d/dx(exp(x)) = exp(x)
0160: 
0161: class TestReductionOperations:
0162:     """Tests for reduction operations"""
0163:     
0164:     def test_sum(self):
0165:         """Test sum reduction"""
0166:         x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
0167:         
0168:         # Test sum all elements
0169:         y1 = x.sum()
0170:         y1.backward()
0171:         assert np.allclose(y1.data, 10.0)
0172:         assert np.allclose(x.grad, np.ones_like(x.data))
0173:         
0174:         # Reset gradients
0175:         x.grad = None
0176:         
0177:         # Test sum along axis
0178:         y2 = x.sum(axis=0)
0179:         y2.backward(np.array([1.0, 1.0]))
0180:         assert np.allclose(y2.data, [4.0, 6.0])
0181:         assert np.allclose(x.grad, np.ones_like(x.data))
0182:         
0183:     def test_mean(self):
0184:         """Test mean reduction"""
0185:         x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
0186:         y = x.mean()
0187:         y.backward()
0188:         
0189:         assert np.allclose(y.data, 2.5)
0190:         assert np.allclose(x.grad, np.full_like(x.data, 0.25))  # 1/n for each element
0191:         
0192:     def test_max(self):
0193:         """Test max reduction"""
0194:         x = Tensor([[1.0, 4.0], [3.0, 2.0]], requires_grad=True)
0195:         y = x.max()
0196:         y.backward()
0197:         
0198:         assert np.allclose(y.data, 4.0)
0199:         expected_grad = np.array([[0.0, 1.0], [0.0, 0.0]])
0200:         assert np.allclose(x.grad, expected_grad)
0201: 
0202: class TestMatrixOperations:
0203:     """Tests for matrix operations"""
0204:     
0205:     def test_transpose(self):
0206:         """Test matrix transpose"""
0207:         x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
0208:         y = x.t()
0209:         y.backward(np.ones_like(y.data))
0210:         
0211:         assert np.allclose(y.data, [[1.0, 3.0], [2.0, 4.0]])
0212:         assert np.allclose(x.grad, np.ones_like(x.data))
0213:         
0214:     def test_transpose_3d(self):
0215:         """Test 3D tensor transpose"""
0216:         x = Tensor(np.arange(8).reshape(2, 2, 2), requires_grad=True)
0217:         y = x.transpose(1, 2, 0)
0218:         y.backward(np.ones_like(y.data))
0219:         
0220:         assert y.data.shape == (2, 2, 2)
0221:         assert np.allclose(x.grad, np.ones_like(x.data))
0222: 
0223: class TestComparisonOperations:
0224:     """Tests for comparison operations"""
0225:     
0226:     def test_greater(self):
0227:         """Test greater than operation"""
0228:         x = Tensor([1.0, 2.0, 3.0])
0229:         y = Tensor([2.0, 2.0, 2.0])
0230:         result = x > y
0231:         assert np.allclose(result.data, [False, False, True])
0232:         
0233:     def test_less_equal(self):
0234:         """Test less than or equal operation"""
0235:         x = Tensor([1.0, 2.0, 3.0])
0236:         y = Tensor([2.0, 2.0, 2.0])
0237:         result = x <= y
0238:         assert np.allclose(result.data, [True, True, False])
0239:         
0240:     def test_equal(self):
0241:         """Test equality operation"""
0242:         x = Tensor([1.0, 2.0, 3.0])
0243:         y = Tensor([1.0, 2.0, 2.0])
0244:         result = x == y
0245:         assert np.allclose(result.data, [True, True, False])
0246:         
0247:     def test_not_equal(self):
0248:         """Test inequality operation"""
0249:         x = Tensor([1.0, 2.0, 3.0])
0250:         y = Tensor([1.0, 2.0, 2.0])
0251:         result = x != y
0252:         assert np.allclose(result.data, [False, False, True])
0253: 
0254: class TestEdgeCases:
0255:     """Tests for edge cases and error conditions"""
0256:     
0257:     def test_log_negative(self):
0258:         """Test log of negative number raises error"""
0259:         x = Tensor([-1.0])
0260:         with pytest.raises(ValueError):
0261:             _ = x.log()
0262:             
0263:     def test_power_non_scalar(self):
0264:         """Test power with non-scalar exponent raises error"""
0265:         x = Tensor([2.0])
0266:         y = Tensor([1.0, 2.0])
0267:         with pytest.raises(ValueError):
0268:             _ = x ** y
0269:             
0270:     def test_reduction_keepdims(self):
0271:         """Test reduction operations with keepdims=True"""
0272:         x = Tensor([[1.0, 2.0], [3.0, 4.0]])
0273:         y = x.sum(axis=0, keepdims=True)
0274:         assert y.shape == (1, 2)
0275:         
0276:     def test_broadcasting_division(self):
0277:         """Test division with broadcasting"""
0278:         x = Tensor([[1.0, 2.0], [3.0, 4.0]])
0279:         y = Tensor([2.0])
0280:         z = x / y
0281:         assert z.shape == x.shape
0282:         assert np.allclose(z.data, [[0.5, 1.0], [1.5, 2.0]])

// File: C:\Users\aluja\Desktop\DLpy\tests\test_optim.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: from DLpy.optim import SGD, Adam, RMSprop, AdaGrad
0005: 
0006: class TestOptimizers:
0007:     """Base test class for all optimizers."""
0008:     
0009:     def setup_method(self):
0010:         """Setup method run before each test."""
0011:         # Create a simple parameter tensor
0012:         self.param = Tensor([1.0, 2.0, 3.0], requires_grad=True)
0013:         self.grad = np.array([0.1, 0.2, 0.3])
0014:         
0015:     def _test_basic_update(self, optimizer_class, **kwargs):
0016:         """Helper method to test basic parameter updates."""
0017:         optimizer = optimizer_class([self.param], **kwargs)
0018:         
0019:         # Initial parameter values
0020:         initial_params = self.param.data.copy()
0021:         
0022:         # Set gradient and perform optimization step
0023:         self.param.grad = self.grad
0024:         optimizer.step()
0025:         
0026:         # Check that parameters were updated
0027:         assert not np.array_equal(self.param.data, initial_params)
0028:         
0029:     def _test_zero_grad(self, optimizer_class, **kwargs):
0030:         """Helper method to test zero_grad functionality."""
0031:         optimizer = optimizer_class([self.param], **kwargs)
0032:         
0033:         # Set some gradient
0034:         self.param.grad = self.grad
0035:         
0036:         # Zero out gradients
0037:         optimizer.zero_grad()
0038:         
0039:         # Check that gradients are zeroed
0040:         assert np.all(self.param.grad == 0)
0041: 
0042: class TestSGD(TestOptimizers):
0043:     """Tests for SGD optimizer."""
0044:     
0045:     def test_basic_sgd(self):
0046:         """Test basic SGD functionality."""
0047:         self._test_basic_update(SGD, lr=0.1)
0048:         
0049:     def test_sgd_momentum(self):
0050:         """Test SGD with momentum."""
0051:         optimizer = SGD([self.param], lr=0.1, momentum=0.9)
0052:         
0053:         # First update
0054:         self.param.grad = self.grad
0055:         optimizer.step()
0056:         first_update = self.param.data.copy()
0057:         
0058:         # Second update with same gradient
0059:         optimizer.step()
0060:         
0061:         # With momentum, second update should be larger
0062:         first_step = np.abs(first_update - np.array([1.0, 2.0, 3.0]))
0063:         second_step = np.abs(self.param.data - first_update)
0064:         assert np.all(second_step > first_step)
0065:         
0066:     def test_sgd_nesterov(self):
0067:         """Test SGD with Nesterov momentum."""
0068:         self._test_basic_update(SGD, lr=0.1, momentum=0.9, nesterov=True)
0069:         
0070:     def test_sgd_weight_decay(self):
0071:         """Test SGD with weight decay."""
0072:         optimizer = SGD([self.param], lr=0.1, weight_decay=0.1)
0073:         self.param.grad = self.grad
0074:         optimizer.step()
0075:         
0076:         # Parameters should decrease more with weight decay
0077:         no_decay_param = Tensor([1.0, 2.0, 3.0], requires_grad=True)
0078:         no_decay_optimizer = SGD([no_decay_param], lr=0.1)
0079:         no_decay_param.grad = self.grad
0080:         no_decay_optimizer.step()
0081:         
0082:         assert np.all(np.abs(self.param.data) < np.abs(no_decay_param.data))
0083: 
0084: class TestAdam(TestOptimizers):
0085:     """Tests for Adam optimizer."""
0086:     
0087:     def test_basic_adam(self):
0088:         """Test basic Adam functionality."""
0089:         self._test_basic_update(Adam, lr=0.001)
0090:         
0091:     def test_adam_bias_correction(self):
0092:         """Test Adam bias correction."""
0093:         optimizer = Adam([self.param], lr=0.001)
0094:         
0095:         initial_param = self.param.data.copy()
0096:         updates = []
0097:         
0098:         # Perform several updates and track parameter changes
0099:         for _ in range(6):
0100:             self.param.grad = self.grad  # Keep constant gradient for testing
0101:             optimizer.step()
0102:             updates.append(np.linalg.norm(self.param.data - initial_param))
0103:         
0104:         # Test that:
0105:         # 1. Parameters are actually being updated
0106:         assert not np.array_equal(self.param.data, initial_param)
0107:         
0108:         # 2. Updates are being affected by bias correction
0109:         # (not necessarily smaller, but should be different)
0110:         assert len(set(updates)) > 1  # Updates should not all be identical
0111:         
0112:         # 3. State contains expected bias correction terms
0113:         assert 'step' in optimizer.state[id(self.param)]
0114:         assert 'exp_avg' in optimizer.state[id(self.param)]
0115:         assert 'exp_avg_sq' in optimizer.state[id(self.param)]
0116:         
0117:     def test_adam_amsgrad(self):
0118:         """Test Adam with AMSGrad."""
0119:         self._test_basic_update(Adam, lr=0.001, amsgrad=True)
0120: 
0121: class TestRMSprop(TestOptimizers):
0122:     """Tests for RMSprop optimizer."""
0123:     
0124:     def test_basic_rmsprop(self):
0125:         """Test basic RMSprop functionality."""
0126:         self._test_basic_update(RMSprop, lr=0.01)
0127:         
0128:     def test_rmsprop_momentum(self):
0129:         """Test RMSprop with momentum."""
0130:         self._test_basic_update(RMSprop, lr=0.01, momentum=0.9)
0131:         
0132:     def test_rmsprop_centered(self):
0133:         """Test centered RMSprop."""
0134:         self._test_basic_update(RMSprop, lr=0.01, centered=True)
0135: 
0136: class TestAdaGrad(TestOptimizers):
0137:     """Tests for AdaGrad optimizer."""
0138:     
0139:     def test_basic_adagrad(self):
0140:         """Test basic AdaGrad functionality."""
0141:         self._test_basic_update(AdaGrad, lr=0.01)
0142:         
0143:     def test_adagrad_lr_decay(self):
0144:         """Test AdaGrad learning rate decay."""
0145:         optimizer = AdaGrad([self.param], lr=0.01, lr_decay=0.1)
0146:         
0147:         initial_param = self.param.data.copy()
0148:         effective_lrs = []
0149:         
0150:         # Perform several updates and track effective learning rates
0151:         for _ in range(6):
0152:             self.param.grad = self.grad  # Keep constant gradient for testing
0153:             prev_param = self.param.data.copy()
0154:             optimizer.step()
0155:             
0156:             # Calculate effective learning rate from parameter update
0157:             param_update = np.linalg.norm(self.param.data - prev_param)
0158:             grad_norm = np.linalg.norm(self.grad)
0159:             effective_lrs.append(param_update / grad_norm if grad_norm != 0 else 0)
0160:         
0161:         # Test that:
0162:         # 1. Parameters are actually being updated
0163:         assert not np.array_equal(self.param.data, initial_param)
0164:         
0165:         # 2. Accumulated sum in state is increasing
0166:         assert np.all(optimizer.state[id(self.param)]['sum'] > 0)
0167:         
0168:         # 3. Effective learning rates should show some variation
0169:         assert len(set(map(lambda x: round(x, 6), effective_lrs))) > 1
0170:         
0171:     def test_adagrad_reset(self):
0172:         """Test AdaGrad state reset."""
0173:         optimizer = AdaGrad([self.param], lr=0.01)
0174:         
0175:         # Perform some updates
0176:         self.param.grad = self.grad
0177:         optimizer.step()
0178:         
0179:         # Reset state
0180:         optimizer.reset_state()
0181:         
0182:         # Check that state was reset
0183:         for state in optimizer.state.values():
0184:             assert state['step'] == 0
0185:             assert np.all(state['sum'] == 0)
0186: 
0187: class TestOptimizerEdgeCases:
0188:     """Tests for optimizer edge cases and error conditions."""
0189:     
0190:     def test_invalid_learning_rates(self):
0191:         """Test that invalid learning rates raise errors."""
0192:         param = Tensor([1.0], requires_grad=True)
0193:         
0194:         with pytest.raises(ValueError):
0195:             SGD([param], lr=-0.1)
0196:         with pytest.raises(ValueError):
0197:             Adam([param], lr=-0.1)
0198:         with pytest.raises(ValueError):
0199:             RMSprop([param], lr=-0.1)
0200:         with pytest.raises(ValueError):
0201:             AdaGrad([param], lr=-0.1)
0202:             
0203:     def test_no_gradients(self):
0204:         """Test optimizer behavior with no gradients."""
0205:         param = Tensor([1.0], requires_grad=True)
0206:         optimizers = [
0207:             SGD([param], lr=0.1),
0208:             Adam([param], lr=0.001),
0209:             RMSprop([param], lr=0.01),
0210:             AdaGrad([param], lr=0.01)
0211:         ]
0212:         
0213:         # Parameter should not change if there's no gradient
0214:         for optimizer in optimizers:
0215:             initial_param = param.data.copy()
0216:             optimizer.step()
0217:             assert np.array_equal(param.data, initial_param)
0218:             
0219:     def test_param_groups(self):
0220:         """Test adding parameter groups."""
0221:         param1 = Tensor([1.0], requires_grad=True)
0222:         param2 = Tensor([2.0], requires_grad=True)
0223:         
0224:         optimizer = SGD([param1], lr=0.1)
0225:         optimizer.add_param_group({'params': [param2]})
0226:         
0227:         # Both parameters should be updated
0228:         param1.grad = np.array([0.1])
0229:         param2.grad = np.array([0.2])
0230:         optimizer.step()
0231:         
0232:         assert not np.array_equal(param1.data, [1.0])
0233:         assert not np.array_equal(param2.data, [2.0])

// File: C:\Users\aluja\Desktop\DLpy\tests\test_tensor.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: 
0005: class TestReshapeOp:
0006:     """Tests for Reshape operation"""
0007: 
0008:     def test_reshape_edge_cases(self):
0009:         """Test edge cases for Reshape operation"""
0010:         x = Tensor([1.0, 2.0], requires_grad=True)
0011:         
0012:         # Test invalid shape
0013:         with pytest.raises(ValueError):
0014:             _ = x.reshape(3)  # Invalid shape
0015:         
0016:         # Test gradients with different shapes
0017:         y = x.reshape(2, 1)
0018:         y.backward(np.array([[1.0], [1.0]]))
0019:         assert np.array_equal(x.grad, [1.0, 1.0])
0020: 
0021: class TestAdvancedOperations:
0022:     """Additional tests for basic operations"""
0023:     
0024:     def test_broadcasting_edge_cases(self):
0025:         """Test broadcasting with different dimensions"""
0026:         # Test broadcasting scalar to matrix
0027:         x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
0028:         y = Tensor(2.0, requires_grad=True)
0029:         z = x * y
0030:         z.backward(np.ones_like(x.data))
0031:         assert y.grad.shape == (1,)
0032:         assert np.array_equal(x.grad, [[2.0, 2.0], [2.0, 2.0]])
0033:         
0034:         # Test broadcasting vector to matrix
0035:         x = Tensor([[1.0], [2.0]], requires_grad=True)
0036:         y = Tensor([1.0, 2.0], requires_grad=True)
0037:         z = x + y  # Should broadcast to [[1,2], [2,3]]
0038:         z.backward(np.ones((2, 2)))
0039:         assert x.grad.shape == (2, 1)
0040:         assert y.grad.shape == (2,)
0041:         
0042:     def test_zero_gradient_handling(self):
0043:         """Test operations with zero gradients"""
0044:         x = Tensor([1.0, 2.0], requires_grad=True)
0045:         y = Tensor([3.0, 4.0], requires_grad=True)
0046:         z = x * y
0047:         z.backward(np.zeros_like(z.data))
0048:         assert np.all(x.grad == 0)
0049:         assert np.all(y.grad == 0)
0050:         
0051:     def test_non_differentiable_inputs(self):
0052:         """Test operations with non-differentiable inputs"""
0053:         x = Tensor([1.0, 2.0], requires_grad=False)
0054:         y = Tensor([3.0, 4.0], requires_grad=True)
0055:         z = x * y
0056:         z.backward(np.ones_like(z.data))
0057:         assert x.grad is None  # Non-differentiable input should have no gradient
0058:         assert np.array_equal(y.grad, [1.0, 2.0])
0059: 
0060:     def test_tensor_creation_edge_cases(self):
0061:         """Test edge cases in tensor creation"""
0062:         # Test with different dtypes
0063:         t1 = Tensor([1, 2, 3], dtype=np.int32)
0064:         assert t1.dtype == np.int32
0065:         
0066:         # Test with nested lists
0067:         t2 = Tensor([[1, 2], [3, 4]])
0068:         assert t2.shape == (2, 2)
0069:         
0070:         # Test with another tensor
0071:         t3 = Tensor(t2)
0072:         assert np.array_equal(t3.data, t2.data)
0073: 
0074:     def test_backward_edge_cases(self):
0075:         """Test edge cases in backward pass"""
0076:         # Test backward with scalar tensor
0077:         x = Tensor(2.0, requires_grad=True)
0078:         y = x * 2
0079:         y.backward(np.array(3.0))
0080:         assert x.grad is not None
0081:         
0082:         # Test backward with non-scalar tensor without gradient
0083:         x = Tensor([1.0, 2.0, 3.0], requires_grad=True)
0084:         y = x * 2
0085:         with pytest.raises(RuntimeError):
0086:             y.backward()  # Should raise error for non-scalar
0087: 
0088:     def test_repr_and_str(self):
0089:         """Test string representations"""
0090:         t = Tensor([1.0, 2.0], requires_grad=True)
0091:         assert 'Tensor' in repr(t)
0092:         assert 'requires_grad=True' in repr(t)

// ----------------------------------------
// Total Python files found: 43
