// Python Files Concatenated on 01/20/2025 12:51:09
// ----------------------------------------



// File: C:\Users\aluja\Desktop\DLpy\DLpy\__init__.py
// ----------------------------------------
0001: """
0002: DLpy: A Deep Learning Library with DAG-based Autograd
0003: 
0004: This library provides a PyTorch-like interface for building and training neural networks,
0005: with a focus on clear implementation and educational value.
0006: """
0007: 
0008: from .core import Tensor, Function, Context
0009: from .ops import Add, Multiply, Reshape
0010: 
0011: __version__ = "0.1.0"
0012: 
0013: __all__ = [
0014:     'Tensor',
0015:     'Function',
0016:     'Context',
0017:     'Add',
0018:     'Multiply',
0019:     'Reshape',
0020: ]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\__init__.py
// ----------------------------------------
0001: """
0002: Core functionality for DLpy.
0003: 
0004: This module contains the fundamental building blocks of the deep learning library.
0005: """
0006: 
0007: from .tensor import Tensor
0008: from .function import Function
0009: from .context import Context
0010: from .autograd import AutogradEngine, get_autograd_engine
0011: 
0012: __all__ = ['Tensor', 'Function', 'Context', 'AutogradEngine', 'get_autograd_engine']

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\autograd.py
// ----------------------------------------
0001: from typing import Dict, Set, List, Optional, Tuple, Union
0002: import numpy as np
0003: from collections import defaultdict
0004: import warnings
0005: 
0006: class Edge:
0007:     """
0008:     Represents a directed edge in the computational graph.
0009:     
0010:     Each edge connects a source node (input tensor) to a destination node
0011:     (output tensor) and stores gradient information for that connection.
0012:     """
0013:     
0014:     def __init__(self, src: 'Node', dst: 'Node'):
0015:         self.src = src
0016:         self.dst = dst
0017:         self.grad: Optional[np.ndarray] = None
0018:         
0019: class Node:
0020:     """
0021:     Represents a node in the computational graph.
0022:     
0023:     Each node corresponds to an operation in the computation and maintains
0024:     connections to its inputs and outputs through edges.
0025:     """
0026:     
0027:     def __init__(self, tensor: 'Tensor'):
0028:         self.tensor = tensor
0029:         self.in_edges: List[Edge] = []
0030:         self.out_edges: List[Edge] = []
0031:         self._backward_fn = tensor._backward_fn
0032: 
0033: class AutogradEngine:
0034:     """
0035:     Engine for managing automatic differentiation computations.
0036:     
0037:     This class handles the creation and execution of the computational graph,
0038:     manages gradient computation and accumulation, and provides utilities for
0039:     graph manipulation and visualization.
0040:     """
0041:     
0042:     def __init__(self):
0043:         self._nodes: Dict[int, Node] = {}
0044:         self._edges: Set[Edge] = set()
0045:         self._currently_computing_gradients = False
0046:         
0047:     def register_tensor(self, tensor: 'Tensor') -> None:
0048:         """
0049:         Registers a tensor with the autograd engine.
0050:         
0051:         Args:
0052:             tensor: Tensor to register
0053:         """
0054:         if id(tensor) not in self._nodes:
0055:             self._nodes[id(tensor)] = Node(tensor)
0056:             
0057:     def add_edge(self, src: 'Tensor', dst: 'Tensor') -> None:
0058:         """
0059:         Adds a directed edge between two tensors in the computational graph.
0060:         
0061:         Args:
0062:             src: Source tensor
0063:             dst: Destination tensor
0064:         """
0065:         src_node = self._nodes[id(src)]
0066:         dst_node = self._nodes[id(dst)]
0067:         
0068:         edge = Edge(src_node, dst_node)
0069:         src_node.out_edges.append(edge)
0070:         dst_node.in_edges.append(edge)
0071:         self._edges.add(edge)
0072:         
0073:     def backward(self, tensor: 'Tensor', gradient: Optional[np.ndarray] = None) -> None:
0074:         """Executes backward pass starting from the given tensor."""
0075:         if self._currently_computing_gradients:
0076:             raise RuntimeError("Nested gradient computation detected")
0077:             
0078:         self._currently_computing_gradients = True
0079:         try:
0080:             # Initialize grad_dict as a regular dictionary
0081:             grad_dict: Dict[int, np.ndarray] = {}
0082:             
0083:             # If no gradient is provided, assume it's 1 (for scalar outputs)
0084:             if gradient is None:
0085:                 if tensor.data.shape == ():
0086:                     grad_dict[id(tensor)] = np.array(1.0)
0087:                 else:
0088:                     grad_dict[id(tensor)] = np.ones_like(tensor.data)
0089:             else:
0090:                 grad_dict[id(tensor)] = gradient
0091:             
0092:             # Perform topological sort
0093:             sorted_nodes = self._topological_sort(tensor)
0094:             
0095:             # Traverse nodes in reverse topological order
0096:             for node in reversed(sorted_nodes):
0097:                 node_id = id(node.tensor)
0098:                 if node_id not in grad_dict or not node.tensor.requires_grad:
0099:                     continue  # No gradient to propagate
0100:                 
0101:                 current_grad = grad_dict[node_id]
0102:                 
0103:                 if node.tensor._backward_fn is not None:
0104:                     node.tensor._backward_fn(current_grad, grad_dict)
0105:                 
0106:                 # Accumulate gradients for leaf nodes
0107:                 if len(node.in_edges) == 0 and node.tensor.requires_grad:
0108:                     if node.tensor.grad is None:
0109:                         node.tensor.grad = current_grad
0110:                     else:
0111:                         try:
0112:                             node.tensor.grad += current_grad
0113:                         except ValueError:
0114:                             # If shapes don't match, reshape current_grad
0115:                             node.tensor.grad += current_grad.reshape(node.tensor.grad.shape)
0116:         finally:
0117:             self._currently_computing_gradients = False
0118: 
0119:     def _topological_sort(self, start_tensor: 'Tensor') -> List[Node]:
0120:         """
0121:         Performs topological sort on the computation graph.
0122:         
0123:         Args:
0124:             start_tensor: Tensor to start the sort from
0125:             
0126:         Returns:
0127:             List of nodes in topological order
0128:             
0129:         Raises:
0130:             RuntimeError: If graph contains cycles
0131:         """
0132:         result: List[Node] = []
0133:         visited: Set[Node] = set()
0134:         temp_visited: Set[Node] = set()
0135:         
0136:         def visit(node: Node) -> None:
0137:             if node in temp_visited:
0138:                 raise RuntimeError("Cycle detected in computation graph")
0139:                 
0140:             if node not in visited:
0141:                 temp_visited.add(node)
0142:                 for edge in node.in_edges:
0143:                     visit(edge.src)
0144:                 temp_visited.remove(node)
0145:                 visited.add(node)
0146:                 result.append(node)
0147:                 
0148:         visit(self._nodes[id(start_tensor)])
0149:         return result
0150:             
0151:     def clear(self) -> None:
0152:         """Clears the computational graph."""
0153:         self._nodes.clear()
0154:         self._edges.clear()
0155:     
0156:     def validate_graph(self) -> List[str]:
0157:         """
0158:         Validates the computational graph structure.
0159:         """
0160:         warnings: List[str] = []
0161:         
0162:         # If no nodes in graph
0163:         if not self._nodes:
0164:             return warnings
0165: 
0166:         # Step 1: Find all nodes that are part of computations
0167:         active_nodes = set()
0168:         output_nodes = []
0169:         for node in self._nodes.values():
0170:             if not node.out_edges:  # Output node
0171:                 output_nodes.append(node)
0172:             if node.in_edges or node.out_edges:  # Node is part of a computation
0173:                 active_nodes.add(node)
0174: 
0175:         # Step 2: Find all connected nodes starting from outputs
0176:         connected_nodes = set()
0177:         for output_node in output_nodes:
0178:             stack = [output_node]
0179:             while stack:
0180:                 curr = stack.pop()
0181:                 connected_nodes.add(curr)
0182:                 for edge in curr.in_edges:
0183:                     if edge.src not in connected_nodes:
0184:                         stack.append(edge.src)
0185:                         
0186:         # Step 3: Find nodes not connected to outputs
0187:         all_nodes = set(self._nodes.values())
0188:         unconnected_nodes = all_nodes - connected_nodes
0189: 
0190:         # Step 4: Find completely isolated nodes
0191:         isolated_nodes = all_nodes - active_nodes
0192: 
0193:         # Add appropriate warnings
0194:         if unconnected_nodes:
0195:             warnings.append(f"Found {len(unconnected_nodes)} nodes not connected to any output")
0196:             
0197:         if isolated_nodes:
0198:             warnings.append(f"Found {len(isolated_nodes)} isolated nodes")
0199:             
0200:         # Check gradient shapes
0201:         for edge in self._edges:
0202:             if edge.grad is not None:
0203:                 src_shape = edge.src.tensor.shape
0204:                 grad_shape = edge.grad.shape
0205:                 if src_shape != grad_shape:
0206:                     warnings.append(
0207:                         f"Gradient shape mismatch: grad shape {grad_shape} vs tensor shape {src_shape}"
0208:                     )
0209:                     
0210:         return warnings
0211: 
0212: 
0213: # Global autograd engine instance
0214: _autograd_engine = AutogradEngine()
0215: 
0216: def get_autograd_engine() -> AutogradEngine:
0217:     """Returns the global autograd engine instance."""
0218:     return _autograd_engine

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\context.py
// ----------------------------------------
0001: from typing import Any, Dict, List, Tuple
0002: from dataclasses import dataclass, field
0003: 
0004: @dataclass
0005: class Context:
0006:     """
0007:     Context class for storing information needed during the backward pass.
0008:     
0009:     The Context class serves as a storage mechanism for tensors and metadata that are 
0010:     needed during backpropagation. It's passed to both forward and backward functions
0011:     to maintain state between the two passes.
0012:     
0013:     Attributes:
0014:         _saved_tensors: List of tensors saved during forward pass for use in backward pass
0015:         _non_tensor_args: Dictionary of additional arguments needed for backward pass
0016:         _intermediate_values: Dictionary storing intermediate computations
0017:     """
0018:     
0019:     _saved_tensors: List[Any] = field(default_factory=list)
0020:     _non_tensor_args: Dict[str, Any] = field(default_factory=dict)
0021:     _intermediate_values: Dict[str, Any] = field(default_factory=dict)
0022: 
0023:     def save_for_backward(self, *args: Any) -> None:
0024:         """
0025:         Saves tensors that will be needed for the backward pass.
0026:         
0027:         Args:
0028:             *args: Variable number of tensors to save
0029:         """
0030:         self._saved_tensors = list(args)
0031: 
0032:     def save_arguments(self, **kwargs: Any) -> None:
0033:         """
0034:         Saves additional arguments that will be needed for the backward pass.
0035:         
0036:         Args:
0037:             **kwargs: Keyword arguments to save
0038:         """
0039:         self._non_tensor_args.update(kwargs)
0040:         
0041:     def store_intermediate(self, name: str, value: Any) -> None:
0042:         """
0043:         Stores intermediate values computed during forward pass that may be
0044:         useful during backward pass or for debugging.
0045:         
0046:         Args:
0047:             name: Identifier for the intermediate value
0048:             value: The value to store
0049:         """
0050:         self._intermediate_values[name] = value
0051: 
0052:     @property
0053:     def saved_tensors(self) -> Tuple[Any, ...]:
0054:         """Returns the saved tensors as a tuple."""
0055:         return tuple(self._saved_tensors)
0056: 
0057:     @property
0058:     def saved_arguments(self) -> Dict[str, Any]:
0059:         """Returns the saved non-tensor arguments."""
0060:         return self._non_tensor_args.copy()
0061:         
0062:     def get_intermediate(self, name: str) -> Any:
0063:         """
0064:         Retrieves a stored intermediate value.
0065:         
0066:         Args:
0067:             name: Identifier for the intermediate value
0068:             
0069:         Returns:
0070:             The stored value
0071:             
0072:         Raises:
0073:             KeyError: If no value exists for the given name
0074:         """
0075:         return self._intermediate_values[name]
0076: 
0077:     def clear(self) -> None:
0078:         """Clears all saved data from the context."""
0079:         self._saved_tensors.clear()
0080:         self._non_tensor_args.clear()
0081:         self._intermediate_values.clear()

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\function.py
// ----------------------------------------
0001: from abc import ABC, abstractmethod
0002: from typing import Any, Tuple, Optional, Dict
0003: import numpy as np
0004: 
0005: from .context import Context
0006: from .tensor import Tensor  # This will be implemented next
0007: 
0008: class Function(ABC):
0009:     """
0010:     Base class for all autograd operations.
0011:     
0012:     This class defines the interface for creating differentiable operations.
0013:     Each operation should implement both a forward pass (computing the result)
0014:     and a backward pass (computing gradients).
0015:     
0016:     The Function class follows a similar design pattern to PyTorch's autograd.Function,
0017:     but with some simplifications and additional features for clarity and debugging.
0018:     """
0019:     
0020:     requires_grad: bool = True
0021:     
0022:     @staticmethod
0023:     @abstractmethod
0024:     def forward(ctx: Context, *args: Any, **kwargs: Any) -> Tensor:
0025:         """
0026:         Performs the forward computation.
0027:         
0028:         Args:
0029:             ctx: Context object for saving information needed in backward pass
0030:             *args: Input tensors and other arguments
0031:             **kwargs: Additional keyword arguments for the operation
0032:             
0033:         Returns:
0034:             Result of the computation as a Tensor
0035:         """
0036:         raise NotImplementedError
0037:         
0038:     @staticmethod
0039:     @abstractmethod
0040:     def backward(ctx: Context, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0041:         """
0042:         Computes gradients of the operation with respect to its inputs.
0043:         
0044:         Args:
0045:             ctx: Context object containing saved tensors from forward pass
0046:             grad_output: Gradient of the loss with respect to the output
0047:             grad_dict: Dictionary mapping tensor IDs to their gradients
0048:         """
0049:         raise NotImplementedError
0050:         
0051:     @classmethod
0052:     def apply(cls, *args: Any, **kwargs: Any) -> Tensor:
0053:         """
0054:         Applies the function to the given inputs.
0055:         
0056:         This method:
0057:         1. Creates a Context object for storing intermediate values
0058:         2. Runs the forward pass
0059:         3. Sets up the computational graph for gradient computation
0060:         4. Returns the result
0061:         """
0062:         ctx = Context()
0063:         result = cls.forward(ctx, *args, **kwargs)
0064:         
0065:         # Check if we need to compute gradients
0066:         needs_grad = cls.requires_grad and any(
0067:             isinstance(arg, Tensor) and arg.requires_grad 
0068:             for arg in args
0069:         )
0070:         
0071:         if needs_grad:
0072:             def backward_fn(grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0073:                 cls.backward(ctx, grad_output, grad_dict)
0074:             
0075:             result._backward_fn = backward_fn
0076:             result.requires_grad_(True)
0077:             
0078:             # Get autograd engine and register edges
0079:             from .autograd import get_autograd_engine
0080:             engine = get_autograd_engine()
0081:             for arg in args:
0082:                 if isinstance(arg, Tensor):
0083:                     engine.add_edge(arg, result)
0084:         
0085:         return result  # Return result in all cases
0086:         
0087:         
0088:     @staticmethod
0089:     def verify_backward(
0090:         forward_fn: Any,
0091:         backward_fn: Any,
0092:         inputs: Tuple[np.ndarray, ...],
0093:         epsilon: float = 1e-6
0094:     ) -> bool:
0095:         """
0096:         Verifies backward pass implementation using numerical gradients.
0097:         
0098:         This helper method compares analytically computed gradients with
0099:         numerically computed gradients to check for correctness.
0100:         
0101:         Args:
0102:             forward_fn: The forward pass function
0103:             backward_fn: The backward pass function
0104:             inputs: Tuple of input arrays
0105:             epsilon: Small value for numerical gradient computation
0106:             
0107:         Returns:
0108:             True if gradients match within tolerance, False otherwise
0109:         """
0110:         def compute_numerical_gradient(idx: int, inp: np.ndarray) -> np.ndarray:
0111:             grad = np.zeros_like(inp)
0112:             it = np.nditer(inp, flags=['multi_index'])
0113:             
0114:             while not it.finished:
0115:                 ix = it.multi_index
0116:                 old_value = inp[ix]
0117:                 
0118:                 # Compute f(x + epsilon)
0119:                 inp[ix] = old_value + epsilon
0120:                 pos_inputs = list(inputs)
0121:                 pos_inputs[idx] = inp.copy()
0122:                 pos_output = forward_fn(*pos_inputs)
0123:                 
0124:                 # Compute f(x - epsilon)
0125:                 inp[ix] = old_value - epsilon
0126:                 neg_inputs = list(inputs)
0127:                 neg_inputs[idx] = inp.copy()
0128:                 neg_output = forward_fn(*neg_inputs)
0129:                 
0130:                 # Restore original value
0131:                 inp[ix] = old_value
0132:                 
0133:                 # Compute numerical gradient
0134:                 grad[ix] = np.sum(pos_output - neg_output) / (2 * epsilon)
0135:                 it.iternext()
0136:                 
0137:             return grad
0138:             
0139:         # Compute analytical gradients
0140:         ctx = Context()
0141:         output = forward_fn(*inputs)
0142:         grad_output = np.ones_like(output)
0143:         analytical_grads = backward_fn(ctx, grad_output)
0144:         
0145:         # Compute numerical gradients
0146:         numerical_grads = tuple(
0147:             compute_numerical_gradient(i, inp.copy()) 
0148:             for i, inp in enumerate(inputs)
0149:         )
0150:         
0151:         # Compare gradients
0152:         for analytical, numerical in zip(analytical_grads, numerical_grads):
0153:             if analytical is not None:
0154:                 rel_error = np.max(
0155:                     np.abs(analytical - numerical) /
0156:                     (np.maximum(np.abs(analytical), np.abs(numerical)) + epsilon)
0157:                 )
0158:                 if rel_error > 1e-5:
0159:                     return False
0160:                     
0161:         return True

// File: C:\Users\aluja\Desktop\DLpy\DLpy\core\tensor.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Optional, Union, List, Tuple, Callable, Dict, Set
0003: from numbers import Number
0004: 
0005: class Tensor:
0006:     """
0007:     A multidimensional array with autograd capabilities.
0008:     
0009:     The Tensor class wraps numpy arrays and adds automatic differentiation
0010:     capabilities. It tracks the computational graph and enables gradient
0011:     computation through backpropagation.
0012:     
0013:     Attributes:
0014:         data: The underlying numpy array holding the tensor's values
0015:         grad: Gradient of the loss with respect to this tensor
0016:         requires_grad: Whether to compute gradients for this tensor
0017:         _prev: Set of immediate predecessor nodes in computational graph
0018:         _backward_fn: Function to compute gradients during backpropagation
0019:         _is_leaf: Whether this tensor is a leaf node (created by user)
0020:     """
0021:     
0022:     def __init__(
0023:         self,
0024:         data: Union[np.ndarray, List, Number],
0025:         requires_grad: bool = False,
0026:         dtype: Optional[np.dtype] = None
0027:     ):
0028:         # Convert scalars to scalar arrays with shape ()
0029:         if isinstance(data, (int, float)):
0030:             self.data = np.array(data, dtype=dtype or np.float64)  # Will have shape ()
0031:         elif isinstance(data, Tensor):
0032:             self.data = data.data
0033:         elif isinstance(data, list):
0034:             self.data = np.array(data, dtype=dtype)
0035:         else:
0036:             self.data = data.astype(dtype) if dtype else data
0037:             
0038:         self.grad: Optional[np.ndarray] = None
0039:         self._requires_grad = requires_grad
0040:         self._backward_fn: Optional[Callable] = None
0041:         self._prev: Set['Tensor'] = set()
0042:         self._is_leaf = True
0043: 
0044:         # Register with autograd engine
0045:         from .autograd import get_autograd_engine
0046:         engine = get_autograd_engine()
0047:         engine.register_tensor(self)
0048:         
0049:         if requires_grad:
0050:             self.zero_grad()
0051: 
0052:     @property
0053:     def shape(self) -> Tuple[int, ...]:
0054:         """Returns the shape of the tensor."""
0055:         return self.data.shape
0056:         
0057:     @property
0058:     def dtype(self) -> np.dtype:
0059:         """Returns the data type of the tensor."""
0060:         return self.data.dtype
0061:         
0062:     @property
0063:     def requires_grad(self) -> bool:
0064:         """Returns whether the tensor requires gradient computation."""
0065:         return self._requires_grad
0066:         
0067:     def requires_grad_(self, requires_grad: bool = True) -> 'Tensor':
0068:         """Sets gradient computation requirement and returns self."""
0069:         self._requires_grad = requires_grad
0070:         if requires_grad and self.grad is None:
0071:             self.zero_grad()
0072:         return self
0073: 
0074:     def zero_grad(self) -> None:
0075:         """Zeros out the gradient."""
0076:         if self.data.shape == ():  # For scalar tensors
0077:             self.grad = np.zeros(1, dtype=np.float64)  # Force 1D array
0078:         else:
0079:             self.grad = np.zeros_like(self.data, dtype=np.float64)
0080:         
0081:     def backward(self, gradient: Optional[np.ndarray] = None) -> None:
0082:         """
0083:         Computes gradients of the loss with respect to this tensor.
0084:         """
0085:         if not self.requires_grad:
0086:             return
0087: 
0088:         # Handle default gradient for scalar tensors
0089:         if gradient is None:
0090:             if np.prod(self.shape) == 1:
0091:                 if self.shape == ():  # scalar tensor
0092:                     gradient = np.array(1.0)
0093:                 else:
0094:                     gradient = np.ones(self.shape)
0095:             else:
0096:                 raise RuntimeError("grad can be implicitly created only for scalar outputs")
0097: 
0098:         # Ensure gradient is numpy array
0099:         if isinstance(gradient, (int, float)):
0100:             gradient = np.array(gradient)
0101:             
0102:         # Ensure matching shapes for scalar case
0103:         if self.shape == () and gradient.shape != ():
0104:             gradient = gradient.sum()
0105:         elif self.shape != () and gradient.shape == ():
0106:             gradient = np.full(self.shape, gradient)
0107: 
0108:         # Get autograd engine and execute backward pass
0109:         from .autograd import get_autograd_engine
0110:         engine = get_autograd_engine()
0111:         engine.backward(self, gradient)
0112: 
0113: 
0114:     def __repr__(self) -> str:
0115:         return f"Tensor({self.data}, requires_grad={self.requires_grad})"
0116: 
0117:     # Basic arithmetic operations that will be connected to Function implementations
0118:     def __add__(self, other: Union['Tensor', Number]) -> 'Tensor':
0119:         from ..ops.basic import Add
0120:         return Add.apply(self, other)
0121:         
0122:     def __mul__(self, other: Union['Tensor', Number]) -> 'Tensor':
0123:         from ..ops.basic import Multiply
0124:         return Multiply.apply(self, other)
0125:         
0126:     def __matmul__(self, other: 'Tensor') -> 'Tensor':
0127:         from ..ops.basic import MatMul
0128:         return MatMul.apply(self, other)
0129:         
0130:     def __neg__(self) -> 'Tensor':
0131:         return self * (-1)
0132:         
0133:     def __sub__(self, other: Union['Tensor', Number]) -> 'Tensor':
0134:         return self + (-other)
0135: 
0136:     def reshape(self, *shape: int) -> 'Tensor':
0137:         from ..ops.reshape import Reshape
0138:         return Reshape.apply(self, shape)
0139: 
0140:     # Helper methods for numpy compatibility
0141:     def numpy(self) -> np.ndarray:
0142:         """Returns the underlying numpy array."""
0143:         return self.data
0144:         
0145:     @classmethod
0146:     def from_numpy(cls, array: np.ndarray, requires_grad: bool = False) -> 'Tensor':
0147:         """Creates a Tensor from a numpy array."""
0148:         return cls(array.copy(), requires_grad=requires_grad)
0149: 
0150:     # Shape manipulation methods
0151:     def reshape(self, *shape: int) -> 'Tensor':
0152:         """Returns a tensor with the same data and new shape."""
0153:         from ..ops import Reshape
0154:         return Reshape.apply(self, shape)
0155: 
0156:     def pow(self, exponent: Union['Tensor', float]) -> 'Tensor':
0157:         """Returns tensor raised to the power of exponent."""
0158:         from ..ops import Power
0159:         return Power.apply(self, exponent)
0160: 
0161:     def div(self, other: Union['Tensor', float]) -> 'Tensor':
0162:         """Returns self divided by other."""
0163:         from ..ops import Divide
0164:         return Divide.apply(self, other)
0165: 
0166:     def log(self) -> 'Tensor':
0167:         """Returns the natural logarithm of the tensor."""
0168:         from ..ops import Log
0169:         return Log.apply(self)
0170: 
0171:     def exp(self) -> 'Tensor':
0172:         """Returns e raised to the power of each element in the tensor."""
0173:         from ..ops import Exp
0174:         return Exp.apply(self)
0175: 
0176:     def sigmoid(self) -> 'Tensor':
0177:         """Returns the sigmoid of the tensor."""
0178:         from ..ops import Sigmoid
0179:         return Sigmoid.apply(self)
0180: 
0181:     def tanh(self) -> 'Tensor':
0182:         """Returns the hyperbolic tangent of the tensor."""
0183:         from ..ops import Tanh
0184:         return Tanh.apply(self)
0185: 
0186:     def sum(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':
0187:         """Returns the sum of all elements in the tensor."""
0188:         from ..ops import Sum
0189:         return Sum.apply(self, axis, keepdims)
0190: 
0191:     def mean(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':
0192:         """Returns the mean of all elements in the tensor."""
0193:         from ..ops import Mean
0194:         return Mean.apply(self, axis, keepdims)
0195: 
0196:     def max(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':
0197:         """Returns the maximum value of all elements in the tensor."""
0198:         from ..ops import Max
0199:         return Max.apply(self, axis, keepdims)
0200: 
0201:     def t(self) -> 'Tensor':
0202:         """Returns the transpose of the tensor."""
0203:         from ..ops import Transpose
0204:         return Transpose.apply(self)
0205: 
0206:     def transpose(self, *axes: int) -> 'Tensor':
0207:         """Returns the transposed tensor."""
0208:         from ..ops import Transpose
0209:         return Transpose.apply(self, axes if axes else None)
0210: 
0211:     # Comparison operations
0212:     def __gt__(self, other: Union['Tensor', float]) -> 'Tensor':
0213:         from ..ops import Greater
0214:         return Greater.apply(self, other)
0215: 
0216:     def __ge__(self, other: Union['Tensor', float]) -> 'Tensor':
0217:         from ..ops import GreaterEqual
0218:         return GreaterEqual.apply(self, other)
0219: 
0220:     def __lt__(self, other: Union['Tensor', float]) -> 'Tensor':
0221:         from ..ops import Less
0222:         return Less.apply(self, other)
0223: 
0224:     def __le__(self, other: Union['Tensor', float]) -> 'Tensor':
0225:         from ..ops import LessEqual
0226:         return LessEqual.apply(self, other)
0227: 
0228:     def __eq__(self, other: Union['Tensor', float]) -> 'Tensor':
0229:         from ..ops import Equal
0230:         return Equal.apply(self, other)
0231: 
0232:     def __ne__(self, other: Union['Tensor', float]) -> 'Tensor':
0233:         from ..ops import NotEqual
0234:         return NotEqual.apply(self, other)
0235: 
0236:     def __truediv__(self, other: Union['Tensor', float]) -> 'Tensor':
0237:         """Implements division using the / operator."""
0238:         from ..ops import Divide
0239:         return Divide.apply(self, other)
0240: 
0241:     def __pow__(self, exponent: Union['Tensor', float]) -> 'Tensor':
0242:         """Implements power using the ** operator."""
0243:         from ..ops import Power
0244:         return Power.apply(self, exponent)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\__init__.py
// ----------------------------------------
0001: """
0002: Neural network module for DLpy.
0003: 
0004: This module contains all components needed for building neural networks.
0005: """
0006: 
0007: from .modules import Module
0008: from .linear import Linear
0009: from .activations import (
0010:     relu, leaky_relu, elu, gelu, sigmoid, tanh,
0011:     ReLU, LeakyReLU, ELU, GELU, Sigmoid, Tanh
0012: )
0013: 
0014: from .conv2d import Conv2d 
0015: 
0016: __all__ = [
0017:     # Base module
0018:     'Module',
0019:     
0020:     # Layers
0021:     'Linear',
0022:     
0023:     # Activation functions
0024:     'relu',
0025:     'leaky_relu',
0026:     'elu',
0027:     'gelu',
0028:     'sigmoid',
0029:     'tanh',
0030:     'ReLU',
0031:     'LeakyReLU',
0032:     'ELU',
0033:     'GELU',
0034:     'Sigmoid',
0035:     'Tanh',
0036: 
0037:     # Convolutional layers
0038:     'Conv2d'
0039: ]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\activations.py
// ----------------------------------------
0001: """
0002: Activation functions module for DLpy.
0003: 
0004: This module contains all standard activation functions used in neural networks.
0005: Each activation function is implemented as a Function subclass for autograd support.
0006: """
0007: 
0008: from typing import Dict, Optional
0009: import numpy as np
0010: from ..core import Function, Tensor
0011: 
0012: class ReLU(Function):
0013:     """
0014:     Rectified Linear Unit activation function.
0015:     
0016:     Forward: f(x) = max(0, x)
0017:     Backward: f'(x) = 1 if x > 0 else 0
0018:     """
0019:     
0020:     @staticmethod
0021:     def forward(ctx, x):
0022:         if not isinstance(x, Tensor):
0023:             x = Tensor(x)
0024:             
0025:         ctx.save_for_backward(x)
0026:         return Tensor(np.maximum(0, x.data))
0027:         
0028:     @staticmethod
0029:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0030:         x, = ctx.saved_tensors
0031:         if x.requires_grad:
0032:             grad = grad_output * (x.data > 0)
0033:             grad_dict[id(x)] = grad
0034: 
0035: class LeakyReLU(Function):
0036:     """
0037:     Leaky Rectified Linear Unit activation function.
0038:     
0039:     Forward: f(x) = x if x > 0 else negative_slope * x
0040:     Backward: f'(x) = 1 if x > 0 else negative_slope
0041:     
0042:     Args:
0043:         negative_slope: Controls slope for negative values. Default: 0.01
0044:     """
0045:     
0046:     @staticmethod
0047:     def forward(ctx, x, negative_slope: float = 0.01):
0048:         if not isinstance(x, Tensor):
0049:             x = Tensor(x)
0050:             
0051:         ctx.save_for_backward(x)
0052:         ctx.save_arguments(negative_slope=negative_slope)
0053:         
0054:         return Tensor(np.where(x.data > 0, x.data, negative_slope * x.data))
0055:         
0056:     @staticmethod
0057:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0058:         x, = ctx.saved_tensors
0059:         negative_slope = ctx.saved_arguments['negative_slope']
0060:         
0061:         if x.requires_grad:
0062:             grad = grad_output * np.where(x.data > 0, 1.0, negative_slope)
0063:             grad_dict[id(x)] = grad
0064: 
0065: class ELU(Function):
0066:     """
0067:     Exponential Linear Unit activation function.
0068:     
0069:     Forward: f(x) = x if x > 0 else alpha * (exp(x) - 1)
0070:     Backward: f'(x) = 1 if x > 0 else alpha * exp(x)
0071:     
0072:     Args:
0073:         alpha: Controls the value to which an ELU saturates for negative inputs. Default: 1.0
0074:     """
0075:     
0076:     @staticmethod
0077:     def forward(ctx, x, alpha: float = 1.0):
0078:         if not isinstance(x, Tensor):
0079:             x = Tensor(x)
0080:             
0081:         ctx.save_for_backward(x)
0082:         ctx.save_arguments(alpha=alpha)
0083:         
0084:         return Tensor(np.where(x.data > 0, x.data, alpha * (np.exp(x.data) - 1)))
0085:         
0086:     @staticmethod
0087:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0088:         x, = ctx.saved_tensors
0089:         alpha = ctx.saved_arguments['alpha']
0090:         
0091:         if x.requires_grad:
0092:             grad = grad_output * np.where(x.data > 0, 1.0, alpha * np.exp(x.data))
0093:             grad_dict[id(x)] = grad
0094: 
0095: class GELU(Function):
0096:     """
0097:     Gaussian Error Linear Unit activation function.
0098:     
0099:     Forward: f(x) = x * Φ(x)
0100:     where Φ(x) is the Gaussian cumulative distribution function.
0101:     
0102:     This implementation uses the approximation:
0103:     f(x) ≈ 0.5x * (1 + tanh(√(2/π) * (x + 0.044715x³)))
0104:     """
0105:     
0106:     @staticmethod
0107:     def forward(ctx, x):
0108:         if not isinstance(x, Tensor):
0109:             x = Tensor(x)
0110:             
0111:         # Constants for the approximation
0112:         sqrt_2_over_pi = np.sqrt(2 / np.pi)
0113:         coeff = 0.044715
0114:         
0115:         # Compute intermediate values
0116:         x_cubed = x.data ** 3
0117:         inner = sqrt_2_over_pi * (x.data + coeff * x_cubed)
0118:         tanh_inner = np.tanh(inner)
0119:         
0120:         # Compute output
0121:         result = 0.5 * x.data * (1 + tanh_inner)
0122:         
0123:         # Save for backward pass
0124:         ctx.save_for_backward(x)
0125:         ctx.save_arguments(tanh_inner=tanh_inner)
0126:         
0127:         return Tensor(result)
0128:         
0129:     @staticmethod
0130:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0131:         x, = ctx.saved_tensors
0132:         tanh_inner = ctx.saved_arguments['tanh_inner']
0133:         
0134:         if x.requires_grad:
0135:             sqrt_2_over_pi = np.sqrt(2 / np.pi)
0136:             coeff = 0.044715
0137:             
0138:             # Compute derivative
0139:             x_cubed = x.data ** 3
0140:             inner = sqrt_2_over_pi * (x.data + coeff * x_cubed)
0141:             
0142:             # d/dx[GELU(x)] = 0.5 * (1 + tanh(inner)) + 
0143:             #                 0.5x * (1 - tanh²(inner)) * sqrt(2/π) * (1 + 3 * 0.044715x²)
0144:             grad = 0.5 * (1 + tanh_inner)
0145:             grad += 0.5 * x.data * (1 - tanh_inner ** 2) * sqrt_2_over_pi * (1 + 3 * coeff * x.data ** 2)
0146:             
0147:             grad_dict[id(x)] = grad_output * grad
0148: 
0149: class Sigmoid(Function):
0150:     """
0151:     Sigmoid activation function.
0152:     
0153:     Forward: f(x) = 1 / (1 + exp(-x))
0154:     Backward: f'(x) = f(x) * (1 - f(x))
0155:     """
0156:     
0157:     @staticmethod
0158:     def forward(ctx, x):
0159:         if not isinstance(x, Tensor):
0160:             x = Tensor(x)
0161:             
0162:         # Compute sigmoid with numerical stability
0163:         x_data = x.data
0164:         exp_neg_x = np.exp(-np.abs(x_data))
0165:         sigmoid_x = np.where(x_data >= 0, 
0166:                            1 / (1 + exp_neg_x),
0167:                            exp_neg_x / (1 + exp_neg_x))
0168:         
0169:         ctx.save_for_backward(x)
0170:         ctx.save_arguments(sigmoid_x=sigmoid_x)
0171:         return Tensor(sigmoid_x)
0172:         
0173:     @staticmethod
0174:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0175:         x, = ctx.saved_tensors
0176:         sigmoid_x = ctx.saved_arguments['sigmoid_x']
0177:         
0178:         if x.requires_grad:
0179:             grad = grad_output * sigmoid_x * (1 - sigmoid_x)
0180:             grad_dict[id(x)] = grad
0181: 
0182: class Tanh(Function):
0183:     """
0184:     Hyperbolic tangent activation function.
0185:     
0186:     Forward: f(x) = tanh(x)
0187:     Backward: f'(x) = 1 - tanh²(x)
0188:     """
0189:     
0190:     @staticmethod
0191:     def forward(ctx, x):
0192:         if not isinstance(x, Tensor):
0193:             x = Tensor(x)
0194:             
0195:         tanh_x = np.tanh(x.data)
0196:         ctx.save_for_backward(x)
0197:         ctx.save_arguments(tanh_x=tanh_x)
0198:         return Tensor(tanh_x)
0199:         
0200:     @staticmethod
0201:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0202:         x, = ctx.saved_tensors
0203:         tanh_x = ctx.saved_arguments['tanh_x']
0204:         
0205:         if x.requires_grad:
0206:             grad = grad_output * (1 - tanh_x ** 2)
0207:             grad_dict[id(x)] = grad
0208: 
0209: # Add convenience functions for each activation
0210: def relu(x: Tensor) -> Tensor:
0211:     """Applies ReLU activation function."""
0212:     return ReLU.apply(x)
0213: 
0214: def leaky_relu(x: Tensor, negative_slope: float = 0.01) -> Tensor:
0215:     """Applies Leaky ReLU activation function."""
0216:     return LeakyReLU.apply(x, negative_slope)
0217: 
0218: def elu(x: Tensor, alpha: float = 1.0) -> Tensor:
0219:     """Applies ELU activation function."""
0220:     return ELU.apply(x, alpha)
0221: 
0222: def gelu(x: Tensor) -> Tensor:
0223:     """Applies GELU activation function."""
0224:     return GELU.apply(x)
0225: 
0226: def sigmoid(x: Tensor) -> Tensor:
0227:     """Applies Sigmoid activation function."""
0228:     return Sigmoid.apply(x)
0229: 
0230: def tanh(x: Tensor) -> Tensor:
0231:     """Applies Tanh activation function."""
0232:     return Tanh.apply(x)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\conv2d.py
// ----------------------------------------
0001: from typing import Tuple, Optional, Union
0002: import numpy as np
0003: from ..core import Tensor
0004: from .modules import Module
0005: from ..ops.cnn import Conv2dFunction
0006: 
0007: def _pair(x: Union[int, Tuple[int, int]]) -> Tuple[int, int]:
0008:     """Convert input to a pair of values."""
0009:     if isinstance(x, tuple):
0010:         return x
0011:     return (x, x)
0012: 
0013: class Conv2d(Module):
0014:     """
0015:     Applies a 2D convolution over an input signal composed of several input planes.
0016:     
0017:     Args:
0018:         in_channels (int): Number of channels in the input image
0019:         out_channels (int): Number of channels produced by the convolution
0020:         kernel_size (int or tuple): Size of the convolving kernel
0021:         stride (int or tuple, optional): Stride of the convolution. Default: 1
0022:         padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0
0023:         dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
0024:         groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 
0025:         bias (bool, optional): If True, adds a learnable bias to the output. Default: True
0026:         
0027:     Shape:
0028:         - Input: (N, C_in, H, W)
0029:         - Output: (N, C_out, H_out, W_out)
0030:           where
0031:           H_out = (H + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1
0032:           W_out = (W + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1
0033:     """
0034:     
0035:     def __init__(
0036:         self,
0037:         in_channels: int,
0038:         out_channels: int,
0039:         kernel_size: Union[int, Tuple[int, int]],
0040:         stride: Union[int, Tuple[int, int]] = 1,
0041:         padding: Union[int, Tuple[int, int]] = 0,
0042:         dilation: Union[int, Tuple[int, int]] = 1,
0043:         groups: int = 1,
0044:         bias: bool = True
0045:     ):
0046:         super().__init__()
0047:         
0048:         if in_channels % groups != 0:
0049:             raise ValueError('in_channels must be divisible by groups')
0050:         if out_channels % groups != 0:
0051:             raise ValueError('out_channels must be divisible by groups')
0052:             
0053:         self.in_channels = in_channels
0054:         self.out_channels = out_channels
0055:         self.kernel_size = _pair(kernel_size)
0056:         self.stride = _pair(stride)
0057:         self.padding = _pair(padding)
0058:         self.dilation = _pair(dilation)
0059:         self.groups = groups
0060:         
0061:         # Initialize weights using He initialization
0062:         # Adjust fan_in to account for groups
0063:         fan_in = in_channels // groups * self.kernel_size[0] * self.kernel_size[1]
0064:         bound = np.sqrt(2.0 / fan_in)
0065:         weight_shape = (out_channels, in_channels // groups, *self.kernel_size)
0066:         weight = Tensor(
0067:             np.random.uniform(-bound, bound, weight_shape),
0068:             requires_grad=True
0069:         )
0070:         self.register_parameter('weight', weight)
0071:         
0072:         if bias:
0073:             # Initialize bias to zero
0074:             bias_data = np.zeros(out_channels)
0075:             self.register_parameter('bias', Tensor(bias_data, requires_grad=True))
0076:         else:
0077:             self.register_parameter('bias', None)
0078:             
0079:     def forward(self, x: Tensor) -> Tensor:
0080:         """
0081:         Forward pass of the convolution layer.
0082:         
0083:         Args:
0084:             x: Input tensor of shape (N, C_in, H, W)
0085:             
0086:         Returns:
0087:             Output tensor of shape (N, C_out, H_out, W_out)
0088:         """
0089:         return Conv2dFunction.apply(
0090:             x, self.weight, self.bias,
0091:             self.stride, self.padding,
0092:             self.dilation, self.groups
0093:         )
0094:         
0095:     def extra_repr(self) -> str:
0096:         """Returns a string with extra representation information."""
0097:         s = (f'{self.in_channels}, {self.out_channels}, '
0098:              f'kernel_size={self.kernel_size}')
0099:         
0100:         if self.stride != (1, 1):
0101:             s += f', stride={self.stride}'
0102:         if self.padding != (0, 0):
0103:             s += f', padding={self.padding}'
0104:         if self.dilation != (1, 1):
0105:             s += f', dilation={self.dilation}'
0106:         if self.groups != 1:
0107:             s += f', groups={self.groups}'
0108:         if self.bias is None:
0109:             s += ', bias=False'
0110:         return s
0111: 
0112:     @staticmethod
0113:     def calc_output_shape(
0114:         input_shape: Tuple[int, ...],
0115:         out_channels: int,
0116:         kernel_size: Tuple[int, int],
0117:         stride: Tuple[int, int],
0118:         padding: Tuple[int, int],
0119:         dilation: Tuple[int, int]
0120:     ) -> Tuple[int, int, int, int]:
0121:         """
0122:         Calculate the output shape of the convolution.
0123:         
0124:         Args:
0125:             input_shape: Input shape (N, C_in, H, W)
0126:             out_channels: Number of output channels
0127:             kernel_size: Size of the kernel
0128:             stride: Stride of the convolution
0129:             padding: Zero-padding added to both sides
0130:             dilation: Spacing between kernel elements
0131:             
0132:         Returns:
0133:             Output shape (N, C_out, H_out, W_out)
0134:         """
0135:         N, _, H, W = input_shape
0136:         
0137:         H_out = ((H + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) 
0138:                 // stride[0] + 1)
0139:         W_out = ((W + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) 
0140:                 // stride[1] + 1)
0141:         
0142:         return (N, out_channels, H_out, W_out)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\linear.py
// ----------------------------------------
0001: from typing import Optional, Dict
0002: import numpy as np
0003: from ..core import Tensor, Function
0004: from .modules import Module
0005: 
0006: class LinearFunction(Function):
0007:     @staticmethod
0008:     def forward(ctx, input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
0009:         # Save tensors needed for backward pass
0010:         ctx.save_for_backward(input, weight, bias)
0011:         
0012:         # Compute output: y = xW^T + b
0013:         output = input.data @ weight.data
0014:         if bias is not None:
0015:             output += bias.data
0016:             
0017:         return Tensor(output)
0018:     
0019:     @staticmethod
0020:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0021:         # Retrieve saved tensors
0022:         input, weight, bias = ctx.saved_tensors
0023:         
0024:         # Compute gradient with respect to input: dx = dout @ W
0025:         if input.requires_grad:
0026:             grad_dict[id(input)] = grad_output @ weight.data.T
0027:             
0028:         # Compute gradient with respect to weight: dW = x^T @ dout
0029:         if weight.requires_grad:
0030:             grad_dict[id(weight)] = input.data.T @ grad_output
0031:             
0032:         # Compute gradient with respect to bias: db = sum(dout, dim=0)
0033:         if bias is not None and bias.requires_grad:
0034:             grad_dict[id(bias)] = grad_output.sum(axis=0)
0035: 
0036: class Linear(Module):
0037:     """
0038:     Applies a linear transformation to the incoming data: y = xW^T + b
0039:     
0040:     Args:
0041:         in_features: size of each input sample
0042:         out_features: size of each output sample
0043:         bias: If set to False, the layer will not learn an additive bias
0044:         
0045:     Shape:
0046:         - Input: (batch_size, in_features)
0047:         - Output: (batch_size, out_features)
0048:         
0049:     Attributes:
0050:         weight: the learnable weights of shape (in_features, out_features)
0051:         bias: the learnable bias of shape (out_features,)
0052:     """
0053:     
0054:     def __init__(self, in_features: int, out_features: int, bias: bool = True):
0055:         super().__init__()
0056:         
0057:         self.in_features = in_features
0058:         self.out_features = out_features
0059:         
0060:         # Initialize weights using He initialization
0061:         bound = np.sqrt(2.0 / in_features)
0062:         weight = Tensor(
0063:             np.random.uniform(-bound, bound, (in_features, out_features)),
0064:             requires_grad=True
0065:         )
0066:         self.register_parameter('weight', weight)
0067:         
0068:         if bias:
0069:             bias = Tensor(np.zeros(out_features), requires_grad=True)
0070:             self.register_parameter('bias', bias)
0071:         else:
0072:             self.register_parameter('bias', None)
0073:             
0074:     def forward(self, input: Tensor) -> Tensor:
0075:         """Forward pass of the linear layer."""
0076:         return LinearFunction.apply(input, self.weight, self.bias)
0077:             
0078:     def extra_repr(self) -> str:
0079:         """Extra information to add to the string representation."""
0080:         return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\modules.py
// ----------------------------------------
0001: from typing import Iterator, Dict, Any, Optional, Union
0002: from collections import OrderedDict
0003: from ..core import Tensor
0004: 
0005: class Module:
0006:     """
0007:     Base class for all neural network modules.
0008:     
0009:     Your models should also subclass this class.
0010:     Modules can also contain other Modules, allowing to nest them in
0011:     a tree structure.
0012:     """
0013:     
0014:     def __init__(self):
0015:         """Initialize the module."""
0016:         # First set these directly to avoid triggering __setattr__
0017:         object.__setattr__(self, 'training', True)
0018:         object.__setattr__(self, '_parameters', OrderedDict())
0019:         object.__setattr__(self, '_buffers', OrderedDict())
0020:         object.__setattr__(self, '_modules', OrderedDict())
0021:         
0022:     def register_parameter(self, name: str, param: Optional[Tensor]) -> None:
0023:         """Add a parameter to the module.
0024:         
0025:         Args:
0026:             name: Name of the parameter
0027:             param: The parameter tensor to register
0028:         """
0029:         if '_parameters' not in self.__dict__:
0030:             raise TypeError(
0031:                 "cannot assign parameter before Module.__init__() call"
0032:             )
0033:             
0034:         if param is not None and not isinstance(param, Tensor):
0035:             raise TypeError(f"Parameter {name} must be a Tensor, not {type(param)}")
0036:             
0037:         self._parameters[name] = param
0038:         
0039:     def register_buffer(self, name: str, tensor: Optional[Tensor]) -> None:
0040:         """Add a persistent buffer to the module.
0041:         
0042:         Buffers are typically used for running statistics in modules like BatchNorm.
0043:         
0044:         Args:
0045:             name: Name of the buffer
0046:             tensor: The tensor to register as a buffer
0047:         """
0048:         if '_buffers' not in self.__dict__:
0049:             raise TypeError(
0050:                 "cannot assign buffer before Module.__init__() call"
0051:             )
0052:             
0053:         if tensor is not None and not isinstance(tensor, Tensor):
0054:             raise TypeError(f"Buffer {name} must be a Tensor, not {type(tensor)}")
0055:             
0056:         self._buffers[name] = tensor
0057:         
0058:     def add_module(self, name: str, module: Optional['Module']) -> None:
0059:         """Add a child module to the current module.
0060:         
0061:         Args:
0062:             name: Name of the child module
0063:             module: The module to add
0064:         """
0065:         if not isinstance(module, (Module, type(None))):
0066:             raise TypeError(f"{name} is not a Module subclass")
0067:             
0068:         if '_modules' not in self.__dict__:
0069:             raise TypeError(
0070:                 "cannot assign module before Module.__init__() call"
0071:             )
0072:             
0073:         self._modules[name] = module
0074:         
0075:     def __getattr__(self, name: str) -> Any:
0076:         """Custom getattr that looks through parameters, buffers, and modules."""
0077:         if '_parameters' in self.__dict__:
0078:             _parameters = self.__dict__['_parameters']
0079:             if name in _parameters:
0080:                 return _parameters[name]
0081:                 
0082:         if '_buffers' in self.__dict__:
0083:             _buffers = self.__dict__['_buffers']
0084:             if name in _buffers:
0085:                 return _buffers[name]
0086:                 
0087:         if '_modules' in self.__dict__:
0088:             modules = self.__dict__['_modules']
0089:             if name in modules:
0090:                 return modules[name]
0091:                 
0092:         raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
0093:         
0094:     def __setattr__(self, name: str, value: Any) -> None:
0095:         """Custom setattr that handles parameter registration."""
0096:         # Handle special module attributes first
0097:         if name in ['training']:
0098:             object.__setattr__(self, name, value)
0099:             return
0100:             
0101:         if isinstance(value, Tensor):
0102:             if not hasattr(self, '_parameters'):
0103:                 raise TypeError(
0104:                     "cannot assign parameters before Module.__init__() call"
0105:                 )
0106:             self.register_parameter(name, value)
0107:         elif isinstance(value, Module):
0108:             if not hasattr(self, '_modules'):
0109:                 raise TypeError(
0110:                     "cannot assign module before Module.__init__() call"
0111:                 )
0112:             self.add_module(name, value)
0113:         else:
0114:             object.__setattr__(self, name, value)
0115:             
0116:     def parameters(self) -> Iterator[Tensor]:
0117:         """Returns an iterator over module parameters."""
0118:         for param in self._parameters.values():
0119:             if param is not None:
0120:                 yield param
0121:         for module in self._modules.values():
0122:             if module is not None:
0123:                 yield from module.parameters()
0124:                 
0125:     def named_parameters(self) -> Iterator[tuple[str, Tensor]]:
0126:         """Returns an iterator over module parameters, yielding both the
0127:         name of the parameter as well as the parameter itself."""
0128:         for name, param in self._parameters.items():
0129:             if param is not None:
0130:                 yield name, param
0131:         for mname, module in self._modules.items():
0132:             if module is not None:
0133:                 for name, param in module.named_parameters():
0134:                     yield f"{mname}.{name}", param
0135:                     
0136:     def train(self, mode: bool = True) -> 'Module':
0137:         """Sets the module in training mode."""
0138:         self.training = mode
0139:         for module in self._modules.values():
0140:             if module is not None:
0141:                 module.train(mode)
0142:         return self
0143:         
0144:     def eval(self) -> 'Module':
0145:         """Sets the module in evaluation mode."""
0146:         return self.train(False)
0147:         
0148:     def __call__(self, *args, **kwargs):
0149:         return self.forward(*args, **kwargs)
0150:         
0151:     def forward(self, *args, **kwargs):
0152:         """Define the computation performed at every call."""
0153:         raise NotImplementedError
0154:         
0155:     def __repr__(self):
0156:         """Returns a string representation of the module."""
0157:         extra_lines = []
0158:         extra_repr = self.extra_repr()
0159:         if extra_repr:
0160:             extra_lines = extra_repr.split('\n')
0161:             
0162:         child_lines = []
0163:         for key, module in self._modules.items():
0164:             mod_str = repr(module)
0165:             mod_str = _addindent(mod_str, 2)
0166:             child_lines.append('(' + key + '): ' + mod_str)
0167:             
0168:         lines = extra_lines + child_lines
0169:         
0170:         main_str = self.__class__.__name__ + '('
0171:         if lines:
0172:             main_str += '\n  ' + '\n  '.join(lines) + '\n'
0173:         main_str += ')'
0174:         return main_str
0175:         
0176:     def extra_repr(self) -> str:
0177:         """Set the extra representation of the module."""
0178:         return ''
0179: 
0180: def _addindent(s_: str, numSpaces: int) -> str:
0181:     """Helper for indenting multiline strings."""
0182:     s = s_.split('\n')
0183:     if len(s) == 1:
0184:         return s_
0185:     first = s.pop(0)
0186:     s = [(numSpaces * ' ') + line for line in s]
0187:     return '\n'.join([first] + s)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\nn\sequential.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\__init__.py
// ----------------------------------------
0001: """
0002: Operations module for DLpy.
0003: 
0004: This module contains all the mathematical operations that can be performed on tensors.
0005: """
0006: 
0007: from .basic import Add, Multiply
0008: from .reshape import Reshape
0009: from .power import Power, Divide
0010: from .elementwise import Log, Exp
0011: from .reduction import Sum, Mean, Max
0012: from .matrix import (
0013:     Transpose,
0014:     Greater,
0015:     GreaterEqual,
0016:     Less,
0017:     LessEqual,
0018:     Equal,
0019:     NotEqual
0020: )
0021: from .loss import (
0022:     MSELoss,
0023:     CrossEntropyLoss,
0024:     BinaryCrossEntropyLoss,
0025:     L1Loss,
0026:     HuberLoss,
0027:     KLDivLoss,
0028:     CosineSimilarityLoss,
0029:     HingeLoss,
0030:     FocalLoss
0031: )
0032: 
0033: from .cnn import Conv2dFunction
0034: 
0035: __all__ = [
0036:     # Basic operations
0037:     'Add',
0038:     'Multiply',
0039:     'Reshape',
0040:     
0041:     # Power operations
0042:     'Power',
0043:     'Divide',
0044:     
0045:     # Element-wise operations
0046:     'Log',
0047:     'Exp',
0048:     
0049:     # Reduction operations
0050:     'Sum',
0051:     'Mean',
0052:     'Max',
0053:     
0054:     # Matrix operations
0055:     'Transpose',
0056:     
0057:     # Comparison operations
0058:     'Greater',
0059:     'GreaterEqual',
0060:     'Less',
0061:     'LessEqual',
0062:     'Equal',
0063:     'NotEqual',
0064: 
0065:     # Loss functions
0066:     'MSELoss',
0067:     'CrossEntropyLoss',
0068:     'BinaryCrossEntropyLoss',
0069:     'L1Loss',
0070:     'HuberLoss',
0071:     'KLDivLoss',
0072:     'CosineSimilarityLoss',
0073:     'HingeLoss',
0074:     'FocalLoss',
0075: 
0076:     # CNN operations
0077:     'Conv2dFunction'
0078: ]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\basic.py
// ----------------------------------------
0001: from typing import Dict  # Add this import at the top
0002: from ..core.function import Function
0003: from ..core.tensor import Tensor
0004: import numpy as np
0005: 
0006: class Add(Function):
0007:     @staticmethod
0008:     def forward(ctx, a, b):
0009:         if not isinstance(a, Tensor):
0010:             a = Tensor(a)
0011:         if not isinstance(b, Tensor):
0012:             b = Tensor(b)
0013:             
0014:         shape_a = a.data.shape
0015:         shape_b = b.data.shape
0016: 
0017:         # Check valid broadcasting manually
0018:         if len(shape_a) == 2 and shape_a[0] == 1 and len(shape_b) == 1:
0019:             # Special case: (1,N) matrix with (M,) vector requires N==M
0020:             if shape_a[1] != shape_b[0]:
0021:                 raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
0022:         elif len(shape_a) == 1 and len(shape_b) == 2 and shape_b[0] == 1:
0023:             # Special case: (N,) vector with (1,M) matrix requires N==M
0024:             if shape_a[0] != shape_b[1]:
0025:                 raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
0026:                 
0027:         # Save tensors for backward pass
0028:         ctx.save_for_backward(a, b)
0029:         
0030:         # If we get here, try the operation
0031:         try:
0032:             result = a.data + b.data
0033:             return Tensor(result)
0034:         except ValueError:
0035:             raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
0036:         
0037:     @staticmethod
0038:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0039:         a, b = ctx.saved_tensors
0040: 
0041:         if a.requires_grad:
0042:             grad_a = grad_output
0043:             grad_a = Add._reduce_grad(grad_a, a.data.shape)
0044:             if id(a) not in grad_dict or grad_dict[id(a)] is None:
0045:                 grad_dict[id(a)] = grad_a
0046:             else:
0047:                 grad_dict[id(a)] += grad_a  # Accumulate gradients
0048: 
0049:         if b.requires_grad:
0050:             grad_b = grad_output
0051:             grad_b = Add._reduce_grad(grad_b, b.data.shape)
0052:             if id(b) not in grad_dict or grad_dict[id(b)] is None:
0053:                 grad_dict[id(b)] = grad_b
0054:             else:
0055:                 grad_dict[id(b)] += grad_b  # Accumulate gradients
0056: 
0057:     @staticmethod
0058:     def _reduce_grad(grad, target_shape):
0059:         """
0060:         Reduces the gradient to match the target shape by summing over broadcasted dimensions.
0061:         """
0062:         # Convert target_shape to a tuple if it's not
0063:         if not isinstance(target_shape, tuple):
0064:             target_shape = tuple(target_shape)
0065:         
0066:         # Align the dimensions by prepending 1s if necessary
0067:         grad_shape = grad.shape
0068:         target_shape = (1,) * (len(grad_shape) - len(target_shape)) + target_shape
0069:         for axis, (grad_dim, target_dim) in enumerate(zip(grad_shape, target_shape)):
0070:             if target_dim == 1 and grad_dim != 1:
0071:                 grad = grad.sum(axis=axis, keepdims=True)
0072:         return grad
0073: 
0074: class Multiply(Function):
0075:     @staticmethod
0076:     def forward(ctx, a, b):
0077:         if not isinstance(a, Tensor):
0078:             a = Tensor(a)
0079:         if not isinstance(b, Tensor):
0080:             b = Tensor(b)
0081:             
0082:         shape_a = a.data.shape
0083:         shape_b = b.data.shape
0084:         
0085:         # Check if shapes can be broadcast according to NumPy rules
0086:         try:
0087:             # Test broadcast compatibility without actually performing the operation
0088:             np.broadcast_shapes(shape_a, shape_b)
0089:             # If we get here, shapes are compatible
0090:             result = a.data * b.data
0091:             ctx.save_for_backward(a, b)
0092:             return Tensor(result)
0093:         except ValueError:
0094:             raise ValueError(f"Cannot broadcast shape {shape_a} with {shape_b}")
0095:         
0096:     @staticmethod
0097:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0098:         a, b = ctx.saved_tensors
0099: 
0100:         if a.requires_grad:
0101:             grad_a = grad_output * b.data
0102:             grad_a = Multiply._reduce_grad(grad_a, a.data.shape)
0103:             if id(a) not in grad_dict or grad_dict[id(a)] is None:
0104:                 grad_dict[id(a)] = grad_a
0105:             else:
0106:                 grad_dict[id(a)] += grad_a  # Accumulate gradients
0107: 
0108:         if b.requires_grad:
0109:             grad_b = grad_output * a.data
0110:             grad_b = Multiply._reduce_grad(grad_b, b.data.shape)
0111:             if id(b) not in grad_dict or grad_dict[id(b)] is None:
0112:                 grad_dict[id(b)] = grad_b
0113:             else:
0114:                 grad_dict[id(b)] += grad_b  # Accumulate gradients
0115: 
0116:     @staticmethod
0117:     def _reduce_grad(grad, target_shape):
0118:         """
0119:         Reduces the gradient to match the target shape by summing over broadcasted dimensions.
0120:         """
0121:         # Convert target_shape to a tuple if it's not
0122:         if not isinstance(target_shape, tuple):
0123:             target_shape = tuple(target_shape)
0124:         
0125:         # Align the dimensions by prepending 1s if necessary
0126:         grad_shape = grad.shape
0127:         target_shape = (1,) * (len(grad_shape) - len(target_shape)) + target_shape
0128:         for axis, (grad_dim, target_dim) in enumerate(zip(grad_shape, target_shape)):
0129:             if target_dim == 1 and grad_dim != 1:
0130:                 grad = grad.sum(axis=axis, keepdims=True)
0131:         return grad

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\cnn.py
// ----------------------------------------
0001: from typing import Tuple, Dict, Optional, Union, List
0002: import numpy as np
0003: from ..core import Function, Tensor, Context
0004: 
0005: class ConvMode:
0006:     """Enumeration of convolution modes."""
0007:     STANDARD = "standard"
0008:     TRANSPOSED = "transposed"
0009:     DEFORMABLE = "deformable"
0010: 
0011: def _validate_conv_params(
0012:     x_shape: tuple,
0013:     weight_shape: tuple,
0014:     stride: tuple,
0015:     padding: tuple,
0016:     dilation: tuple,
0017:     groups: int,
0018:     mode: str = ConvMode.STANDARD,
0019:     offset: Optional[Tensor] = None,
0020:     weight: Optional[Tensor] = None,
0021:     mask: Optional[Tensor] = None
0022: ) -> None:
0023:     """Validates convolution parameters."""
0024:     N, C_in, H, W = x_shape
0025:     C_out, C_in_per_group, kH, kW = weight_shape
0026: 
0027:     # Basic validations for all modes
0028:     if mode not in [ConvMode.STANDARD, ConvMode.TRANSPOSED, ConvMode.DEFORMABLE]:
0029:         raise ValueError(f"Invalid convolution mode: {mode}")
0030: 
0031:     # Validate groups configuration
0032:     if C_in % groups != 0:
0033:         raise ValueError(f"Input channels ({C_in}) must be divisible by groups ({groups})")
0034:     if C_out % groups != 0:
0035:         raise ValueError(f"Output channels ({C_out}) must be divisible by groups ({groups})")
0036: 
0037:     # Validate kernel dimensions
0038:     if kH <= 0 or kW <= 0:
0039:         raise ValueError(f"Kernel dimensions must be positive, got ({kH}, {kW})")
0040: 
0041:     # Validate stride and dilation
0042:     if any(s <= 0 for s in stride):
0043:         raise ValueError(f"Stride values must be positive, got {stride}")
0044:     if any(d <= 0 for d in dilation):
0045:         raise ValueError(f"Dilation values must be positive, got {dilation}")
0046:     
0047:     # Validate padding
0048:     if any(p < 0 for p in padding):
0049:         raise ValueError(f"Padding values must be non-negative, got {padding}")
0050: 
0051:     if mode == ConvMode.TRANSPOSED:
0052:         # For transposed conv:
0053:         # - x shape is (N, C_in, H, W)
0054:         # - weight shape should be (C_out, C_in, kH, kW)
0055:         # - C_in from x should match C_in_per_group * groups from weight
0056:         if C_in_per_group != C_in // groups:
0057:             raise ValueError(
0058:                 f"For transposed conv, expected {C_in // groups} input channels per group, "
0059:                 f"got {C_in_per_group}"
0060:             )
0061:             
0062:         # Calculate and validate output size
0063:         H_out = (H - 1) * stride[0] - 2 * padding[0] + kH
0064:         W_out = (W - 1) * stride[1] - 2 * padding[1] + kW
0065:         if H_out <= 0 or W_out <= 0:
0066:             raise ValueError(
0067:                 f"Transposed conv output size would be negative or zero: ({H_out}, {W_out})"
0068:             )
0069:     
0070:     else:  # Standard and deformable validation
0071:         # Validate channels per group
0072:         if C_in_per_group != C_in // groups:
0073:             raise ValueError(f"Expected {C_in // groups} input channels per group, got {C_in_per_group}")
0074:         
0075:         # Calculate and validate output size
0076:         H_out = ((H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0077:         W_out = ((W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0078:         if H_out <= 0 or W_out <= 0:
0079:             raise ValueError(
0080:                 f"Conv output size would be negative or zero: ({H_out}, {W_out})"
0081:             )
0082: 
0083:     if mode == ConvMode.DEFORMABLE:
0084:         # Validate offset tensor presence and shape
0085:         if offset is None and weight is not None:
0086:             offset = getattr(weight, 'offset', None)
0087:             
0088:         if offset is None:
0089:             raise ValueError("Deformable convolution requires offset parameter")
0090:         
0091:         # Calculate output size for offset validation
0092:         H_out = (H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0] + 1
0093:         W_out = (W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1] + 1
0094:         
0095:         # Validate offset tensor shape
0096:         expected_offset_shape = (N, 2 * kH * kW, H_out, W_out)
0097:         if offset.shape != expected_offset_shape:
0098:             raise ValueError(f"Expected offset shape {expected_offset_shape}, got {offset.shape}")
0099:             
0100:         # Validate mask tensor if present
0101:         if mask is not None:
0102:             expected_mask_shape = (N, kH * kW, H_out, W_out)
0103:             if mask.shape != expected_mask_shape:
0104:                 raise ValueError(f"Expected mask shape {expected_mask_shape}, got {mask.shape}")
0105: 
0106: def _pad_input(x: np.ndarray, padding: Tuple[int, int]) -> np.ndarray:
0107:     """
0108:     Pads input tensor with zeros.
0109:     
0110:     Args:
0111:         x: Input tensor
0112:         padding: (padding_height, padding_width)
0113:     """
0114:     if padding[0] == 0 and padding[1] == 0:
0115:         return x
0116:     pad_width = ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1]))
0117:     return np.pad(x, pad_width, mode='constant', constant_values=0)
0118: 
0119: def _get_output_shape(input_shape: Tuple[int, ...], kernel_size: Tuple[int, int],
0120:                     stride: Tuple[int, int], padding: Tuple[int, int],
0121:                     dilation: Tuple[int, int], mode: str = ConvMode.STANDARD,
0122:                     output_padding: Tuple[int, int] = (0, 0)) -> Tuple[int, int]:
0123:     """
0124:     Calculates output shape for different convolution types.
0125:     
0126:     Args:
0127:         input_shape: Shape of input tensor (N, C, H, W)
0128:         kernel_size: Size of convolution kernel (kH, kW)
0129:         stride: Stride of convolution (sH, sW)
0130:         padding: Zero-padding size (pH, pW)
0131:         dilation: Dilation rate (dH, dW)
0132:         mode: Convolution mode (standard, transposed, or deformable)
0133:         output_padding: Additional size added to output shape (only for transposed conv)
0134:         
0135:     Returns:
0136:         Tuple of output height and width (H_out, W_out)
0137:     """
0138:     if mode == ConvMode.STANDARD:
0139:         H = ((input_shape[2] + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) 
0140:              // stride[0] + 1)
0141:         W = ((input_shape[3] + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) 
0142:              // stride[1] + 1)
0143:     elif mode == ConvMode.TRANSPOSED:
0144:         H = (input_shape[2] - 1) * stride[0] - 2 * padding[0] + kernel_size[0] + output_padding[0]
0145:         W = (input_shape[3] - 1) * stride[1] - 2 * padding[1] + kernel_size[1] + output_padding[1]
0146:     else:  # Deformable follows standard conv shape
0147:         H = ((input_shape[2] + 2 * padding[0] - kernel_size[0]) // stride[0] + 1)
0148:         W = ((input_shape[3] + 2 * padding[1] - kernel_size[1]) // stride[1] + 1)
0149:     return H, W
0150: 
0151: def _get_deformable_offsets(offset_data: np.ndarray, 
0152:                          kernel_size: Tuple[int, int],
0153:                          input_shape: Tuple[int, ...],
0154:                          dilation: Tuple[int, int] = (1, 1)) -> np.ndarray:
0155:     """
0156:     Computes sampling locations for deformable convolution.
0157:     
0158:     Args:
0159:         offset_data: Offset tensor of shape (N, 2*kH*kW, H_out, W_out)
0160:         kernel_size: Tuple of (kH, kW)
0161:         input_shape: Shape of input tensor (N, C, H, W)
0162:         dilation: Dilation rate
0163:         
0164:     Returns:
0165:         Sampling locations of shape (N, H_out*W_out, kH*kW, 2)
0166:     """
0167:     N, _, H_out, W_out = offset_data.shape
0168:     kH, kW = kernel_size
0169:     
0170:     # Generate base grid for the kernel
0171:     h_range = np.arange(kH) * dilation[0]
0172:     w_range = np.arange(kW) * dilation[1]
0173:     h_grid, w_grid = np.meshgrid(h_range, w_range, indexing='ij')
0174:     kernel_grid = np.stack([h_grid, w_grid], axis=-1)  # (kH, kW, 2)
0175:     kernel_grid = kernel_grid.reshape(-1, 2)  # (kH*kW, 2)
0176:     
0177:     # Reshape offsets
0178:     offset = offset_data.reshape(N, 2, kH*kW, H_out, W_out)
0179:     offset = offset.transpose(0, 3, 4, 2, 1)  # (N, H_out, W_out, kH*kW, 2)
0180:     offset = offset.reshape(N, H_out*W_out, kH*kW, 2)
0181:     
0182:     # Add base grid to offsets
0183:     kernel_grid = np.expand_dims(np.expand_dims(kernel_grid, 0), 0)  # (1, 1, kH*kW, 2)
0184:     sampling_locations = kernel_grid + offset
0185:     
0186:     return sampling_locations
0187: 
0188: def _bilinear_interpolate(input: np.ndarray, points: np.ndarray, align_corners: bool = True) -> np.ndarray:
0189:     """
0190:     Performs bilinear interpolation on the input tensor at specified points.
0191:     
0192:     Args:
0193:         input: Input tensor (N, C, H, W)
0194:         points: Points to sample (N, P, 2) in normalized coordinates [-1, 1]
0195:         align_corners: Whether to align corners in interpolation
0196:         
0197:     Returns:
0198:         Interpolated values (N, C, P)
0199:     """
0200:     N, C, H, W = input.shape
0201:     
0202:     # Ensure points is correct shape (N, P, 2)
0203:     if points.ndim == 4:
0204:         points = points.reshape(points.shape[0], -1, 2)
0205:     
0206:     _, P, _ = points.shape
0207:     
0208:     # Convert normalized coordinates to pixel coordinates
0209:     if align_corners:
0210:         x = (points[..., 0] + 1) * (W - 1) / 2
0211:         y = (points[..., 1] + 1) * (H - 1) / 2
0212:     else:
0213:         x = ((points[..., 0] + 1) * W - 1) / 2
0214:         y = ((points[..., 1] + 1) * H - 1) / 2
0215:     
0216:     # Get corner indices
0217:     x0 = np.floor(x).astype(np.int32)
0218:     x1 = x0 + 1
0219:     y0 = np.floor(y).astype(np.int32)
0220:     y1 = y0 + 1
0221:     
0222:     # Clip to image boundaries
0223:     x0 = np.clip(x0, 0, W - 1)
0224:     x1 = np.clip(x1, 0, W - 1)
0225:     y0 = np.clip(y0, 0, H - 1)
0226:     y1 = np.clip(y1, 0, H - 1)
0227:     
0228:     # Calculate interpolation weights
0229:     wa = (x1 - x) * (y1 - y)
0230:     wb = (x1 - x) * (y - y0)
0231:     wc = (x - x0) * (y1 - y)
0232:     wd = (x - x0) * (y - y0)
0233:     
0234:     # Reshape weights for broadcasting
0235:     wa = wa[..., None]
0236:     wb = wb[..., None]
0237:     wc = wc[..., None]
0238:     wd = wd[..., None]
0239:     
0240:     # Gather corner values and compute weighted sum
0241:     output = np.zeros((N, C, P))
0242:     for n in range(N):
0243:         output[n] = (wa[n] * input[n, :, y0[n], x0[n]] +
0244:                     wb[n] * input[n, :, y1[n], x0[n]] +
0245:                     wc[n] * input[n, :, y0[n], x1[n]] +
0246:                     wd[n] * input[n, :, y1[n], x1[n]])
0247:     
0248:     return output
0249: 
0250: def _im2col_dilated(x: np.ndarray, 
0251:                     kernel_size: Tuple[int, int],
0252:                     stride: Tuple[int, int], 
0253:                     dilation: Tuple[int, int],
0254:                     padding: Tuple[int, int],
0255:                     mode: str = ConvMode.STANDARD,
0256:                     sampling_locations: Optional[np.ndarray] = None) -> np.ndarray:
0257:     """Rearranges dilated image blocks into columns."""
0258:     N, C, H, W = x.shape
0259:     kH, kW = kernel_size
0260:     
0261:     # Calculate output dimensions
0262:     H_out = ((H - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0263:     W_out = ((W - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0264:     
0265:     # Initialize output array
0266:     # For standard convolution:
0267:     # - Each column represents a dot product of the kernel with a specific output position
0268:     # - Number of rows = C * kH * kW (all values needed for one output value)
0269:     # - Number of columns = N * H_out * W_out (total number of output values)
0270:     cols = np.zeros((C * kH * kW, N * H_out * W_out))
0271:     
0272:     # Process each input position in the kernel
0273:     for c in range(C):
0274:         for kh in range(kH):
0275:             for kw in range(kW):
0276:                 h_start = np.arange(H_out) * stride[0]
0277:                 w_start = np.arange(W_out) * stride[1]
0278:                 
0279:                 h_offset = kh * dilation[0]
0280:                 w_offset = kw * dilation[1]
0281:                 
0282:                 h_pos, w_pos = np.meshgrid(h_start + h_offset, w_start + w_offset, indexing='ij')
0283:                 h_pos = h_pos.reshape(-1)
0284:                 w_pos = w_pos.reshape(-1)
0285:                 
0286:                 row_idx = c * kH * kW + kh * kW + kw
0287:                 for n in range(N):
0288:                     col_idx = n * H_out * W_out + np.arange(H_out * W_out)
0289:                     cols[row_idx, col_idx] = x[n, c, h_pos, w_pos]
0290:     
0291:     print(f"im2col output shape: {cols.shape}")
0292:     print(f"Expected reshape: C*kH*kW={C*kH*kW}, N={N}, H_out={H_out}, W_out={W_out}")
0293:     return cols
0294: 
0295: def _get_output_size(input_shape: Tuple[int, ...], 
0296:                     kernel_size: Tuple[int, int],
0297:                     stride: Tuple[int, int], 
0298:                     padding: Tuple[int, int],
0299:                     dilation: Tuple[int, int],
0300:                     mode: str) -> Tuple[int, int]:
0301:     """Calculate output dimensions for different convolution modes."""
0302:     _, _, H, W = input_shape
0303:     kH, kW = kernel_size
0304: 
0305:     if mode == ConvMode.TRANSPOSED:
0306:         H_out = (H - 1) * stride[0] - 2 * padding[0] + kH
0307:         W_out = (W - 1) * stride[1] - 2 * padding[1] + kW
0308:     else:
0309:         H_out = ((H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0310:         W_out = ((W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0311: 
0312:     return H_out, W_out
0313: 
0314: def _col2im_dilated(cols: np.ndarray, output_size: Tuple[int, ...],
0315:                     kernel_size: Tuple[int, int], stride: Tuple[int, int],
0316:                     dilation: Tuple[int, int], mode: str = ConvMode.STANDARD) -> np.ndarray:
0317:     """Convert columns back to dilated image."""
0318:     N, C, H, W = output_size
0319:     kH, kW = kernel_size
0320:     
0321:     # Calculate output dimensions based on mode
0322:     if mode == ConvMode.TRANSPOSED:
0323:         H_out = H * stride[0]
0324:         W_out = W * stride[1]
0325:     else:
0326:         H_out = ((H - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0327:         W_out = ((W - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0328:     
0329:     output = np.zeros(output_size)
0330:     weights = np.zeros(output_size)  # For averaging overlapping values
0331:     
0332:     if mode == ConvMode.TRANSPOSED:
0333:         for h in range(H):
0334:             for w in range(W):
0335:                 for c in range(C):
0336:                     for kh in range(kH):
0337:                         for kw in range(kW):
0338:                             h_out = h * stride[0] + kh
0339:                             w_out = w * stride[1] + kw
0340:                             
0341:                             if h_out < H_out and w_out < W_out:
0342:                                 col_idx = c * kH * kW + kh * kW + kw
0343:                                 row_idx = h * W + w
0344:                                 
0345:                                 for n in range(N):
0346:                                     output[n, c, h_out, w_out] += cols[col_idx, n * H * W + row_idx]
0347:                                     weights[n, c, h_out, w_out] += 1
0348:     else:
0349:         for h_out in range(H_out):
0350:             for w_out in range(W_out):
0351:                 for c in range(C):
0352:                     for i in range(kH):
0353:                         for j in range(kW):
0354:                             h_in = h_out * stride[0] + i * dilation[0]
0355:                             w_in = w_out * stride[1] + j * dilation[1]
0356:                             
0357:                             if 0 <= h_in < H and 0 <= w_in < W:
0358:                                 col_idx = c * kH * kW + i * kW + j
0359:                                 row_idx = h_out * W_out + w_out
0360:                                 
0361:                                 for n in range(N):
0362:                                     output[n, c, h_in, w_in] += cols[col_idx, n * H_out * W_out + row_idx]
0363:                                     weights[n, c, h_in, w_in] += 1
0364:     
0365:     # Average overlapping values
0366:     np.divide(output, weights, out=output, where=weights != 0)
0367:     return output
0368: 
0369: def _compute_conv_output_shape(input_size: int, kernel_size: int, stride: int,
0370:                              padding: int, dilation: int) -> int:
0371:     """Computes output dimension for a single axis."""
0372:     numerator = input_size + 2 * padding - dilation * (kernel_size - 1) - 1
0373:     return numerator // stride + 1
0374: 
0375: def _compute_conv_grad_input_padding(grad_output_size: int, input_size: int,
0376:                                    kernel_size: int, stride: int, padding: int,
0377:                                    dilation: int) -> Tuple[int, int]:
0378:     """Computes padding needed for gradient computation."""
0379:     grad_input_padding = kernel_size - 1 - padding
0380:     return grad_input_padding
0381: 
0382: def _compute_output_padding(input_size: int, output_size: int, kernel_size: int,
0383:                           stride: int, padding: int, dilation: int) -> int:
0384:     """Computes additional padding needed for transposed convolution."""
0385:     expected_output = (input_size - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + 1
0386:     return output_size - expected_output
0387: 
0388: def _unfold(input_tensor: np.ndarray,
0389:            kernel_size: Tuple[int, ...],
0390:            dilation: Tuple[int, ...],
0391:            padding: Tuple[int, ...],
0392:            stride: Tuple[int, ...]) -> np.ndarray:
0393:     """Extracts sliding local blocks from input tensor."""
0394:     N, C, H, W = input_tensor.shape
0395:     kH, kW = kernel_size
0396:     
0397:     # Apply padding if needed
0398:     if padding[0] > 0 or padding[1] > 0:
0399:         input_tensor = np.pad(input_tensor,
0400:                           ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])),
0401:                           mode='constant')
0402:     
0403:     # Calculate output dimensions
0404:     H_out = ((H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0405:     W_out = ((W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0406:     
0407:     # Initialize output array with correct shape
0408:     output = np.zeros((C * kH * kW, N * H_out * W_out))
0409:     
0410:     # Extract patches
0411:     for h in range(H_out):
0412:         for w in range(W_out):
0413:             for i in range(kH):
0414:                 for j in range(kW):
0415:                     h_start = h * stride[0] + i * dilation[0]
0416:                     w_start = w * stride[1] + j * dilation[1]
0417:                     
0418:                     # Extract patch for all channels and batches
0419:                     patch = input_tensor[:, :, h_start:h_start+1, w_start:w_start+1]
0420:                     
0421:                     # Place in output array
0422:                     row_idx = (i * kW + j) * C + np.arange(C)
0423:                     col_idx = h * W_out + w + np.arange(N) * H_out * W_out
0424:                     output[row_idx[:, None], col_idx] = patch.reshape(N, C).T
0425:     
0426:     return output
0427: 
0428: def _fold(input: np.ndarray,
0429:          output_size: Tuple[int, ...],
0430:          kernel_size: Tuple[int, ...],
0431:          dilation: Tuple[int, ...],
0432:          padding: Tuple[int, ...],
0433:          stride: Tuple[int, ...]) -> np.ndarray:
0434:     """Combines an array of sliding local blocks into a large tensor."""
0435:     H, W = output_size
0436:     kH, kW = kernel_size
0437:     C = input.shape[0] // (kH * kW)
0438:     N = input.shape[1] // ((H + 2 * padding[0] - kH + 1) * (W + 2 * padding[1] - kW + 1))
0439:     
0440:     # Initialize output tensor
0441:     output = np.zeros((N, C, H + 2 * padding[0], W + 2 * padding[1]))
0442:     divisor = np.zeros_like(output)  # For averaging overlapping values
0443:     
0444:     # Calculate output dimensions
0445:     H_out = ((H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0]) + 1
0446:     W_out = ((W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1]) + 1
0447:     
0448:     # Fold patches back
0449:     for h in range(H_out):
0450:         for w in range(W_out):
0451:             for i in range(kH):
0452:                 for j in range(kW):
0453:                     h_start = h * stride[0] + i * dilation[0]
0454:                     w_start = w * stride[1] + j * dilation[1]
0455:                     
0456:                     row_idx = (i * kW + j) * C + np.arange(C)
0457:                     col_idx = h * W_out + w + np.arange(N) * H_out * W_out
0458:                     
0459:                     patch = input[row_idx[:, None], col_idx].T.reshape(N, C, 1, 1)
0460:                     output[:, :, h_start:h_start+1, w_start:w_start+1] += patch
0461:                     divisor[:, :, h_start:h_start+1, w_start:w_start+1] += 1
0462:     
0463:     # Average overlapping values
0464:     output = np.divide(output, divisor, where=divisor != 0)
0465:     
0466:     # Remove padding if necessary
0467:     if padding[0] > 0 or padding[1] > 0:
0468:         output = output[:, :, padding[0]:-padding[0] if padding[0] > 0 else None,
0469:                        padding[1]:-padding[1] if padding[1] > 0 else None]
0470:     
0471:     return output
0472: 
0473: def _dilate(input: np.ndarray, dilation: Tuple[int, ...]) -> np.ndarray:
0474:     """
0475:     Dilates the input tensor by inserting zeros between elements.
0476:     
0477:     Args:
0478:         input: Input tensor
0479:         dilation: Dilation factors for each dimension
0480:         
0481:     Returns:
0482:         Dilated tensor
0483:     """
0484:     if all(d == 1 for d in dilation):
0485:         return input
0486: 
0487:     N, C, H, W = input.shape
0488:     dH, dW = dilation
0489:     
0490:     H_dilated = H + (H - 1) * (dH - 1)
0491:     W_dilated = W + (W - 1) * (dW - 1)
0492:     
0493:     output = np.zeros((N, C, H_dilated, W_dilated))
0494:     output[:, :, ::dH, ::dW] = input
0495:     
0496:     return output
0497: 
0498: def _bilinear_interpolate(input: np.ndarray,
0499:                        points: np.ndarray,
0500:                        align_corners: bool = True) -> np.ndarray:
0501:     """
0502:     Performs bilinear interpolation on the input tensor at specified points.
0503:     
0504:     Args:
0505:         input: Input tensor (N, C, H, W)
0506:         points: Points to sample (N, P, 2) in normalized coordinates [-1, 1]
0507:         align_corners: Whether to align corners in interpolation
0508:         
0509:     Returns:
0510:         Interpolated values (N, C, P)
0511:     """ 
0512:     N, C, H, W = input.shape
0513:     _, P, _ = points.shape
0514: 
0515:     # Convert normalized coordinates to pixel coordinates
0516:     if align_corners:
0517:         x = (points[..., 0] + 1) * (W - 1) / 2
0518:         y = (points[..., 1] + 1) * (H - 1) / 2
0519:     else:
0520:         x = ((points[..., 0] + 1) * W - 1) / 2
0521:         y = ((points[..., 1] + 1) * H - 1) / 2
0522: 
0523:     # Get corner indices
0524:     x0 = np.floor(x).astype(np.int32)
0525:     x1 = x0 + 1
0526:     y0 = np.floor(y).astype(np.int32)
0527:     y1 = y0 + 1
0528: 
0529:     # Clip to image boundaries
0530:     x0 = np.clip(x0, 0, W - 1)
0531:     x1 = np.clip(x1, 0, W - 1)
0532:     y0 = np.clip(y0, 0, H - 1)
0533:     y1 = np.clip(y1, 0, H - 1)
0534: 
0535:     # Calculate interpolation weights
0536:     wa = (x1 - x) * (y1 - y)
0537:     wb = (x1 - x) * (y - y0)
0538:     wc = (x - x0) * (y1 - y)
0539:     wd = (x - x0) * (y - y0)
0540: 
0541:     # Gather corner values
0542:     Ia = np.zeros((N, C, P))
0543:     Ib = np.zeros((N, C, P))
0544:     Ic = np.zeros((N, C, P))
0545:     Id = np.zeros((N, C, P))
0546: 
0547:     for n in range(N):
0548:         for p in range(P):
0549:             Ia[n, :, p] = input[n, :, y0[n, p], x0[n, p]]
0550:             Ib[n, :, p] = input[n, :, y1[n, p], x0[n, p]]
0551:             Ic[n, :, p] = input[n, :, y0[n, p], x1[n, p]]
0552:             Id[n, :, p] = input[n, :, y1[n, p], x1[n, p]]
0553: 
0554:     # Reshape weights for broadcasting
0555:     wa = wa.reshape(N, 1, P)
0556:     wb = wb.reshape(N, 1, P)
0557:     wc = wc.reshape(N, 1, P)
0558:     wd = wd.reshape(N, 1, P)
0559: 
0560:     # Interpolate
0561:     out = wa * Ia + wb * Ib + wc * Ic + wd * Id
0562:     return out
0563: 
0564: def _bilinear_interpolate_gradient(grad_output: np.ndarray,
0565:                                 points: np.ndarray,
0566:                                 input_size: Tuple[int, ...],
0567:                                 input_tensor: np.ndarray,
0568:                                 align_corners: bool = True) -> Tuple[np.ndarray, np.ndarray]:
0569:     """
0570:     Computes gradients for bilinear interpolation.
0571:     
0572:     Args:
0573:         grad_output: Gradient of loss with respect to interpolated values (can be any shape)
0574:         points: Points that were sampled (N, P, 2) in normalized coordinates [-1, 1]
0575:         input_size: Size of the input tensor (H, W)
0576:         input_tensor: The input tensor being sampled from (N, C, H, W)
0577:         align_corners: Whether corners were aligned in interpolation
0578:         
0579:     Returns:
0580:         Tuple of:
0581:             - grad_input: Gradient with respect to input tensor (N, C, H, W)
0582:             - grad_points: Gradient with respect to sampling points (N, P, 2)
0583:             
0584:     Raises:
0585:         ValueError: If input shapes are incompatible
0586:     """
0587:     # Ensure points is properly shaped first
0588:     if points.ndim == 2:
0589:         points = points.reshape(1, *points.shape)
0590:     
0591:     # Get number of points from points tensor
0592:     N, P, _ = points.shape
0593:     
0594:     # Ensure grad_output is properly shaped (N, C, P) to match points
0595:     if grad_output.ndim == 1:
0596:         grad_output = grad_output.reshape(1, 1, -1)
0597:     elif grad_output.ndim == 2:
0598:         grad_output = grad_output.reshape(1, -1, 1)
0599:         
0600:     # Broadcast grad_output to match number of points if necessary
0601:     if grad_output.shape[2] == 1:
0602:         grad_output = np.broadcast_to(grad_output, (grad_output.shape[0], grad_output.shape[1], P))
0603:     elif grad_output.shape[2] != P:
0604:         raise ValueError(f"Gradient shape {grad_output.shape} cannot be broadcast to number of points {P}")
0605:         
0606:     C = grad_output.shape[1]
0607:     H, W = input_size
0608: 
0609:     # Validate input_tensor shape
0610:     if input_tensor.shape[2:] != input_size:
0611:         raise ValueError(f"Input tensor spatial dimensions {input_tensor.shape[2:]} "
0612:                         f"don't match input_size {input_size}")
0613:     
0614:     # Convert normalized coordinates to pixel coordinates
0615:     if align_corners:
0616:         x = (points[..., 0] + 1) * (W - 1) / 2
0617:         y = (points[..., 1] + 1) * (H - 1) / 2
0618:     else:
0619:         x = ((points[..., 0] + 1) * W - 1) / 2
0620:         y = ((points[..., 1] + 1) * H - 1) / 2
0621:     
0622:     # Get corner indices
0623:     x0 = np.floor(x).astype(np.int32)
0624:     x1 = x0 + 1
0625:     y0 = np.floor(y).astype(np.int32)
0626:     y1 = y0 + 1
0627:     
0628:     # Clip to image boundaries
0629:     x0 = np.clip(x0, 0, W - 1)
0630:     x1 = np.clip(x1, 0, W - 1)
0631:     y0 = np.clip(y0, 0, H - 1)
0632:     y1 = np.clip(y1, 0, H - 1)
0633:     
0634:     # Compute weights for bilinear interpolation
0635:     wa = (x1 - x) * (y1 - y)
0636:     wb = (x1 - x) * (y - y0)
0637:     wc = (x - x0) * (y1 - y)
0638:     wd = (x - x0) * (y - y0)
0639:     
0640:     # Reshape weights for broadcasting with channel dimension
0641:     wa = wa[..., None]  # Shape: (N, P, 1)
0642:     wb = wb[..., None]
0643:     wc = wc[..., None]
0644:     wd = wd[..., None]
0645:     
0646:     # Initialize gradients
0647:     grad_input = np.zeros((N, C, H, W))
0648:     grad_points = np.zeros_like(points)  # Shape: (N, P, 2)
0649:     
0650:     # Compute gradients with respect to input
0651:     for n in range(N):
0652:         for c in range(C):
0653:             grad_chan = grad_output[n, c]  # Shape: (P,)
0654:             for p in range(P):
0655:                 grad = grad_chan[p]
0656:                 grad_input[n, c, y0[n, p], x0[n, p]] += grad * wa[n, p, 0]
0657:                 grad_input[n, c, y1[n, p], x0[n, p]] += grad * wb[n, p, 0]
0658:                 grad_input[n, c, y0[n, p], x1[n, p]] += grad * wc[n, p, 0]
0659:                 grad_input[n, c, y1[n, p], x1[n, p]] += grad * wd[n, p, 0]
0660:     
0661:     # Compute scaling factors for coordinate gradients
0662:     if align_corners:
0663:         dx = (W - 1) / 2
0664:         dy = (H - 1) / 2
0665:     else:
0666:         dx = W / 2
0667:         dy = H / 2
0668:     
0669:     # Compute gradients with respect to sampling points
0670:     for n in range(N):
0671:         for p in range(P):
0672:             grad = grad_output[n, :, p].sum()  # Sum over channels
0673:             
0674:             # Gradient with respect to x
0675:             gx = grad * (
0676:                 (y1[n, p] - y[n, p]) * (
0677:                     input_tensor[n, :, y0[n, p], x1[n, p]] - 
0678:                     input_tensor[n, :, y0[n, p], x0[n, p]]
0679:                 ).sum() +
0680:                 (y[n, p] - y0[n, p]) * (
0681:                     input_tensor[n, :, y1[n, p], x1[n, p]] - 
0682:                     input_tensor[n, :, y1[n, p], x0[n, p]]
0683:                 ).sum()
0684:             ) * dx
0685:             
0686:             # Gradient with respect to y
0687:             gy = grad * (
0688:                 (x1[n, p] - x[n, p]) * (
0689:                     input_tensor[n, :, y1[n, p], x0[n, p]] - 
0690:                     input_tensor[n, :, y0[n, p], x0[n, p]]
0691:                 ).sum() +
0692:                 (x[n, p] - x0[n, p]) * (
0693:                     input_tensor[n, :, y1[n, p], x1[n, p]] - 
0694:                     input_tensor[n, :, y0[n, p], x1[n, p]]
0695:                 ).sum()
0696:             ) * dy
0697:             
0698:             grad_points[n, p] = [gx, gy]
0699:     
0700:     return grad_input, grad_points
0701: 
0702: def _generate_grid(batch_size: int, height: int, width: int,
0703:                  align_corners: bool = True) -> np.ndarray:
0704:     """
0705:     Generates a coordinate grid for grid sampling.
0706:     
0707:     Args:
0708:         batch_size: Number of samples in batch
0709:         height: Height of the grid
0710:         width: Width of the grid
0711:         align_corners: Whether to align corners
0712:         
0713:     Returns:
0714:         Grid tensor of shape (N, H, W, 2) with normalized coordinates
0715:     """
0716:     if align_corners:
0717:         x = np.linspace(-1, 1, width)
0718:         y = np.linspace(-1, 1, height)
0719:     else:
0720:         x = np.linspace(-1 + (1/width), 1 - (1/width), width)
0721:         y = np.linspace(-1 + (1/height), 1 - (1/height), height)
0722: 
0723:     x_coords, y_coords = np.meshgrid(x, y)
0724:     grid = np.stack([x_coords, y_coords], axis=-1)
0725:     grid = np.tile(grid[None], (batch_size, 1, 1, 1))
0726:     
0727:     return grid
0728: 
0729: def _deform_grid(grid: np.ndarray, offset: np.ndarray) -> np.ndarray:
0730:     """
0731:     Deforms a regular grid using offset values.
0732:     
0733:     Args:
0734:         grid: Regular coordinate grid (N, H, W, 2)
0735:         offset: Offset values for deformation (N, 2, H, W)
0736:         
0737:     Returns:
0738:         Deformed grid (N, H, W, 2)
0739:     """
0740:     N, H, W, _ = grid.shape
0741:     
0742:     # Reshape offset to match grid shape
0743:     offset = offset.transpose(0, 2, 3, 1)
0744:     
0745:     # Add offset to grid
0746:     deformed_grid = grid + offset
0747:     
0748:     # Clamp values to [-1, 1] to ensure valid sampling
0749:     return np.clip(deformed_grid, -1, 1)
0750: 
0751: def _modulated_deform_grid(grid: np.ndarray, offset: np.ndarray, 
0752:                         mask: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
0753:     """
0754:     Deforms a regular grid using offset values and modulation mask.
0755:     Used in Deformable ConvNets v2.
0756:     
0757:     Args:
0758:         grid: Regular coordinate grid (N, H, W, 2)
0759:         offset: Offset values for deformation (N, 2, H, W)
0760:         mask: Modulation mask (N, 1, H, W)
0761:         
0762:     Returns:
0763:         Tuple of deformed grid and modulation mask
0764:     """
0765:     # Deform grid
0766:     deformed_grid = _deform_grid(grid, offset)
0767:     
0768:     # Reshape mask to match sampling points
0769:     mask = mask.transpose(0, 2, 3, 1)
0770:     
0771:     return deformed_grid, mask
0772: 
0773: def _compute_indices_weights(points: np.ndarray, size: Tuple[int, int]) -> Tuple[np.ndarray, ...]:
0774:     """
0775:     Computes indices and weights for bilinear interpolation.
0776:     
0777:     Args:
0778:         points: Sampling points (N, H, W, 2)
0779:         size: Size of the input feature map (H, W)
0780:         
0781:     Returns:
0782:         Tuple of indices and weights for bilinear interpolation
0783:     """
0784:     H, W = size
0785:     
0786:     # Convert points to pixel coordinates
0787:     x = (points[..., 0] + 1) * (W - 1) / 2
0788:     y = (points[..., 1] + 1) * (H - 1) / 2
0789:     
0790:     # Get corner indices
0791:     x0 = np.floor(x).astype(np.int32)
0792:     x1 = x0 + 1
0793:     y0 = np.floor(y).astype(np.int32)
0794:     y1 = y0 + 1
0795:     
0796:     # Clip to image boundaries
0797:     x0 = np.clip(x0, 0, W - 1)
0798:     x1 = np.clip(x1, 0, W - 1)
0799:     y0 = np.clip(y0, 0, H - 1)
0800:     y1 = np.clip(y1, 0, H - 1)
0801:     
0802:     # Compute weights
0803:     wa = (x1 - x) * (y1 - y)
0804:     wb = (x1 - x) * (y - y0)
0805:     wc = (x - x0) * (y1 - y)
0806:     wd = (x - x0) * (y - y0)
0807:     
0808:     return (x0, x1, y0, y1), (wa, wb, wc, wd)
0809: 
0810: def _apply_deform_conv(input: np.ndarray, weight: np.ndarray, offset: np.ndarray,
0811:                     stride: Tuple[int, int], padding: Tuple[int, int],
0812:                     dilation: Tuple[int, int], mask: Optional[np.ndarray] = None) -> np.ndarray:
0813:     """
0814:     Applies deformable convolution operation.
0815:     
0816:     Args:
0817:         input: Input feature map (N, C_in, H, W)
0818:         weight: Convolution weights (C_out, C_in, kH, kW)
0819:         offset: Sampling offsets (N, 2*kH*kW, H_out, W_out)
0820:         stride: Convolution stride
0821:         padding: Zero-padding size
0822:         dilation: Dilation rate
0823:         mask: Optional modulation mask for v2 (N, kH*kW, H_out, W_out)
0824:         
0825:     Returns:
0826:         Output feature map (N, C_out, H_out, W_out)
0827:     """
0828:     N, C_in, H, W = input.shape
0829:     C_out, _, kH, kW = weight.shape
0830:     H_out = (H + 2 * padding[0] - dilation[0] * (kH - 1) - 1) // stride[0] + 1
0831:     W_out = (W + 2 * padding[1] - dilation[1] * (kW - 1) - 1) // stride[1] + 1
0832:     
0833:     # Generate sampling grid
0834:     grid = _generate_grid(N, H_out, W_out)
0835:     
0836:     # Deform grid using offsets
0837:     if mask is not None:
0838:         deformed_grid, modulation = _modulated_deform_grid(grid, offset, mask)
0839:     else:
0840:         deformed_grid = _deform_grid(grid, offset)
0841:         modulation = None
0842:     
0843:     # Get sampling indices and weights
0844:     indices, weights = _compute_indices_weights(deformed_grid, (H, W))
0845:     x0, x1, y0, y1 = indices
0846:     wa, wb, wc, wd = weights
0847:     
0848:     # Initialize output
0849:     output = np.zeros((N, C_out, H_out, W_out))
0850:     
0851:     # Apply convolution with deformed sampling
0852:     for i in range(kH):
0853:         for j in range(kW):
0854:             # Get values from input feature map
0855:             values = (wa[..., None] * input[:, :, y0, x0] +
0856:                      wb[..., None] * input[:, :, y1, x0] +
0857:                      wc[..., None] * input[:, :, y0, x1] +
0858:                      wd[..., None] * input[:, :, y1, x1])
0859:                      
0860:             # Apply modulation if available
0861:             if modulation is not None:
0862:                 values = values * modulation[:, i*kW + j, ..., None]
0863:                 
0864:             # Accumulate weighted values
0865:             for cout in range(C_out):
0866:                 output[:, cout] += np.sum(values * weight[cout, :, i, j], axis=1)
0867:     
0868:     return output
0869:         
0870: class Conv2dFunction(Function):
0871:     @staticmethod
0872:     def forward(ctx: Context, x: Tensor, weight: Tensor, bias: Optional[Tensor] = None,
0873:             stride: Tuple[int, int] = (1, 1), padding: Tuple[int, int] = (0, 0),
0874:             dilation: Tuple[int, int] = (1, 1), groups: int = 1,
0875:             mode: str = ConvMode.STANDARD, offset: Optional[Tensor] = None,
0876:             mask: Optional[Tensor] = None, output_padding: Tuple[int, int] = (0, 0)) -> Tensor:
0877:         """Forward pass of flexible 2D convolution."""
0878:         # Validate parameters
0879:         _validate_conv_params(x.shape, weight.shape, stride, padding, dilation, groups,
0880:                             mode, offset, weight, mask)
0881:         
0882:         # Save tensors and info for backward pass
0883:         if mode == ConvMode.DEFORMABLE:
0884:             ctx.save_for_backward(x, weight, bias, offset)
0885:         else:
0886:             ctx.save_for_backward(x, weight, bias)
0887:             
0888:         ctx.save_arguments(stride=stride, padding=padding, dilation=dilation,
0889:                         groups=groups, mode=mode, sampling_locations=None,
0890:                         output_padding=output_padding)
0891: 
0892:         if mode == ConvMode.TRANSPOSED:
0893:             # Get shapes
0894:             batch_size, C_in, H_in, W_in = x.shape
0895:             C_out, C_in_per_group, kH, kW = weight.shape
0896:             C_in_per_group = C_in // groups
0897:             C_out_per_group = C_out // groups
0898: 
0899:             # Calculate output dimensions
0900:             H_out = (H_in - 1) * stride[0] - 2 * padding[0] + kH + output_padding[0]
0901:             W_out = (W_in - 1) * stride[1] - 2 * padding[1] + kW + output_padding[1]
0902: 
0903:             # Initialize output tensor
0904:             output = np.zeros((batch_size, C_out, H_out, W_out))
0905: 
0906:             # Process each group
0907:             for g in range(groups):
0908:                 # Get input and weight for current group
0909:                 x_g = x.data[:, g*C_in_per_group:(g+1)*C_in_per_group]
0910:                 w_g = weight.data[g*C_out_per_group:(g+1)*C_out_per_group]
0911:                 
0912:                 # Flip kernel for transposed convolution
0913:                 w_g = np.flip(np.flip(w_g, 2), 3).transpose(1, 0, 2, 3)
0914:                 
0915:                 # Create the output tensor for this group
0916:                 output_g = np.zeros((batch_size, C_out_per_group, H_out, W_out))
0917:                 
0918:                 # Process each input element
0919:                 for n in range(batch_size):
0920:                     for h in range(H_in):
0921:                         for w in range(W_in):
0922:                             # Get input value for all channels
0923:                             x_val = x_g[n, :, h, w]  # Shape: (C_in_per_group,)
0924:                             
0925:                             # Calculate output position
0926:                             h_start = h * stride[0] - padding[0]
0927:                             w_start = w * stride[1] - padding[1]
0928:                             
0929:                             # Apply kernel at this position
0930:                             for c_out in range(C_out_per_group):
0931:                                 for c_in in range(C_in_per_group):
0932:                                     for kh in range(kH):
0933:                                         for kw in range(kW):
0934:                                             h_out = h_start + kh
0935:                                             w_out = w_start + kw
0936:                                             
0937:                                             if (0 <= h_out < H_out and 0 <= w_out < W_out):
0938:                                                 output_g[n, c_out, h_out, w_out] += (
0939:                                                     x_val[c_in] * w_g[c_in, c_out, kh, kw]
0940:                                                 )
0941:                 
0942:                 # Add this group's output to the final output
0943:                 output[:, g*C_out_per_group:(g+1)*C_out_per_group] = output_g
0944: 
0945:             if bias is not None:
0946:                 output += bias.data.reshape(1, -1, 1, 1)
0947: 
0948:             return Tensor(output)
0949:         else:
0950:             # Rest of the code for standard/deformable convolution remains the same
0951:             x_padded = _pad_input(x.data, padding)
0952:             C_in_per_group = x.shape[1] // groups
0953:             C_out_per_group = weight.shape[0] // groups
0954: 
0955:             H_out = ((x.shape[2] + 2 * padding[0] - dilation[0] * (weight.shape[2] - 1) - 1)
0956:                     // stride[0] + 1)
0957:             W_out = ((x.shape[3] + 2 * padding[1] - dilation[1] * (weight.shape[3] - 1) - 1)
0958:                     // stride[1] + 1)
0959: 
0960:             output = np.zeros((x.shape[0], weight.shape[0], H_out, W_out))
0961: 
0962:             for g in range(groups):
0963:                 x_g = x_padded[:, g*C_in_per_group:(g+1)*C_in_per_group]
0964:                 w_g = weight.data[g*C_out_per_group:(g+1)*C_out_per_group]
0965: 
0966:                 x_cols = _im2col_dilated(x_g, weight.shape[2:], stride, dilation, padding,
0967:                         mode=mode, sampling_locations=None)
0968:                 w_reshaped = w_g.reshape(C_out_per_group, -1)
0969:                 out = w_reshaped @ x_cols
0970: 
0971:                 out = out.reshape(C_out_per_group, H_out * W_out, x.shape[0])
0972:                 out = out.transpose(2, 0, 1).reshape(x.shape[0], C_out_per_group, H_out, W_out)
0973:                 output[:, g*C_out_per_group:(g+1)*C_out_per_group] = out
0974: 
0975:             if bias is not None:
0976:                 output += bias.data.reshape(1, -1, 1, 1)
0977: 
0978:             return Tensor(output)
0979:     
0980:     @staticmethod
0981:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0982:         """Backward pass of 2D convolution."""
0983:         # Retrieve saved tensors and arguments
0984:         saved_tensors = ctx.saved_tensors
0985:         num_saved = len(saved_tensors)
0986:         
0987:         # Initialize variables
0988:         offset_tensor = None
0989:         mask = None
0990:         grad_offset = None
0991:         grad_mask = None
0992:         
0993:         # Get saved tensors based on mode
0994:         if num_saved == 4:  # Deformable case
0995:             x, weight, bias, offset_tensor = saved_tensors
0996:         else:  # Standard/transposed case
0997:             x, weight, bias = saved_tensors
0998:             
0999:         # Get saved arguments
1000:         stride = ctx.saved_arguments['stride']
1001:         padding = ctx.saved_arguments['padding']
1002:         dilation = ctx.saved_arguments['dilation']
1003:         groups = ctx.saved_arguments['groups']
1004:         mode = ctx.saved_arguments['mode']
1005:         sampling_locations = ctx.saved_arguments.get('sampling_locations', None)
1006:         
1007:         # Calculate dimensions
1008:         N, C_in, H, W = x.shape
1009:         C_out, _, kH, kW = weight.shape
1010:         C_in_per_group = C_in // groups
1011:         C_out_per_group = C_out // groups
1012:         H_out, W_out = _get_output_shape(x.shape, weight.shape[2:], stride, padding, dilation, mode)
1013:         
1014:         # Initialize gradients based on requires_grad flags
1015:         grad_x = None
1016:         grad_x_padded = None
1017:         grad_weight = None
1018:         grad_bias = None
1019:         
1020:         if x.requires_grad:
1021:             if mode in [ConvMode.STANDARD, ConvMode.DEFORMABLE]:
1022:                 x_padded = _pad_input(x.data, padding)
1023:                 grad_x_padded = np.zeros_like(x_padded)
1024:             else:  # Transposed
1025:                 grad_x = np.zeros_like(x.data)
1026:                 
1027:         if weight.requires_grad:
1028:             grad_weight = np.zeros_like(weight.data)
1029:             
1030:         if bias is not None and bias.requires_grad:
1031:             grad_bias = np.zeros_like(bias.data)
1032:             
1033:         if mode == ConvMode.DEFORMABLE:
1034:             if offset_tensor is not None and offset_tensor.requires_grad:
1035:                 grad_offset = np.zeros_like(offset_tensor.data)
1036:                 
1037:         # Compute gradients based on mode
1038:         if mode == ConvMode.STANDARD:
1039:             Conv2dFunction._backward_standard(
1040:                 grad_output, grad_x_padded, grad_weight, grad_bias,
1041:                 x_padded, weight, bias, stride, padding, dilation, groups,
1042:                 N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
1043:             )
1044:         elif mode == ConvMode.TRANSPOSED:
1045:             Conv2dFunction._backward_transposed(
1046:                 grad_output, grad_x, grad_weight, grad_bias,
1047:                 x, weight, bias, stride, padding, dilation, groups,
1048:                 N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
1049:             )
1050:         elif mode == ConvMode.DEFORMABLE:
1051:             Conv2dFunction._backward_deformable(
1052:                 grad_output, grad_x_padded, grad_weight, grad_bias, grad_offset, grad_mask,
1053:                 x_padded, weight, bias, offset_tensor, mask, stride, padding, dilation, groups,
1054:                 sampling_locations, N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
1055:             )
1056:             
1057:         # Assign gradients to grad_dict
1058:         if x.requires_grad:
1059:             if mode in [ConvMode.STANDARD, ConvMode.DEFORMABLE]:
1060:                 if padding[0] > 0 or padding[1] > 0:
1061:                     grad_x = grad_x_padded[:, :, 
1062:                                        padding[0]:grad_x_padded.shape[2]-padding[0],
1063:                                        padding[1]:grad_x_padded.shape[3]-padding[1]]
1064:                 else:
1065:                     grad_x = grad_x_padded
1066:             grad_dict[id(x)] = grad_x
1067:             
1068:         if weight.requires_grad:
1069:             grad_dict[id(weight)] = grad_weight
1070:             
1071:         if bias is not None and bias.requires_grad:
1072:             grad_dict[id(bias)] = grad_bias
1073:             
1074:         if offset_tensor is not None and offset_tensor.requires_grad:
1075:             grad_dict[id(offset_tensor)] = grad_offset
1076: 
1077:     @staticmethod
1078:     def _backward_standard(grad_output, grad_x_padded, grad_weight, grad_bias,
1079:                         x_padded, weight, bias, stride, padding, dilation, groups,
1080:                         N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out):
1081:         """Backward pass for standard convolution."""
1082:         for g in range(groups):
1083:             # Get weight and grad_output for current group
1084:             w_g = weight.data[g * C_out_per_group:(g + 1) * C_out_per_group]
1085:             grad_out_g = grad_output[:, g * C_out_per_group:(g + 1) * C_out_per_group]
1086:             
1087:             # Convert grad_output to columns
1088:             grad_out_col = grad_out_g.transpose(1, 0, 2, 3).reshape(C_out_per_group, -1)
1089:             
1090:             # Get input columns
1091:             x_cols = _im2col_dilated(
1092:                 x_padded[:, g * C_in_per_group:(g + 1) * C_in_per_group],
1093:                 (kH, kW), stride, dilation, padding, ConvMode.STANDARD
1094:             )
1095:             
1096:             # Compute weight gradients
1097:             grad_weight[g * C_out_per_group:(g + 1) * C_out_per_group] = \
1098:                 (grad_out_col @ x_cols.T).reshape(C_out_per_group, C_in_per_group, kH, kW)
1099:             
1100:             # Compute input gradients
1101:             w_reshaped = w_g.reshape(C_out_per_group, -1).T
1102:             grad_cols = w_reshaped @ grad_out_col
1103:             
1104:             # Reshape grad_cols to match the expected shape
1105:             grad_cols = grad_cols.reshape(-1, N * H_out * W_out)
1106:             
1107:             # Convert columns back to image format
1108:             grad_x_padded[:, g * C_in_per_group:(g + 1) * C_in_per_group] += \
1109:                 _col2im_dilated(grad_cols, x_padded.shape, (kH, kW), stride, dilation)
1110:         
1111:         # Compute bias gradients if needed
1112:         if bias is not None and grad_bias is not None:
1113:             grad_bias[:] = grad_output.sum(axis=(0, 2, 3))
1114: 
1115:     @staticmethod
1116:     def _backward_transposed(
1117:         grad_output_padded, grad_x, grad_weight, grad_bias,
1118:         x, weight, bias, stride, padding, dilation, groups,
1119:         N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out
1120:     ):
1121:         """
1122:         Backward pass for transposed convolution.
1123:         """
1124:         for g in range(groups):
1125:             # Slice weights and grad_output for the current group
1126:             w_g = weight.data[g * C_out_per_group:(g + 1) * C_out_per_group]
1127:             grad_out_g = grad_output_padded[:, g * C_out_per_group:(g + 1) * C_out_per_group]
1128: 
1129:             # Reshape weights for gradient computation
1130:             w_flipped = np.flip(np.flip(w_g, 2), 3).transpose(1, 0, 2, 3)  # Flip kernel
1131:             w_reshaped = w_flipped.reshape(-1, C_out_per_group)  # Shape: (C_in_per_group * kH * kW, C_out_per_group)
1132: 
1133:             # Compute gradient columns with swapped stride and dilation
1134:             grad_cols = _im2col_dilated(
1135:                 grad_out_g,
1136:                 weight.shape[2:],
1137:                 dilation,  # Use dilation as stride
1138:                 stride,    # Use stride as dilation
1139:                 ConvMode.STANDARD
1140:             )
1141: 
1142:             # Compute gradient for input
1143:             grad_x[:, g * C_in_per_group:(g + 1) * C_in_per_group] += \
1144:                 (w_reshaped @ grad_cols).reshape(N, C_in_per_group, *x.shape[2:])
1145: 
1146:             # Compute gradient for weights
1147:             x_original = x.data[:, g * C_in_per_group:(g + 1) * C_in_per_group]
1148:             x_cols = _im2col_dilated(
1149:                 x_original,
1150:                 weight.shape[2:], stride, dilation, ConvMode.STANDARD, sampling_locations=None
1151:             )
1152:             for n in range(N):
1153:                 grad_out_n = grad_out_g[n].reshape(C_out_per_group, -1)  # Shape: (C_out_per_group, H_out * W_out)
1154:                 grad_weight[g * C_out_per_group:(g + 1) * C_out_per_group] += \
1155:                     (grad_out_n @ x_cols[:, n * H_out * W_out:(n + 1) * H_out * W_out].T).reshape(
1156:                         C_out_per_group, C_in_per_group, kH, kW
1157:                     )
1158: 
1159:         # Compute gradient for bias
1160:         if bias is not None:
1161:             grad_bias += grad_output_padded.sum(axis=(0, 2, 3))
1162: 
1163:     @staticmethod
1164:     def _backward_deformable(grad_output, grad_x_padded, grad_weight, grad_bias, grad_offset, grad_mask,
1165:                         x_padded, weight, bias, offset_tensor, mask, stride, padding, dilation, groups,
1166:                         sampling_locations, N, C_in_per_group, C_out_per_group, kH, kW, H_out, W_out,
1167:                         align_corners=True):
1168:         """Backward pass for deformable convolution."""
1169:         
1170:         if sampling_locations is None and offset_tensor is not None:
1171:             # If sampling_locations weren't provided but we have offset tensor, compute them
1172:             sampling_locations = _get_deformable_offsets(
1173:                 offset_tensor.data,
1174:                 (kH, kW),
1175:                 x_padded.shape,
1176:                 dilation
1177:             )
1178:         
1179:         # Now proceed with backward pass using sampling_locations
1180:         for g in range(groups):
1181:             w_g = weight.data[g * C_out_per_group:(g + 1) * C_out_per_group]
1182:             grad_out_g = grad_output[:, g * C_out_per_group:(g + 1) * C_out_per_group]
1183: 
1184:             for n in range(N):
1185:                 for h in range(H_out):
1186:                     for w_ in range(W_out):
1187:                         i = h * W_out + w_
1188:                         grad_out_slice = grad_out_g[n, :, h, w_]
1189: 
1190:                         for c_out in range(C_out_per_group):
1191:                             grad = grad_out_slice[c_out]
1192: 
1193:                             for c_in in range(C_in_per_group):
1194:                                 for kh in range(kH):
1195:                                     for kw in range(kW):
1196:                                         k_idx = kh * kW + kw
1197:                                         
1198:                                         if sampling_locations is not None:
1199:                                             loc = sampling_locations[n, i, k_idx]
1200:                                             h_in = int(loc[0])
1201:                                             w_in = int(loc[1])
1202:                                             
1203:                                             if (0 <= h_in < x_padded.shape[2] and 
1204:                                                 0 <= w_in < x_padded.shape[3]):
1205:                                                 # Update gradients
1206:                                                 if grad_x_padded is not None:
1207:                                                     grad_x_padded[n, g*C_in_per_group + c_in, h_in, w_in] += (
1208:                                                         grad * w_g[c_out, c_in, kh, kw]
1209:                                                     )
1210:                                                 if grad_weight is not None:
1211:                                                     grad_weight[g*C_out_per_group + c_out, c_in, kh, kw] += (
1212:                                                         grad * x_padded[n, g*C_in_per_group + c_in, h_in, w_in]
1213:                                                     )
1214: 
1215:         # Compute gradients for bias if needed
1216:         if grad_bias is not None and bias is not None:
1217:             grad_bias[:] = grad_output.sum((0, 2, 3))

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\elementwise.py
// ----------------------------------------
0001: from typing import Dict
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class Log(Function):
0006:     """
0007:     Natural logarithm operation.
0008:     
0009:     Forward: f(x) = ln(x)
0010:     Backward: f'(x) = 1/x
0011:     """
0012:     @staticmethod
0013:     def forward(ctx, x):
0014:         if not isinstance(x, Tensor):
0015:             x = Tensor(x)
0016:             
0017:         # Check for negative values
0018:         if np.any(x.data <= 0):
0019:             raise ValueError("Log of negative numbers or zero is undefined")
0020:             
0021:         ctx.save_for_backward(x)
0022:         return Tensor(np.log(x.data))
0023:         
0024:     @staticmethod
0025:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0026:         x, = ctx.saved_tensors
0027:         if x.requires_grad:
0028:             # d/dx(log(x)) = 1/x
0029:             grad_dict[id(x)] = grad_output / x.data
0030: 
0031: class Exp(Function):
0032:     """
0033:     Exponential operation.
0034:     
0035:     Forward: f(x) = exp(x)
0036:     Backward: f'(x) = exp(x)
0037:     """
0038:     @staticmethod
0039:     def forward(ctx, x):
0040:         if not isinstance(x, Tensor):
0041:             x = Tensor(x)
0042:             
0043:         result = np.exp(x.data)
0044:         ctx.save_for_backward(x)  # Save x for backward pass
0045:         ctx.save_arguments(exp_x=result)  # Save exp(x) as argument
0046:         return Tensor(result)
0047:         
0048:     @staticmethod
0049:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0050:         x, = ctx.saved_tensors
0051:         exp_x = ctx.saved_arguments['exp_x']
0052:         
0053:         if x.requires_grad:
0054:             # d/dx(exp(x)) = exp(x)
0055:             grad_dict[id(x)] = grad_output * exp_x

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\loss.py
// ----------------------------------------
0001: from typing import Dict, Optional
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class MSELoss(Function):
0006:     """
0007:     Mean Squared Error Loss: L = 1/N * Σ(y - ŷ)²
0008:     
0009:     Args:
0010:         reduction (str): Specifies the reduction to apply to the output:
0011:             'mean' (default) | 'sum' | 'none'
0012:     """
0013:     
0014:     @staticmethod
0015:     def forward(ctx, predictions, targets, reduction='mean'):
0016:         if not isinstance(predictions, Tensor):
0017:             predictions = Tensor(predictions)
0018:         if not isinstance(targets, Tensor):
0019:             targets = Tensor(targets)
0020:             
0021:         if predictions.shape != targets.shape:
0022:             raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
0023:             
0024:         diff = predictions.data - targets.data
0025:         squared_diff = diff * diff
0026:         
0027:         if reduction == 'none':
0028:             result = squared_diff
0029:         elif reduction == 'sum':
0030:             result = np.sum(squared_diff)
0031:         elif reduction == 'mean':
0032:             result = np.mean(squared_diff)
0033:         else:
0034:             raise ValueError(f"Invalid reduction method: {reduction}")
0035:             
0036:         ctx.save_for_backward(predictions, targets)
0037:         ctx.save_arguments(reduction=reduction)
0038:         return Tensor(result)
0039:         
0040:     @staticmethod
0041:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0042:         predictions, targets = ctx.saved_tensors
0043:         reduction = ctx.saved_arguments['reduction']
0044:         
0045:         diff = predictions.data - targets.data
0046:         
0047:         if reduction == 'mean':
0048:             grad = grad_output * 2 * diff / np.prod(diff.shape)
0049:         elif reduction == 'sum':
0050:             grad = grad_output * 2 * diff
0051:         else:  # 'none'
0052:             grad = grad_output * 2 * diff
0053:             
0054:         if predictions.requires_grad:
0055:             grad_dict[id(predictions)] = grad
0056: 
0057: class CrossEntropyLoss(Function):
0058:     """
0059:     Cross Entropy Loss with built-in LogSoftmax: L = -Σ y_true * log(softmax(y_pred))
0060:     
0061:     Args:
0062:         reduction (str): Specifies the reduction to apply to the output:
0063:             'mean' (default) | 'sum' | 'none'
0064:     """
0065:     
0066:     @staticmethod
0067:     def _log_softmax(x):
0068:         # Compute log(softmax(x)) in a numerically stable way
0069:         max_x = np.max(x, axis=1, keepdims=True)
0070:         exp_x = np.exp(x - max_x)
0071:         sum_exp_x = np.sum(exp_x, axis=1, keepdims=True)
0072:         return (x - max_x) - np.log(sum_exp_x)
0073:         
0074:     @staticmethod
0075:     def forward(ctx, predictions, targets, reduction='mean'):
0076:         if not isinstance(predictions, Tensor):
0077:             predictions = Tensor(predictions)
0078:         if not isinstance(targets, Tensor):
0079:             targets = Tensor(targets)
0080:             
0081:         log_softmax = CrossEntropyLoss._log_softmax(predictions.data)
0082:         nll_loss = -np.sum(targets.data * log_softmax, axis=1)
0083:         
0084:         if reduction == 'none':
0085:             result = nll_loss
0086:         elif reduction == 'sum':
0087:             result = np.sum(nll_loss)
0088:         elif reduction == 'mean':
0089:             result = np.mean(nll_loss)
0090:         else:
0091:             raise ValueError(f"Invalid reduction method: {reduction}")
0092:             
0093:         ctx.save_for_backward(predictions, targets)
0094:         ctx.save_arguments(reduction=reduction, log_softmax=log_softmax)
0095:         return Tensor(result)
0096:         
0097:     @staticmethod
0098:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0099:         predictions, targets = ctx.saved_tensors
0100:         reduction = ctx.saved_arguments['reduction']
0101:         log_softmax = ctx.saved_arguments['log_softmax']
0102:         
0103:         grad_output = np.array(grad_output)
0104:         if reduction == 'mean':
0105:             grad_output = grad_output / len(targets.data)
0106:         
0107:         softmax = np.exp(log_softmax)
0108:         grad = grad_output.reshape(-1, 1) * (softmax - targets.data)
0109:             
0110:         if predictions.requires_grad:
0111:             grad_dict[id(predictions)] = grad
0112: 
0113: class BinaryCrossEntropyLoss(Function):
0114:     """
0115:     Binary Cross Entropy Loss: L = -Σ (y * log(p) + (1-y) * log(1-p))
0116:     
0117:     Args:
0118:         reduction (str): Specifies the reduction to apply to the output:
0119:             'mean' (default) | 'sum' | 'none'
0120:         eps (float): Small value for numerical stability
0121:     """
0122:     
0123:     @staticmethod
0124:     def forward(ctx, predictions, targets, reduction='mean', eps=1e-7):
0125:         if not isinstance(predictions, Tensor):
0126:             predictions = Tensor(predictions)
0127:         if not isinstance(targets, Tensor):
0128:             targets = Tensor(targets)
0129: 
0130:         # Check valid probability values
0131:         if np.any(predictions.data < 0) or np.any(predictions.data > 1):
0132:             raise ValueError("Predictions must be in range [0, 1]")
0133:             
0134:         # Clip predictions to prevent log(0)
0135:         predictions_clipped = np.clip(predictions.data, eps, 1 - eps)
0136:         
0137:         loss = -(targets.data * np.log(predictions_clipped) + 
0138:                 (1 - targets.data) * np.log(1 - predictions_clipped))
0139:                 
0140:         if reduction == 'none':
0141:             result = loss
0142:         elif reduction == 'sum':
0143:             result = float(np.sum(loss))  # Convert to scalar
0144:         elif reduction == 'mean':
0145:             result = float(np.mean(loss))  # Convert to scalar
0146:         else:
0147:             raise ValueError(f"Invalid reduction method: {reduction}")
0148:             
0149:         ctx.save_for_backward(predictions, targets)
0150:         ctx.save_arguments(reduction=reduction, eps=eps)
0151:         return Tensor(result)
0152:         
0153:     @staticmethod
0154:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0155:         predictions, targets = ctx.saved_tensors
0156:         reduction = ctx.saved_arguments['reduction']
0157:         eps = ctx.saved_arguments['eps']
0158:         
0159:         predictions_clipped = np.clip(predictions.data, eps, 1 - eps)
0160:         
0161:         grad = grad_output * (predictions_clipped - targets.data) / (
0162:             predictions_clipped * (1 - predictions_clipped))
0163:             
0164:         if reduction == 'mean':
0165:             grad = grad / np.prod(targets.shape)
0166:             
0167:         if predictions.requires_grad:
0168:             grad_dict[id(predictions)] = grad
0169: 
0170: class L1Loss(Function):
0171:     """
0172:     L1 Loss (Mean Absolute Error): L = |y - ŷ|
0173:     
0174:     Args:
0175:         reduction (str): Specifies the reduction to apply to the output:
0176:             'mean' (default) | 'sum' | 'none'
0177:     """
0178:     
0179:     @staticmethod
0180:     def forward(ctx, predictions, targets, reduction='mean'):
0181:         if not isinstance(predictions, Tensor):
0182:             predictions = Tensor(predictions)
0183:         if not isinstance(targets, Tensor):
0184:             targets = Tensor(targets)
0185:             
0186:         if predictions.shape != targets.shape:
0187:             raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
0188:             
0189:         diff = predictions.data - targets.data
0190:         abs_diff = np.abs(diff)
0191:         
0192:         if reduction == 'none':
0193:             result = abs_diff
0194:         elif reduction == 'sum':
0195:             result = np.sum(abs_diff)
0196:         elif reduction == 'mean':
0197:             result = np.mean(abs_diff)
0198:         else:
0199:             raise ValueError(f"Invalid reduction method: {reduction}")
0200:             
0201:         ctx.save_for_backward(predictions, targets)
0202:         ctx.save_arguments(reduction=reduction)
0203:         return Tensor(result)
0204:         
0205:     @staticmethod
0206:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0207:         predictions, targets = ctx.saved_tensors
0208:         reduction = ctx.saved_arguments['reduction']
0209:         
0210:         diff = predictions.data - targets.data
0211:         grad = np.sign(diff)
0212:         
0213:         if reduction == 'mean':
0214:             grad = grad * grad_output / np.prod(diff.shape)
0215:         else:  # 'sum' or 'none'
0216:             grad = grad * grad_output
0217:             
0218:         if predictions.requires_grad:
0219:             grad_dict[id(predictions)] = grad
0220: 
0221: class KLDivLoss(Function):
0222:     """
0223:     Kullback-Leibler Divergence Loss.
0224:     KL divergence measures the relative entropy between two probability distributions.
0225:     
0226:     Args:
0227:         reduction (str): Specifies the reduction to apply to the output:
0228:             'mean' (default) | 'sum' | 'none'
0229:         log_target (bool): If True, target is expected to be log-probabilities
0230:     """
0231:     
0232:     @staticmethod
0233:     def forward(ctx, predictions, targets, reduction='mean', log_target=False):
0234:         if not isinstance(predictions, Tensor):
0235:             predictions = Tensor(predictions)
0236:         if not isinstance(targets, Tensor):
0237:             targets = Tensor(targets)
0238:             
0239:         if not log_target:
0240:             targets_log = np.log(np.clip(targets.data, 1e-7, 1.0))
0241:         else:
0242:             targets_log = targets.data
0243:             
0244:         # KL divergence formula: KL(P||Q) = P * (log(P) - log(Q))
0245:         loss = np.exp(targets_log) * (targets_log - predictions.data)
0246:         loss = -loss  # Correct the sign to make it positive
0247:         
0248:         if reduction == 'none':
0249:             result = loss
0250:         elif reduction == 'sum':
0251:             result = np.sum(loss)
0252:         elif reduction == 'mean':
0253:             result = np.mean(loss)
0254:         else:
0255:             raise ValueError(f"Invalid reduction method: {reduction}")
0256:             
0257:         ctx.save_for_backward(predictions, targets)
0258:         ctx.save_arguments(reduction=reduction, log_target=log_target)
0259:         return Tensor(result)
0260:         
0261:     @staticmethod
0262:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0263:         predictions, targets = ctx.saved_tensors
0264:         reduction = ctx.saved_arguments['reduction']
0265:         log_target = ctx.saved_arguments['log_target']
0266:         
0267:         if not log_target:
0268:             targets_log = np.log(np.clip(targets.data, 1e-7, 1.0))
0269:         else:
0270:             targets_log = targets.data
0271:             
0272:         grad = -np.exp(targets_log) * grad_output
0273:         
0274:         if reduction == 'mean':
0275:             grad = grad / np.prod(predictions.shape)
0276:             
0277:         if predictions.requires_grad:
0278:             grad_dict[id(predictions)] = grad
0279: 
0280: class CosineSimilarityLoss(Function):
0281:     """
0282:     Cosine Similarity Loss.
0283:     Measures the cosine similarity between two vectors.
0284:     
0285:     Args:
0286:         dim (int): Dimension along which cosine similarity is computed
0287:         eps (float): Small value to avoid division by zero
0288:         reduction (str): Specifies the reduction to apply to the output
0289:     """
0290:     
0291:     @staticmethod
0292:     def forward(ctx, x1, x2, dim=1, eps=1e-8, reduction='mean'):
0293:         if not isinstance(x1, Tensor):
0294:             x1 = Tensor(x1)
0295:         if not isinstance(x2, Tensor):
0296:             x2 = Tensor(x2)
0297:             
0298:         # Compute norms
0299:         norm1 = np.sqrt(np.sum(x1.data * x1.data, axis=dim, keepdims=True))
0300:         norm2 = np.sqrt(np.sum(x2.data * x2.data, axis=dim, keepdims=True))
0301:         
0302:         # Normalize vectors
0303:         x1_normalized = x1.data / np.maximum(norm1, eps)
0304:         x2_normalized = x2.data / np.maximum(norm2, eps)
0305:         
0306:         # Compute cosine similarity
0307:         cos_sim = np.sum(x1_normalized * x2_normalized, axis=dim)
0308:         
0309:         # For orthogonal vectors, cos_sim = 0, we want loss = 1
0310:         # For identical vectors, cos_sim = 1, we want loss = 0
0311:         # Therefore, loss = 1 - cos_sim
0312:         if reduction == 'none':
0313:             result = 1 - cos_sim
0314:         elif reduction == 'sum':
0315:             result = float(np.sum(1 - cos_sim))
0316:         elif reduction == 'mean':
0317:             result = float(np.mean(1 - cos_sim))
0318:         else:
0319:             raise ValueError(f"Invalid reduction method: {reduction}")
0320:             
0321:         ctx.save_for_backward(x1, x2)
0322:         ctx.save_arguments(dim=dim, eps=eps, reduction=reduction,
0323:                          x1_normalized=x1_normalized,
0324:                          x2_normalized=x2_normalized)
0325:         return Tensor(result)
0326:         
0327:     @staticmethod
0328:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0329:         x1, x2 = ctx.saved_tensors
0330:         dim = ctx.saved_arguments['dim']
0331:         eps = ctx.saved_arguments['eps']
0332:         reduction = ctx.saved_arguments['reduction']
0333:         x1_normalized = ctx.saved_arguments['x1_normalized']
0334:         x2_normalized = ctx.saved_arguments['x2_normalized']
0335:         
0336:         if reduction == 'mean':
0337:             grad_output = grad_output / x1.shape[0]
0338:         
0339:         # Gradient with respect to x1
0340:         if x1.requires_grad:
0341:             grad_x1 = -grad_output[..., None] * x2_normalized
0342:             grad_dict[id(x1)] = grad_x1
0343:             
0344:         # Gradient with respect to x2
0345:         if x2.requires_grad:
0346:             grad_x2 = -grad_output[..., None] * x1_normalized
0347:             grad_dict[id(x2)] = grad_x2
0348: 
0349: class HingeLoss(Function):
0350:     """
0351:     Hinge Loss (max-margin loss).
0352:     Commonly used for SVM training.
0353:     L = max(0, margin - y * f(x))
0354:     
0355:     Args:
0356:         margin (float): Margin in the hinge loss
0357:         reduction (str): Specifies the reduction to apply to the output
0358:     """
0359:     
0360:     @staticmethod
0361:     def forward(ctx, predictions, targets, margin=1.0, reduction='mean'):
0362:         if not isinstance(predictions, Tensor):
0363:             predictions = Tensor(predictions)
0364:         if not isinstance(targets, Tensor):
0365:             targets = Tensor(targets)
0366:             
0367:         # Convert targets to ±1
0368:         signed_targets = 2.0 * targets.data - 1.0
0369:         
0370:         # Compute raw hinge loss
0371:         loss = np.maximum(0, margin - signed_targets * predictions.data)
0372:         
0373:         if reduction == 'none':
0374:             result = loss
0375:         elif reduction == 'sum':
0376:             result = np.sum(loss)
0377:         elif reduction == 'mean':
0378:             result = np.mean(loss)
0379:         else:
0380:             raise ValueError(f"Invalid reduction method: {reduction}")
0381:             
0382:         ctx.save_for_backward(predictions, targets)
0383:         ctx.save_arguments(margin=margin, reduction=reduction,
0384:                          signed_targets=signed_targets)
0385:         return Tensor(result)
0386:         
0387:     @staticmethod
0388:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0389:         predictions, targets = ctx.saved_tensors
0390:         margin = ctx.saved_arguments['margin']
0391:         reduction = ctx.saved_arguments['reduction']
0392:         signed_targets = ctx.saved_arguments['signed_targets']
0393:         
0394:         # Gradient is -y when margin - y*f(x) > 0, 0 otherwise
0395:         mask = (margin - signed_targets * predictions.data) > 0
0396:         grad = -signed_targets * mask * grad_output
0397:         
0398:         if reduction == 'mean':
0399:             grad = grad / np.prod(predictions.shape)
0400:             
0401:         if predictions.requires_grad:
0402:             grad_dict[id(predictions)] = grad
0403: 
0404: class FocalLoss(Function):
0405:     """
0406:     Focal Loss.
0407:     Addresses class imbalance by down-weighting easily classified examples.
0408:     FL(p) = -alpha * (1-p)^gamma * log(p)
0409:     
0410:     Args:
0411:         alpha (float): Weighting factor for rare classes
0412:         gamma (float): Focusing parameter
0413:         reduction (str): Specifies the reduction to apply to the output
0414:     """
0415:     
0416:     @staticmethod
0417:     def forward(ctx, predictions, targets, alpha=0.25, gamma=2.0, reduction='mean'):
0418:         if not isinstance(predictions, Tensor):
0419:             predictions = Tensor(predictions)
0420:         if not isinstance(targets, Tensor):
0421:             targets = Tensor(targets)
0422:             
0423:         # Clip predictions for numerical stability
0424:         eps = 1e-7
0425:         predictions_clipped = np.clip(predictions.data, eps, 1 - eps)
0426:         
0427:         # Compute pt (probability of target class)
0428:         pt = predictions_clipped * targets.data + (1 - predictions_clipped) * (1 - targets.data)
0429:         
0430:         # Compute focal weight
0431:         focal_weight = alpha * ((1 - pt) ** gamma)
0432:         
0433:         # Compute binary cross entropy
0434:         bce = -(targets.data * np.log(predictions_clipped) + 
0435:                 (1 - targets.data) * np.log(1 - predictions_clipped))
0436:         
0437:         # Apply focal weight
0438:         loss = focal_weight * bce
0439:         
0440:         if reduction == 'none':
0441:             result = loss
0442:         elif reduction == 'sum':
0443:             result = np.sum(loss)
0444:         elif reduction == 'mean':
0445:             result = np.mean(loss)
0446:         else:
0447:             raise ValueError(f"Invalid reduction method: {reduction}")
0448:             
0449:         ctx.save_for_backward(predictions, targets)
0450:         ctx.save_arguments(alpha=alpha, gamma=gamma, reduction=reduction,
0451:                          pt=pt, focal_weight=focal_weight)
0452:         return Tensor(result)
0453:         
0454:     @staticmethod
0455:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0456:         predictions, targets = ctx.saved_tensors
0457:         alpha = ctx.saved_arguments['alpha']
0458:         gamma = ctx.saved_arguments['gamma']
0459:         reduction = ctx.saved_arguments['reduction']
0460:         pt = ctx.saved_arguments['pt']
0461:         focal_weight = ctx.saved_arguments['focal_weight']
0462:         
0463:         # Compute gradient
0464:         grad = grad_output * focal_weight * (
0465:             gamma * pt * np.log(pt) + pt - targets.data
0466:         )
0467:         
0468:         if reduction == 'mean':
0469:             grad = grad / np.prod(predictions.shape)
0470:             
0471:         if predictions.requires_grad:
0472:             grad_dict[id(predictions)] = grad
0473: 
0474: class HuberLoss(Function):
0475:     """
0476:     Huber Loss: L = 0.5 * (y - ŷ)² if |y - ŷ| <= delta else delta * |y - ŷ| - 0.5 * delta²
0477:     
0478:     This loss combines the best properties of MSE and L1 loss.
0479:     For small errors it behaves like MSE, for large errors it behaves like L1.
0480:     
0481:     Args:
0482:         delta (float): Threshold where loss transitions from squared to linear
0483:         reduction (str): Specifies the reduction to apply to the output:
0484:             'mean' (default) | 'sum' | 'none'
0485:     """
0486:     
0487:     @staticmethod
0488:     def forward(ctx, predictions, targets, delta=1.0, reduction='mean'):
0489:         if not isinstance(predictions, Tensor):
0490:             predictions = Tensor(predictions)
0491:         if not isinstance(targets, Tensor):
0492:             targets = Tensor(targets)
0493:             
0494:         if predictions.shape != targets.shape:
0495:             raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
0496:             
0497:         diff = predictions.data - targets.data
0498:         abs_diff = np.abs(diff)
0499:         quadratic = np.minimum(abs_diff, delta)
0500:         linear = abs_diff - quadratic
0501:         loss = 0.5 * quadratic ** 2 + delta * linear
0502:         
0503:         if reduction == 'none':
0504:             result = loss
0505:         elif reduction == 'sum':
0506:             result = np.sum(loss)
0507:         elif reduction == 'mean':
0508:             result = np.mean(loss)
0509:         else:
0510:             raise ValueError(f"Invalid reduction method: {reduction}")
0511:             
0512:         ctx.save_for_backward(predictions, targets)
0513:         ctx.save_arguments(delta=delta, reduction=reduction)
0514:         return Tensor(result)
0515:         
0516:     @staticmethod
0517:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0518:         predictions, targets = ctx.saved_tensors
0519:         delta = ctx.saved_arguments['delta']
0520:         reduction = ctx.saved_arguments['reduction']
0521:         
0522:         diff = predictions.data - targets.data
0523:         abs_diff = np.abs(diff)
0524:         
0525:         # Gradient is diff/|diff| * min(|diff|, delta)
0526:         grad = np.sign(diff) * np.minimum(abs_diff, delta)
0527:         
0528:         if reduction == 'mean':
0529:             grad = grad * grad_output / np.prod(diff.shape)
0530:         else:  # 'sum' or 'none'
0531:             grad = grad * grad_output
0532:             
0533:         if predictions.requires_grad:
0534:             grad_dict[id(predictions)] = grad

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\matrix.py
// ----------------------------------------
0001: from typing import Dict, Optional, Union, Tuple
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class Transpose(Function):
0006:     @staticmethod
0007:     def forward(ctx, x, axes: Optional[Tuple[int, ...]] = None):
0008:         if not isinstance(x, Tensor):
0009:             x = Tensor(x)
0010:             
0011:         ctx.save_for_backward(x)
0012:         ctx.save_arguments(axes=axes)
0013:         
0014:         if axes is None:
0015:             return Tensor(np.transpose(x.data))
0016:         return Tensor(np.transpose(x.data, axes))
0017:         
0018:     @staticmethod
0019:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0020:         x, = ctx.saved_tensors
0021:         axes = ctx.saved_arguments['axes']
0022:         
0023:         if x.requires_grad:
0024:             if axes is None:
0025:                 # For standard transpose, just transpose the gradient
0026:                 grad_dict[id(x)] = np.transpose(grad_output)
0027:             else:
0028:                 # For specific axes, need to invert the permutation
0029:                 inverse_axes = np.argsort(axes)
0030:                 grad_dict[id(x)] = np.transpose(grad_output, inverse_axes)
0031: 
0032: class Compare(Function):
0033:     """Base class for comparison operations"""
0034:     @staticmethod
0035:     def _compare(op, x1, x2):
0036:         if not isinstance(x1, Tensor):
0037:             x1 = Tensor(x1)
0038:         if not isinstance(x2, Tensor):
0039:             x2 = Tensor(x2)
0040:             
0041:         return Tensor(op(x1.data, x2.data))
0042:         
0043:     @staticmethod
0044:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0045:         # Comparison operations have no gradient
0046:         pass
0047: 
0048: class Greater(Compare):
0049:     @staticmethod
0050:     def forward(ctx, x1, x2):
0051:         return Compare._compare(np.greater, x1, x2)
0052: 
0053: class GreaterEqual(Compare):
0054:     @staticmethod
0055:     def forward(ctx, x1, x2):
0056:         return Compare._compare(np.greater_equal, x1, x2)
0057: 
0058: class Less(Compare):
0059:     @staticmethod
0060:     def forward(ctx, x1, x2):
0061:         return Compare._compare(np.less, x1, x2)
0062: 
0063: class LessEqual(Compare):
0064:     @staticmethod
0065:     def forward(ctx, x1, x2):
0066:         return Compare._compare(np.less_equal, x1, x2)
0067: 
0068: class Equal(Compare):
0069:     @staticmethod
0070:     def forward(ctx, x1, x2):
0071:         return Compare._compare(np.equal, x1, x2)
0072: 
0073: class NotEqual(Compare):
0074:     @staticmethod
0075:     def forward(ctx, x1, x2):
0076:         return Compare._compare(np.not_equal, x1, x2)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\nn.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\power.py
// ----------------------------------------
0001: from typing import Dict
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class Power(Function):
0006:     @staticmethod
0007:     def forward(ctx, base, exponent):
0008:         if not isinstance(base, Tensor):
0009:             base = Tensor(base)
0010:         if not isinstance(exponent, (Tensor, int, float)):
0011:             raise TypeError("Exponent must be a Tensor, int, or float")
0012:             
0013:         # Convert Tensor exponent to scalar if possible
0014:         if isinstance(exponent, Tensor):
0015:             if exponent.data.size == 1:
0016:                 exponent = float(exponent.data)
0017:             else:
0018:                 raise ValueError("Only scalar exponents are supported")
0019:                 
0020:         ctx.save_for_backward(base)
0021:         ctx.save_arguments(exponent=exponent)
0022:         
0023:         return Tensor(np.power(base.data, exponent))
0024:         
0025:     @staticmethod
0026:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0027:         base, = ctx.saved_tensors
0028:         exponent = ctx.saved_arguments['exponent']
0029:         
0030:         if base.requires_grad:
0031:             # d/dx(x^n) = nx^(n-1)
0032:             grad = grad_output * exponent * np.power(base.data, exponent - 1)
0033:             grad_dict[id(base)] = grad
0034: 
0035: class Divide(Function):
0036:     @staticmethod
0037:     def forward(ctx, numerator, denominator):
0038:         if not isinstance(numerator, Tensor):
0039:             numerator = Tensor(numerator)
0040:         if not isinstance(denominator, Tensor):
0041:             denominator = Tensor(denominator)
0042:             
0043:         # Check for division by zero
0044:         if np.any(denominator.data == 0):
0045:             raise ValueError("Division by zero encountered")
0046:             
0047:         ctx.save_for_backward(numerator, denominator)
0048:         return Tensor(numerator.data / denominator.data)
0049:         
0050:     @staticmethod
0051:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0052:         numerator, denominator = ctx.saved_tensors
0053:         
0054:         if numerator.requires_grad:
0055:             # d/dx(x/y) = 1/y
0056:             grad_dict[id(numerator)] = grad_output / denominator.data
0057:             
0058:         if denominator.requires_grad:
0059:             # d/dy(x/y) = -x/y^2
0060:             grad_dict[id(denominator)] = -grad_output * numerator.data / (denominator.data ** 2)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\reduction.py
// ----------------------------------------
0001: from typing import Dict, Optional, Union, Tuple
0002: import numpy as np
0003: from ..core import Function, Tensor
0004: 
0005: class Sum(Function):
0006:     @staticmethod
0007:     def forward(ctx, x, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False):
0008:         if not isinstance(x, Tensor):
0009:             x = Tensor(x)
0010:             
0011:         ctx.save_for_backward(x)
0012:         ctx.save_arguments(axis=axis, keepdims=keepdims, input_shape=x.shape)
0013:         
0014:         return Tensor(np.sum(x.data, axis=axis, keepdims=keepdims))
0015:         
0016:     @staticmethod
0017:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0018:         x, = ctx.saved_tensors
0019:         axis = ctx.saved_arguments['axis']
0020:         keepdims = ctx.saved_arguments['keepdims']
0021:         input_shape = ctx.saved_arguments['input_shape']
0022:         
0023:         if x.requires_grad:
0024:             # If not keeping dims, need to reshape grad_output to match broadcast
0025:             if not keepdims and axis is not None:
0026:                 grad_output = np.expand_dims(grad_output, axis=axis)
0027:                 
0028:             # Broadcast gradient to match input shape
0029:             grad = np.broadcast_to(grad_output, input_shape)
0030:             grad_dict[id(x)] = grad
0031: 
0032: class Mean(Function):
0033:     @staticmethod
0034:     def forward(ctx, x, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False):
0035:         if not isinstance(x, Tensor):
0036:             x = Tensor(x)
0037:             
0038:         ctx.save_for_backward(x)
0039:         ctx.save_arguments(axis=axis, keepdims=keepdims, input_shape=x.shape)
0040:         
0041:         return Tensor(np.mean(x.data, axis=axis, keepdims=keepdims))
0042:         
0043:     @staticmethod
0044:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0045:         x, = ctx.saved_tensors
0046:         axis = ctx.saved_arguments['axis']
0047:         keepdims = ctx.saved_arguments['keepdims']
0048:         input_shape = ctx.saved_arguments['input_shape']
0049:         
0050:         if x.requires_grad:
0051:             # If not keeping dims, need to reshape grad_output to match broadcast
0052:             if not keepdims and axis is not None:
0053:                 grad_output = np.expand_dims(grad_output, axis=axis)
0054:                 
0055:             # Calculate number of elements we're taking mean over
0056:             if axis is None:
0057:                 n = np.prod(input_shape)
0058:             else:
0059:                 n = np.prod([input_shape[i] for i in (axis,) if i < len(input_shape)])
0060:                 
0061:             # Broadcast gradient to match input shape and divide by n
0062:             grad = np.broadcast_to(grad_output, input_shape) / n
0063:             grad_dict[id(x)] = grad
0064: 
0065: class Max(Function):
0066:     @staticmethod
0067:     def forward(ctx, x, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False):
0068:         if not isinstance(x, Tensor):
0069:             x = Tensor(x)
0070:             
0071:         result = np.amax(x.data, axis=axis, keepdims=True)
0072:         ctx.save_for_backward(x)
0073:         ctx.save_arguments(axis=axis, keepdims=keepdims, max_vals=result)
0074:         
0075:         if not keepdims:
0076:             result = np.squeeze(result, axis=axis)
0077:             
0078:         return Tensor(result)
0079:         
0080:     @staticmethod
0081:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0082:         x, = ctx.saved_tensors
0083:         axis = ctx.saved_arguments['axis']
0084:         keepdims = ctx.saved_arguments['keepdims']
0085:         max_vals = ctx.saved_arguments['max_vals']
0086:         
0087:         if x.requires_grad:
0088:             # If not keeping dims, need to reshape grad_output
0089:             if not keepdims and axis is not None:
0090:                 grad_output = np.expand_dims(grad_output, axis=axis)
0091:                 
0092:             # Create gradient mask (1 where x equals max, 0 elsewhere)
0093:             mask = (x.data == max_vals)
0094:             
0095:             # In case of multiple maxima, distribute gradient equally
0096:             mask = mask / np.sum(mask, axis=axis, keepdims=True)
0097:             
0098:             grad_dict[id(x)] = grad_output * mask

// File: C:\Users\aluja\Desktop\DLpy\DLpy\ops\reshape.py
// ----------------------------------------
0001: # DLpy/ops/reshape.py
0002: from ..core.function import Function
0003: from ..core.tensor import Tensor
0004: import numpy as np
0005: from typing import Dict
0006: 
0007: class Reshape(Function):
0008:     @staticmethod
0009:     def forward(ctx, tensor, shape):
0010:         # Save both the input tensor and the target shape
0011:         ctx.save_for_backward(tensor)
0012:         ctx.save_arguments(target_shape=shape)
0013:         # Create and return a new tensor with the reshaped data
0014:         return Tensor(tensor.data.reshape(shape))
0015:         
0016:     @staticmethod
0017:     def backward(ctx, grad_output: np.ndarray, grad_dict: Dict[int, np.ndarray]) -> None:
0018:         # Get the original tensor and reshape the gradient back to its shape
0019:         original_tensor, = ctx.saved_tensors
0020:         if original_tensor.requires_grad:
0021:             # Reshape gradient back to the original tensor's shape
0022:             grad_dict[id(original_tensor)] = grad_output.reshape(original_tensor.shape)

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\__init__.py
// ----------------------------------------
0001: """
0002: Optimization algorithms for DLpy.
0003: 
0004: This module implements various optimization algorithms used in deep learning.
0005: """
0006: 
0007: from .optimizer import Optimizer
0008: from .sgd import SGD
0009: from .adam import Adam
0010: from .rmsprop import RMSprop
0011: from .adagrad import AdaGrad
0012: from .adadelta import AdaDelta
0013: from .adamax import AdaMax
0014: 
0015: __all__ = [
0016:     'Optimizer',
0017:     'SGD',
0018:     'Adam',
0019:     'RMSprop',
0020:     'AdaGrad',
0021:     'AdaDelta',
0022:     'AdaMax'
0023: ]

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\adadelta.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Dict, Iterator, Optional
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class AdaDelta(Optimizer):
0007:     """
0008:     Implements AdaDelta algorithm.
0009:     
0010:     AdaDelta is a more robust extension of AdaGrad that adapts learning rates based on a 
0011:     moving window of gradient updates, instead of accumulating all past squared gradients.
0012:     The main advantage is that it doesn't need an initial learning rate.
0013:     
0014:     Args:
0015:         params: Iterable of parameters to optimize
0016:         rho (float): Coefficient for computing a running average of squared gradients (default: 0.9)
0017:         eps (float): Term added to denominator to improve numerical stability (default: 1e-6)
0018:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0019:     """
0020:     
0021:     def __init__(self, params, rho: float = 0.9, eps: float = 1e-6, weight_decay: float = 0):
0022:         if not 0.0 <= rho <= 1.0:
0023:             raise ValueError(f"Invalid rho value: {rho}")
0024:         if not 0.0 <= eps:
0025:             raise ValueError(f"Invalid epsilon value: {eps}")
0026:         if not 0.0 <= weight_decay:
0027:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0028:         
0029:         defaults = dict(rho=rho, eps=eps, weight_decay=weight_decay)
0030:         super().__init__(params, defaults)
0031: 
0032:         # Initialize state for each parameter
0033:         for group in self._params:
0034:             state = self.state[id(group)]
0035:             state['step'] = 0
0036:             state['square_avg'] = np.zeros_like(group.data, dtype=np.float64)  # E[g^2]
0037:             state['acc_delta'] = np.zeros_like(group.data, dtype=np.float64)   # E[Δx^2]
0038: 
0039:     def step(self) -> None:
0040:         """
0041:         Performs a single optimization step.
0042:         
0043:         For each parameter:
0044:         1. Compute running average of squared gradients
0045:         2. Compute parameter update using accumulated squared updates
0046:         3. Update running average of squared updates
0047:         4. Apply update to parameters
0048:         """
0049:         for p in self._params:
0050:             if p.grad is None:
0051:                 continue
0052:             
0053:             grad = p.grad
0054:             state = self.state[id(p)]
0055: 
0056:             # Apply weight decay if specified
0057:             if self.defaults['weight_decay'] != 0:
0058:                 grad = grad + self.defaults['weight_decay'] * p.data
0059: 
0060:             state['step'] += 1
0061: 
0062:             # Get parameters
0063:             rho = self.defaults['rho']
0064:             eps = self.defaults['eps']
0065: 
0066:             # Update running average of squared gradients
0067:             square_avg = state['square_avg']
0068:             acc_delta = state['acc_delta']
0069:             
0070:             # Update square_avg using numpy operations
0071:             square_avg = rho * square_avg + (1 - rho) * grad * grad
0072:             state['square_avg'] = square_avg
0073:             
0074:             # Compute update
0075:             std = np.sqrt(acc_delta + eps)
0076:             delta = np.sqrt(square_avg + eps)
0077:             update = grad * std / delta
0078: 
0079:             # Update running average of squared updates
0080:             acc_delta = rho * acc_delta + (1 - rho) * update * update
0081:             state['acc_delta'] = acc_delta
0082:             
0083:             # Apply update
0084:             p.data -= update
0085: 
0086:     def state_dict(self) -> Dict:
0087:         """Returns the state of the optimizer as a Dict."""
0088:         return {
0089:             'state': self.state,
0090:             'defaults': self.defaults
0091:         }
0092: 
0093:     def load_state_dict(self, state_dict: Dict) -> None:
0094:         """Loads the optimizer state."""
0095:         self.state = state_dict['state']
0096:         self.defaults = state_dict['defaults']

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\adagrad.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Dict, Iterator, Optional
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class AdaGrad(Optimizer):
0007:     """
0008:     Implements AdaGrad algorithm.
0009:     
0010:     AdaGrad is an optimizer with parameter-specific learning rates,
0011:     which are adapted based on historical gradient information.
0012:     
0013:     Args:
0014:         params: Iterable of parameters to optimize
0015:         lr (float): Learning rate (default: 1e-2)
0016:         lr_decay (float): Learning rate decay (default: 0)
0017:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0018:         eps (float): Term added to denominator to improve numerical stability (default: 1e-10)
0019:         initial_accumulator_value (float): Initial value for accumulator (default: 0)
0020:     """
0021:     
0022:     def __init__(self, params, lr: float = 1e-2, lr_decay: float = 0,
0023:                  weight_decay: float = 0, initial_accumulator_value: float = 0,
0024:                  eps: float = 1e-10):
0025:         if not 0.0 <= lr:
0026:             raise ValueError(f"Invalid learning rate: {lr}")
0027:         if not 0.0 <= lr_decay:
0028:             raise ValueError(f"Invalid lr_decay value: {lr_decay}")
0029:         if not 0.0 <= weight_decay:
0030:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0031:         if not 0.0 <= initial_accumulator_value:
0032:             raise ValueError(f"Invalid initial_accumulator_value value: {initial_accumulator_value}")
0033:         if not 0.0 <= eps:
0034:             raise ValueError(f"Invalid epsilon value: {eps}")
0035:             
0036:         defaults = dict(lr=lr, lr_decay=lr_decay, eps=eps, 
0037:                        weight_decay=weight_decay,
0038:                        initial_accumulator_value=initial_accumulator_value)
0039:         super().__init__(params, defaults)
0040: 
0041:         for group in self._params:
0042:             state = self.state[id(group)]
0043:             state['step'] = 0
0044:             state['sum'] = np.full_like(group.data, initial_accumulator_value, dtype=np.float64)
0045: 
0046:     def step(self) -> None:
0047:         """
0048:         Performs a single optimization step.
0049:         
0050:         For each parameter p, accumulates the square of the gradient and then
0051:         updates the parameter using the formula:
0052:         p = p - lr * g / (sqrt(accumulator) + eps)
0053:         where g is the gradient.
0054:         """
0055:         for p in self._params:
0056:             if p.grad is None:
0057:                 continue
0058:                 
0059:             grad = p.grad
0060:             state = self.state[id(p)]
0061: 
0062:             state['step'] += 1
0063: 
0064:             if self.defaults['weight_decay'] != 0:
0065:                 grad = grad + self.defaults['weight_decay'] * p.data
0066: 
0067:             # Update accumulator with squared gradient
0068:             state['sum'] += grad * grad
0069: 
0070:             # Compute the adaptive learning rate
0071:             std = np.sqrt(state['sum'])
0072:             
0073:             # Add epsilon for numerical stability before division
0074:             denom = std + self.defaults['eps']
0075: 
0076:             # Apply learning rate decay if specified
0077:             if self.defaults['lr_decay'] != 0:
0078:                 lr = self.defaults['lr'] / (1 + (state['step'] - 1) * self.defaults['lr_decay'])
0079:             else:
0080:                 lr = self.defaults['lr']
0081: 
0082:             # Update parameters
0083:             p.data -= lr * grad / denom
0084: 
0085:     def reset_state(self) -> None:
0086:         """
0087:         Resets the state of the optimizer.
0088:         
0089:         This can be useful when you want to restart training or when you want to 
0090:         reset the accumulated gradients without creating a new optimizer instance.
0091:         """
0092:         initial_accumulator_value = self.defaults['initial_accumulator_value']
0093:         
0094:         for group in self._params:
0095:             state = self.state[id(group)]
0096:             state['step'] = 0
0097:             state['sum'] = np.full_like(group.data, initial_accumulator_value, dtype=np.float64)
0098: 
0099:     def state_dict(self) -> Dict:
0100:         """
0101:         Returns the state of the optimizer as a Dict.
0102:         
0103:         The returned state dict contains two entries:
0104:             * state - a dict holding current optimization state. Its content
0105:                 differs between optimizer classes.
0106:             * param_groups - a dict containing all parameter groups
0107:         """
0108:         return {
0109:             'state': self.state,
0110:             'defaults': self.defaults
0111:         }
0112: 
0113:     def load_state_dict(self, state_dict: Dict) -> None:
0114:         """
0115:         Loads the optimizer state.
0116:         
0117:         Args:
0118:             state_dict (dict): Optimizer state. Should be an object returned
0119:                 from a call to state_dict().
0120:         """
0121:         self.state = state_dict['state']
0122:         self.defaults = state_dict['defaults']

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\adam.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Dict, Iterator, Optional
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class Adam(Optimizer):
0007:     """
0008:     Implements Adam algorithm.
0009:     
0010:     Args:
0011:         params: Iterable of parameters to optimize
0012:         lr (float): Learning rate (default: 0.001)
0013:         betas (tuple): Coefficients for computing running averages of gradient and its square
0014:             (default: (0.9, 0.999))
0015:         eps (float): Term added to denominator to improve numerical stability (default: 1e-8)
0016:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0017:         amsgrad (bool): Whether to use the AMSGrad variant (default: False)
0018:     """
0019:     
0020:     def __init__(self, params, lr: float = 0.001, betas: tuple = (0.9, 0.999),
0021:                  eps: float = 1e-8, weight_decay: float = 0, amsgrad: bool = False):
0022:         if not 0.0 <= lr:
0023:             raise ValueError(f"Invalid learning rate: {lr}")
0024:         if not 0.0 <= eps:
0025:             raise ValueError(f"Invalid epsilon value: {eps}")
0026:         if not 0.0 <= betas[0] < 1.0:
0027:             raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
0028:         if not 0.0 <= betas[1] < 1.0:
0029:             raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")
0030:         if not 0.0 <= weight_decay:
0031:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0032:             
0033:         defaults = dict(lr=lr, betas=betas, eps=eps,
0034:                        weight_decay=weight_decay, amsgrad=amsgrad)
0035:         super().__init__(params, defaults)
0036:         
0037:     def step(self) -> None:
0038:         """Performs a single optimization step."""
0039:         for p in self._params:
0040:             if p.grad is None:
0041:                 continue
0042:                 
0043:             grad = p.grad
0044:             
0045:             # Get optimizer state
0046:             state = self.state[id(p)]
0047:             
0048:             # State initialization
0049:             if len(state) == 0:
0050:                 state['step'] = 0
0051:                 # Exponential moving average of gradient values
0052:                 state['exp_avg'] = np.zeros_like(p.data)
0053:                 # Exponential moving average of squared gradient values
0054:                 state['exp_avg_sq'] = np.zeros_like(p.data)
0055:                 if self.defaults['amsgrad']:
0056:                     # Maintains max of all exp. moving avg. of sq. grad. values
0057:                     state['max_exp_avg_sq'] = np.zeros_like(p.data)
0058:                     
0059:             exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
0060:             if self.defaults['amsgrad']:
0061:                 max_exp_avg_sq = state['max_exp_avg_sq']
0062:             beta1, beta2 = self.defaults['betas']
0063:             
0064:             state['step'] += 1
0065:             bias_correction1 = 1 - beta1 ** state['step']
0066:             bias_correction2 = 1 - beta2 ** state['step']
0067:             
0068:             if self.defaults['weight_decay'] != 0:
0069:                 grad = grad + self.defaults['weight_decay'] * p.data
0070:                 
0071:             # Decay the first and second moment running average coefficient
0072:             exp_avg = beta1 * exp_avg + (1 - beta1) * grad
0073:             exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * grad * grad
0074:             
0075:             if self.defaults['amsgrad']:
0076:                 # Maintains the maximum of all 2nd moment running avg. till now
0077:                 max_exp_avg_sq = np.maximum(max_exp_avg_sq, exp_avg_sq)
0078:                 # Use the max. for normalizing running avg. of gradient
0079:                 denom = (np.sqrt(max_exp_avg_sq) / np.sqrt(bias_correction2)) + self.defaults['eps']
0080:             else:
0081:                 denom = (np.sqrt(exp_avg_sq) / np.sqrt(bias_correction2)) + self.defaults['eps']
0082:                 
0083:             step_size = self.defaults['lr'] / bias_correction1
0084:             
0085:             p.data -= step_size * exp_avg / denom
0086:             
0087:             # Save state
0088:             state['exp_avg'] = exp_avg
0089:             state['exp_avg_sq'] = exp_avg_sq
0090:             if self.defaults['amsgrad']:
0091:                 state['max_exp_avg_sq'] = max_exp_avg_sq

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\adamax.py
// ----------------------------------------
0001: import numpy as np
0002: from typing import Dict, Iterator, Optional
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class AdaMax(Optimizer):
0007:     """
0008:     Implements AdaMax algorithm, a variant of Adam based on the infinity norm.
0009:     
0010:     AdaMax is a variant of Adam that adopts the infinity norm in place of the L2 norm.
0011:     It tends to be more stable than Adam in some cases.
0012:     
0013:     Args:
0014:         params: Iterable of parameters to optimize
0015:         lr (float): Learning rate (default: 0.002)
0016:         betas (tuple): Coefficients for computing running averages (default: (0.9, 0.999))
0017:         eps (float): Term added to denominator to improve numerical stability (default: 1e-8)
0018:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0019:     """
0020:     
0021:     def __init__(self, params, lr: float = 0.002, betas: tuple = (0.9, 0.999), 
0022:                  eps: float = 1e-8, weight_decay: float = 0):
0023:         if not 0.0 <= lr:
0024:             raise ValueError(f"Invalid learning rate: {lr}")
0025:         if not 0.0 <= eps:
0026:             raise ValueError(f"Invalid epsilon value: {eps}")
0027:         if not 0.0 <= betas[0] < 1.0:
0028:             raise ValueError(f"Invalid beta1 parameter: {betas[0]}")
0029:         if not 0.0 <= betas[1] < 1.0:
0030:             raise ValueError(f"Invalid beta2 parameter: {betas[1]}")
0031:         if not 0.0 <= weight_decay:
0032:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0033:             
0034:         defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
0035:         super().__init__(params, defaults)
0036:         
0037:         # Initialize state for each parameter
0038:         for group in self._params:
0039:             state = self.state[id(group)]
0040:             state['step'] = 0
0041:             state['exp_avg'] = np.zeros_like(group.data, dtype=np.float64)  # m_t
0042:             state['exp_inf'] = np.zeros_like(group.data, dtype=np.float64)  # u_t
0043: 
0044:     def step(self) -> None:
0045:         """
0046:         Performs a single optimization step.
0047:         
0048:         For each parameter:
0049:         1. Update biased first moment estimate
0050:         2. Update the exponentially weighted infinity norm
0051:         3. Compute bias-corrected learning rate
0052:         4. Update parameters
0053:         """
0054:         for p in self._params:
0055:             if p.grad is None:
0056:                 continue
0057:                 
0058:             grad = p.grad
0059:             state = self.state[id(p)]
0060: 
0061:             # Apply weight decay if specified
0062:             if self.defaults['weight_decay'] != 0:
0063:                 grad = grad + self.defaults['weight_decay'] * p.data
0064: 
0065:             # Get parameters
0066:             beta1, beta2 = self.defaults['betas']
0067:             lr = self.defaults['lr']
0068:             eps = self.defaults['eps']
0069:             
0070:             state['step'] += 1
0071:             bias_correction = 1 - beta1 ** state['step']
0072: 
0073:             # Get momentum buffer
0074:             exp_avg = state['exp_avg']
0075:             exp_inf = state['exp_inf']
0076: 
0077:             # Update biased first moment estimate using numpy operations
0078:             exp_avg = beta1 * exp_avg + (1 - beta1) * grad
0079:             state['exp_avg'] = exp_avg
0080:             
0081:             # Update the exponentially weighted infinity norm
0082:             exp_inf = np.maximum(beta2 * exp_inf, np.abs(grad))
0083:             state['exp_inf'] = exp_inf
0084: 
0085:             # Compute bias-corrected learning rate
0086:             step_size = lr / bias_correction
0087: 
0088:             # Update parameters
0089:             p.data -= step_size * exp_avg / (exp_inf + eps)
0090: 
0091:     def state_dict(self) -> Dict:
0092:         """Returns the state of the optimizer as a Dict."""
0093:         return {
0094:             'state': self.state,
0095:             'defaults': self.defaults
0096:         }
0097: 
0098:     def load_state_dict(self, state_dict: Dict) -> None:
0099:         """Loads the optimizer state."""
0100:         self.state = state_dict['state']
0101:         self.defaults = state_dict['defaults']

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\optimizer.py
// ----------------------------------------
0001: from typing import Dict, Iterator, Optional
0002: from ..core import Tensor
0003: 
0004: class Optimizer:
0005:     """
0006:     Base class for all optimizers.
0007:     
0008:     Args:
0009:         params: An iterable of parameters to optimize or a dict of parameter groups
0010:         defaults: Dictionary of default hyperparameter values
0011:     """
0012:     
0013:     def __init__(self, params, defaults: Dict):
0014:         self.defaults = defaults
0015:         self._params = list(params)  # Convert iterator to list
0016:         self.state: Dict = {}  # State dict for optimizer states
0017:         
0018:         # Initialize state for each parameter
0019:         for p in self._params:
0020:             self.state[id(p)] = {}
0021:             
0022:     def zero_grad(self) -> None:
0023:         """Clears the gradients of all optimized parameters."""
0024:         for p in self._params:
0025:             if p.grad is not None:
0026:                 p.grad.fill(0)
0027:                 
0028:     def step(self) -> None:
0029:         """Performs a single optimization step.
0030:         
0031:         This method should be overridden by all optimizers.
0032:         """
0033:         raise NotImplementedError
0034:         
0035:     def add_param_group(self, param_group: Dict) -> None:
0036:         """Add a param group to the optimizer's param groups.
0037:         
0038:         Args:
0039:             param_group (dict): Specifies parameters and parameter-specific options
0040:         """
0041:         params = param_group['params']
0042:         if isinstance(params, Tensor):
0043:             param_group['params'] = [params]
0044:         elif isinstance(params, set):
0045:             param_group['params'] = list(params)
0046:             
0047:         for param in param_group['params']:
0048:             if id(param) not in self.state:
0049:                 self.state[id(param)] = {}
0050:             self._params.append(param)
0051:             
0052:     def load_state_dict(self, state_dict: Dict) -> None:
0053:         """Loads the optimizer state.
0054:         
0055:         Args:
0056:             state_dict (dict): Optimizer state dict
0057:         """
0058:         self.state = state_dict['state']
0059:         
0060:     def state_dict(self) -> Dict:
0061:         """Returns the state of the optimizer as a dict.
0062:         
0063:         Returns:
0064:             dict: The state of the optimizer
0065:         """
0066:         return {
0067:             'state': self.state,
0068:         }

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\rmsprop.py
// ----------------------------------------
0001: import numpy as np 
0002: from typing import Dict, Iterator, Optional
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class RMSprop(Optimizer):
0007:     """
0008:     Implements RMSprop algorithm.
0009:     
0010:     Args:
0011:         params: Iterable of parameters to optimize
0012:         lr (float): Learning rate (default: 0.01)
0013:         alpha (float): Smoothing constant (default: 0.99)
0014:         eps (float): Term added to denominator to improve numerical stability (default: 1e-8)
0015:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0016:         momentum (float): Momentum factor (default: 0)
0017:         centered (bool): If True, compute centered RMSprop, gradients normalized by their variance
0018:     """
0019:     
0020:     def __init__(self, params, lr: float = 0.01, alpha: float = 0.99,
0021:                  eps: float = 1e-8, weight_decay: float = 0,
0022:                  momentum: float = 0, centered: bool = False):
0023:         if not 0.0 <= lr:
0024:             raise ValueError(f"Invalid learning rate: {lr}")
0025:         if not 0.0 <= eps:
0026:             raise ValueError(f"Invalid epsilon value: {eps}")
0027:         if not 0.0 <= momentum:
0028:             raise ValueError(f"Invalid momentum value: {momentum}")
0029:         if not 0.0 <= alpha:
0030:             raise ValueError(f"Invalid alpha value: {alpha}")
0031:         if not 0.0 <= weight_decay:
0032:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0033:             
0034:         defaults = dict(lr=lr, alpha=alpha, eps=eps, weight_decay=weight_decay,
0035:                        momentum=momentum, centered=centered)
0036:         super().__init__(params, defaults)
0037:         
0038:     def step(self) -> None:
0039:         """Performs a single optimization step."""
0040:         for p in self._params:
0041:             if p.grad is None:
0042:                 continue
0043:                 
0044:             grad = p.grad
0045:             state = self.state[id(p)]
0046:             
0047:             # State initialization
0048:             if len(state) == 0:
0049:                 state['step'] = 0
0050:                 state['square_avg'] = np.zeros_like(p.data)
0051:                 if self.defaults['momentum'] > 0:
0052:                     state['momentum_buffer'] = np.zeros_like(p.data)
0053:                 if self.defaults['centered']:
0054:                     state['grad_avg'] = np.zeros_like(p.data)
0055:                     
0056:             square_avg = state['square_avg']
0057:             alpha = self.defaults['alpha']
0058:             
0059:             state['step'] += 1
0060:             
0061:             if self.defaults['weight_decay'] != 0:
0062:                 grad = grad + self.defaults['weight_decay'] * p.data
0063:                 
0064:             # Update squared average
0065:             square_avg = alpha * square_avg + (1 - alpha) * grad * grad
0066:             
0067:             if self.defaults['centered']:
0068:                 grad_avg = state['grad_avg']
0069:                 grad_avg = alpha * grad_avg + (1 - alpha) * grad
0070:                 avg = square_avg - grad_avg * grad_avg
0071:                 state['grad_avg'] = grad_avg
0072:             else:
0073:                 avg = square_avg
0074:                 
0075:             # Apply momentum if enabled
0076:             if self.defaults['momentum'] > 0:
0077:                 buf = state.get('momentum_buffer', np.zeros_like(grad))
0078:                 buf = self.defaults['momentum'] * buf + grad / (np.sqrt(avg) + self.defaults['eps'])
0079:                 state['momentum_buffer'] = buf
0080:                 p.data -= self.defaults['lr'] * buf
0081:             else:
0082:                 p.data -= self.defaults['lr'] * grad / (np.sqrt(avg) + self.defaults['eps'])
0083:                 
0084:             # Save state
0085:             state['square_avg'] = square_avg

// File: C:\Users\aluja\Desktop\DLpy\DLpy\optim\sgd.py
// ----------------------------------------
0001: from typing import Dict, Iterator, Optional
0002: import numpy as np
0003: from .optimizer import Optimizer
0004: from ..core import Tensor
0005: 
0006: class SGD(Optimizer):
0007:     """
0008:     Implements stochastic gradient descent with momentum.
0009:     
0010:     Args:
0011:         params: Iterable of parameters to optimize
0012:         lr (float): Learning rate (default: 0.1)
0013:         momentum (float): Momentum factor (default: 0)
0014:         weight_decay (float): Weight decay (L2 penalty) (default: 0)
0015:         dampening (float): Dampening for momentum (default: 0)
0016:         nesterov (bool): Enables Nesterov momentum (default: False)
0017:     """
0018:     
0019:     def __init__(self, params, lr: float = 0.1, momentum: float = 0.0,
0020:                  weight_decay: float = 0.0, dampening: float = 0.0,
0021:                  nesterov: bool = False):
0022:         if lr < 0.0:
0023:             raise ValueError(f"Invalid learning rate: {lr}")
0024:         if momentum < 0.0:
0025:             raise ValueError(f"Invalid momentum value: {momentum}")
0026:         if weight_decay < 0.0:
0027:             raise ValueError(f"Invalid weight_decay value: {weight_decay}")
0028:             
0029:         defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay,
0030:                        dampening=dampening, nesterov=nesterov)
0031:         super().__init__(params, defaults)
0032:         
0033:     def step(self) -> None:
0034:         """Performs a single optimization step."""
0035:         
0036:         for p in self._params:
0037:             if p.grad is None:
0038:                 continue
0039:                 
0040:             grad = p.grad
0041:             
0042:             # Apply weight decay
0043:             if self.defaults['weight_decay'] != 0:
0044:                 grad = grad + self.defaults['weight_decay'] * p.data
0045:                 
0046:             # Get or initialize momentum buffer
0047:             if 'momentum_buffer' not in self.state[id(p)]:
0048:                 buf = self.state[id(p)]['momentum_buffer'] = np.zeros_like(p.data)
0049:             else:
0050:                 buf = self.state[id(p)]['momentum_buffer']
0051:                 
0052:             # Update momentum buffer
0053:             if self.defaults['momentum'] != 0:
0054:                 buf *= self.defaults['momentum']
0055:                 if self.defaults['dampening'] != 0:
0056:                     grad *= 1 - self.defaults['dampening']
0057:                 buf += grad
0058:             else:
0059:                 buf = grad
0060:                 
0061:             # Nesterov momentum
0062:             if self.defaults['nesterov']:
0063:                 grad += self.defaults['momentum'] * buf
0064:             else:
0065:                 grad = buf
0066:                 
0067:             # Update parameters
0068:             p.data -= self.defaults['lr'] * grad
0069:             
0070:             # Store updated momentum buffer
0071:             self.state[id(p)]['momentum_buffer'] = buf

// File: C:\Users\aluja\Desktop\DLpy\examples\basic_autograd.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\examples\neural_network.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\setup.py
// ----------------------------------------
0001: from setuptools import setup, find_packages
0002: 
0003: setup(
0004:     name="DLpy",  # Changed from DLpy to DLpy
0005:     version="0.1.0",
0006:     packages=find_packages(include=["DLpy", "DLpy.*"]),  # Changed from DLpy to DLpy
0007:     install_requires=[
0008:         "numpy>=1.20.0",
0009:     ],
0010:     extras_require={
0011:         "dev": [
0012:             "pytest>=7.0.0",
0013:             "pytest-cov>=4.0.0",
0014:             "pytest-xdist>=3.0.0",
0015:             "black>=22.0.0",
0016:             "isort>=5.0.0",
0017:             "mypy>=1.0.0",
0018:         ],
0019:     },
0020:     python_requires=">=3.8",
0021: )

// File: C:\Users\aluja\Desktop\DLpy\tests\__init__.py
// ----------------------------------------

// File: C:\Users\aluja\Desktop\DLpy\tests\test_activations.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: from DLpy.nn.activations import (
0005:     relu, leaky_relu, elu, gelu, sigmoid, tanh,
0006:     ReLU, LeakyReLU, ELU, GELU, Sigmoid, Tanh
0007: )
0008: 
0009: class TestActivations:
0010:     """Tests for activation functions"""
0011:     
0012:     def test_relu(self):
0013:         """Test ReLU activation"""
0014:         x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
0015:         y = relu(x)
0016:         y.backward(np.ones_like(x.data))
0017:         
0018:         assert np.array_equal(y.data, [0.0, 0.0, 0.0, 1.0, 2.0])
0019:         assert np.array_equal(x.grad, [0.0, 0.0, 0.0, 1.0, 1.0])
0020:         
0021:     def test_leaky_relu(self):
0022:         """Test Leaky ReLU activation"""
0023:         x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
0024:         slope = 0.1
0025:         y = leaky_relu(x, negative_slope=slope)
0026:         y.backward(np.ones_like(x.data))
0027:         
0028:         expected_forward = [-0.2, -0.1, 0.0, 1.0, 2.0]
0029:         expected_backward = [slope, slope, slope, 1.0, 1.0]
0030:         
0031:         assert np.allclose(y.data, expected_forward)
0032:         assert np.allclose(x.grad, expected_backward)
0033:         
0034:     def test_elu(self):
0035:         """Test ELU activation"""
0036:         x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
0037:         alpha = 1.0
0038:         y = elu(x, alpha=alpha)
0039:         y.backward(np.ones_like(x.data))
0040:         
0041:         expected_forward = [alpha * (np.exp(-2.0) - 1), alpha * (np.exp(-1.0) - 1), 0.0, 1.0, 2.0]
0042:         expected_backward = [alpha * np.exp(-2.0), alpha * np.exp(-1.0), alpha * 1.0, 1.0, 1.0]
0043:         
0044:         assert np.allclose(y.data, expected_forward)
0045:         assert np.allclose(x.grad, expected_backward)
0046:         
0047:     def test_gelu(self):
0048:         """Test GELU activation"""
0049:         x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
0050:         y = gelu(x)
0051:         y.backward(np.ones_like(x.data))
0052:         
0053:         # Values should be finite and have correct shape
0054:         assert not np.any(np.isnan(y.data))
0055:         assert not np.any(np.isinf(y.data))
0056:         assert y.data.shape == x.data.shape
0057:         assert not np.any(np.isnan(x.grad))
0058:         assert not np.any(np.isinf(x.grad))
0059:         
0060:         # Test specific known values
0061:         assert np.allclose(y.data[2], 0.0)  # GELU(0) = 0
0062:         assert y.data[3] > 0.8  # GELU(1) ≈ 0.841
0063:         assert y.data[1] < 0  # GELU(-1) is negative
0064:         
0065:     def test_sigmoid(self):
0066:         """Test Sigmoid activation"""
0067:         x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
0068:         y = sigmoid(x)
0069:         y.backward(np.ones_like(x.data))
0070:         
0071:         sigmoid_x = 1 / (1 + np.exp(-x.data))
0072:         expected_backward = sigmoid_x * (1 - sigmoid_x)
0073:         
0074:         assert np.allclose(y.data, sigmoid_x)
0075:         assert np.allclose(x.grad, expected_backward)
0076:         
0077:         # Test special values
0078:         assert np.allclose(y.data[2], 0.5)  # sigmoid(0) = 0.5
0079:         assert y.data[0] < 0.5  # sigmoid(-2) < 0.5
0080:         assert y.data[4] > 0.5  # sigmoid(2) > 0.5
0081:         
0082:     def test_tanh(self):
0083:         """Test Tanh activation"""
0084:         x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
0085:         y = tanh(x)
0086:         y.backward(np.ones_like(x.data))
0087:         
0088:         expected_forward = np.tanh(x.data)
0089:         expected_backward = 1 - np.tanh(x.data) ** 2
0090:         
0091:         assert np.allclose(y.data, expected_forward)
0092:         assert np.allclose(x.grad, expected_backward)
0093:         
0094:         # Test special values
0095:         assert np.allclose(y.data[2], 0.0)  # tanh(0) = 0
0096:         assert y.data[0] < -0.9  # tanh(-2) ≈ -0.964
0097:         assert y.data[4] > 0.9   # tanh(2) ≈ 0.964
0098: 
0099: class TestNumericalStability:
0100:     """Tests for numerical stability of activation functions"""
0101:     
0102:     def test_sigmoid_stability(self):
0103:         """Test Sigmoid with large inputs"""
0104:         x = Tensor([1000.0, -1000.0], requires_grad=True)
0105:         y = sigmoid(x)
0106:         y.backward(np.ones_like(x.data))
0107:         
0108:         # Check that values are properly clamped
0109:         assert np.allclose(y.data[0], 1.0)
0110:         assert np.allclose(y.data[1], 0.0)
0111:         assert not np.any(np.isnan(y.data))
0112:         assert not np.any(np.isnan(x.grad))
0113:         
0114:     def test_elu_stability(self):
0115:         """Test ELU with large negative inputs"""
0116:         x = Tensor([-1000.0], requires_grad=True)
0117:         y = elu(x)
0118:         y.backward(np.ones_like(x.data))
0119:         
0120:         # Should be close to -1.0 for large negative values
0121:         assert np.allclose(y.data[0], -1.0, rtol=1e-3)
0122:         assert not np.any(np.isnan(y.data))
0123:         assert not np.any(np.isnan(x.grad))
0124:         
0125:     def test_gelu_stability(self):
0126:         """Test GELU with large inputs"""
0127:         x = Tensor([1000.0, -1000.0], requires_grad=True)
0128:         y = gelu(x)
0129:         y.backward(np.ones_like(x.data))
0130:         
0131:         # Check that outputs are finite
0132:         assert not np.any(np.isnan(y.data))
0133:         assert not np.any(np.isinf(y.data))
0134:         assert not np.any(np.isnan(x.grad))
0135:         assert not np.any(np.isinf(x.grad))
0136: 
0137: class TestGradientFlow:
0138:     """Tests for gradient flow through activation functions"""
0139:     
0140:     def test_relu_dead_neurons(self):
0141:         """Test ReLU gradient flow for negative inputs"""
0142:         x = Tensor([-1.0], requires_grad=True)
0143:         y = relu(x)
0144:         y.backward(np.array([1.0]))
0145:         
0146:         assert x.grad[0] == 0.0  # Gradient should be zero for negative input
0147:         
0148:     def test_leaky_relu_gradient_flow(self):
0149:         """Test Leaky ReLU gradient flow"""
0150:         x = Tensor([-1.0], requires_grad=True)
0151:         slope = 0.01
0152:         y = leaky_relu(x, negative_slope=slope)
0153:         y.backward(np.array([1.0]))
0154:         
0155:         assert x.grad[0] == slope  # Should have small but non-zero gradient
0156:         
0157:     def test_elu_gradient_flow(self):
0158:         """Test ELU gradient flow"""
0159:         x = Tensor([-1.0, 1.0], requires_grad=True)
0160:         y = elu(x)
0161:         y.backward(np.ones_like(x.data))
0162:         
0163:         assert x.grad[0] > 0.0  # Should have positive gradient for negative input
0164:         assert x.grad[1] == 1.0  # Should have gradient 1 for positive input
0165: 
0166: class TestShapes:
0167:     """Tests for handling different input shapes"""
0168:     
0169:     def test_batch_input(self):
0170:         """Test activations with batched input"""
0171:         batch_size, features = 32, 10
0172:         x = Tensor(np.random.randn(batch_size, features), requires_grad=True)
0173:         
0174:         # Test all activations with batched input
0175:         activations = [relu, leaky_relu, elu, gelu, sigmoid, tanh]
0176:         for activation in activations:
0177:             y = activation(x)
0178:             assert y.shape == x.shape
0179:             y.backward(np.ones_like(x.data))
0180:             assert x.grad.shape == x.shape
0181:             
0182:     def test_scalar_input_single(self):
0183:         """Test scalar input handling for a single activation"""
0184:         x = Tensor(2.0, requires_grad=True)
0185:         y = relu(x)
0186:         
0187:         # Check forward pass maintains scalar nature
0188:         assert y.data.ndim == 0
0189:         assert isinstance(y.data, np.ndarray)
0190:         assert y.data.shape == ()
0191:         
0192:         # Check backward pass (gradient should be size 1 array as in PyTorch)
0193:         y.backward(np.array(1.0))
0194:         assert x.grad.size == 1
0195:         assert isinstance(x.grad, np.ndarray)
0196: 
0197:     def test_scalar_input(self):
0198:         """Test activations with scalar input"""
0199:         x = Tensor(2.0, requires_grad=True)
0200:         
0201:         # Test all activations with scalar input
0202:         activations = [relu, leaky_relu, elu, gelu, sigmoid, tanh]
0203:         for activation in activations:
0204:             y = activation(x)
0205:             assert y.data.ndim == 0  # Should preserve scalar nature
0206:             y.backward(np.array(1.0))
0207:             assert x.grad.size == 1  # Gradient should be size 1 array (matching PyTorch behavior)
0208: 
0209: class TestCustomGradients:
0210:     """Tests for custom gradient computations"""
0211:     
0212:     def test_relu_custom_gradient(self):
0213:         """Test ReLU with custom gradient"""
0214:         x = Tensor([1.0, -1.0], requires_grad=True)
0215:         y = relu(x)
0216:         y.backward(np.array([2.0, 2.0]))  # Custom gradient values
0217:         
0218:         assert np.array_equal(x.grad, [2.0, 0.0])  # Should scale gradient for positive input
0219:         
0220:     def test_sigmoid_custom_gradient(self):
0221:         """Test Sigmoid with custom gradient"""
0222:         x = Tensor([0.0], requires_grad=True)
0223:         y = sigmoid(x)
0224:         y.backward(np.array([2.0]))  # Custom gradient value
0225:         
0226:         expected_grad = 2.0 * 0.25  # 2.0 * sigmoid(0) * (1 - sigmoid(0))
0227:         assert np.allclose(x.grad, expected_grad)

// File: C:\Users\aluja\Desktop\DLpy\tests\test_autograd.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import (
0004:     Tensor,
0005:     get_autograd_engine
0006: )
0007: from DLpy.ops import Add, Multiply
0008: from DLpy.core.autograd import Edge
0009: 
0010: class TestAutogradEngine:
0011:     """Tests for the autograd engine's core functionality."""
0012:     
0013:     def setup_method(self):
0014:         """Setup method run before each test."""
0015:         self.engine = get_autograd_engine()
0016:         self.engine.clear()
0017: 
0018:     def test_register_tensor(self):
0019:         """Test registering a tensor with the autograd engine."""
0020:         tensor = Tensor([1.0], requires_grad=True)
0021:         self.engine.register_tensor(tensor)
0022:         assert id(tensor) in self.engine._nodes
0023: 
0024:     def test_add_edge(self):
0025:         """Test adding edges between tensors in the computational graph."""
0026:         t1 = Tensor([1.0], requires_grad=True)
0027:         t2 = Tensor([2.0], requires_grad=True)
0028:         
0029:         self.engine.register_tensor(t1)
0030:         self.engine.register_tensor(t2)
0031:         self.engine.add_edge(t1, t2)
0032:         
0033:         node1 = self.engine._nodes[id(t1)]
0034:         node2 = self.engine._nodes[id(t2)]
0035:         
0036:         assert len(node1.out_edges) == 1
0037:         assert len(node2.in_edges) == 1
0038:         assert node1.out_edges[0].dst == node2
0039: 
0040: class TestGradientComputation:
0041:     """Tests for gradient computation in different graph structures."""
0042:     
0043:     def setup_method(self):
0044:         self.engine = get_autograd_engine()
0045:         self.engine.clear()
0046: 
0047:     def test_linear_graph(self):
0048:         """Test gradient computation in a linear graph."""
0049:         # Create a simple linear computation: z = 2x + y
0050:         x = Tensor([2.0], requires_grad=True)
0051:         y = Tensor([3.0], requires_grad=True)
0052:         z = Add.apply(x, y)
0053:         self.engine.backward(z, np.array([1.0]))
0054:         
0055:         # Check gradients
0056:         assert np.allclose(x.grad, [1.0])
0057:         assert np.allclose(y.grad, [1.0])
0058: 
0059:     def test_branching_graph(self):
0060:         """Test gradient computation in a graph with multiple paths."""
0061: 
0062:         # The test creates a computation graph shaped like:
0063:         #     x
0064:         #   /   \  
0065:         #  y1   y2
0066:         #   \   /
0067:         #     z
0068: 
0069:         # This tests whether gradients properly flow and accumulate through 
0070:         # multiple paths back to the same input.
0071:         x = Tensor([2.0], requires_grad=True)
0072:         y1 = Multiply.apply(x, Tensor([2.0]))  # y1 = 2x
0073:         y2 = Multiply.apply(x, Tensor([3.0]))  # y2 = 3x
0074:         z = Add.apply(y1, y2)  # z = y1 + y2 = 5x
0075: 
0076:         self.engine.backward(z, np.array([1.0]))
0077:         # Gradient should be 5.0 (sum of both paths: 2 + 3)
0078:         assert np.allclose(x.grad, [5.0])
0079: 
0080:     def test_diamond_graph(self):
0081:         """Test gradient computation in a diamond-shaped graph."""
0082:         # Create a diamond computation:
0083:         #     x
0084:         #    / \
0085:         #   h1  h2
0086:         #    \ /
0087:         #     y
0088:         x = Tensor([1.0], requires_grad=True)
0089:         w1 = Tensor([2.0], requires_grad=True)
0090:         w2 = Tensor([3.0], requires_grad=True)
0091:         
0092:         h1 = Multiply.apply(x, w1)
0093:         h2 = Multiply.apply(x, w2)
0094:         y = Add.apply(h1, h2)
0095:         
0096:         self.engine.backward(y, np.array([1.0]))
0097:         
0098:         # x's gradient should include effects from both paths
0099:         assert np.allclose(x.grad, [5.0])  # 2 + 3
0100:         assert np.allclose(w1.grad, [1.0])
0101:         assert np.allclose(w2.grad, [1.0])
0102: 
0103: class TestGradientAccumulation:
0104:     """Tests for correct gradient accumulation behavior."""
0105: 
0106:     def setup_method(self):
0107:         self.engine = get_autograd_engine()
0108:         self.engine.clear()
0109: 
0110:     def test_reused_variable(self):
0111:         """Test gradient accumulation when a variable is used multiple times."""
0112:         x = Tensor([2.0], requires_grad=True)
0113:         
0114:         # Use x in three separate computations
0115:         y1 = Multiply.apply(x, Tensor([2.0]))
0116:         y2 = Multiply.apply(x, Tensor([3.0]))
0117:         y3 = Multiply.apply(x, Tensor([4.0]))
0118:         
0119:         # Backward on all three outputs
0120:         self.engine.backward(y1, np.array([1.0]))
0121:         self.engine.backward(y2, np.array([1.0]))
0122:         self.engine.backward(y3, np.array([1.0]))
0123:         
0124:         # Gradient should accumulate: 2 + 3 + 4 = 9
0125:         assert np.allclose(x.grad, [9.0])
0126: 
0127:     def test_shared_structure(self):
0128:         """Test gradient computation with shared subgraphs."""
0129:         # Create a computation where the same subgraph is used multiple times
0130:         x = Tensor([1.0], requires_grad=True)
0131:         y = Tensor([2.0], requires_grad=True)
0132:         
0133:         # Shared computation
0134:         shared = Multiply.apply(x, y)
0135:         
0136:         # Use shared result multiple times
0137:         out1 = Multiply.apply(shared, Tensor([2.0]))
0138:         out2 = Multiply.apply(shared, Tensor([3.0]))
0139:         
0140:         # Final sum
0141:         final = Add.apply(out1, out2)
0142:         
0143:         self.engine.backward(final, np.array([1.0]))
0144:         
0145:         # Verify gradients include effects from all paths
0146:         assert x.grad is not None
0147:         assert y.grad is not None
0148: 
0149: class TestAdvancedAutogradFeatures:
0150:     """Tests for advanced AutogradEngine features and edge cases"""
0151:     
0152:     def setup_method(self):
0153:         self.engine = get_autograd_engine()
0154:         self.engine.clear()
0155: 
0156:     def test_validate_graph(self):
0157:         """Test graph validation functionality"""
0158:         # Create a disconnected subgraph
0159:         x = Tensor([1.0], requires_grad=True)
0160:         y = Tensor([2.0], requires_grad=True)
0161:         _ = Add.apply(x, y)
0162:         
0163:         # Add an isolated node
0164:         z = Tensor([3.0], requires_grad=True)
0165:         self.engine.register_tensor(z)
0166:         
0167:         warnings = self.engine.validate_graph()
0168:         assert len(warnings) > 0
0169:         assert "isolated nodes" in warnings[0]  
0170: 
0171:     def test_nested_gradient_computation(self):
0172:         """Test detection of nested gradient computations"""
0173:         x = Tensor([1.0], requires_grad=True)
0174:         y = Add.apply(x, Tensor([2.0]))
0175:         
0176:         # Simulate nested gradient computation
0177:         self.engine._currently_computing_gradients = True
0178:         with pytest.raises(RuntimeError, match="Nested gradient computation detected"):
0179:             self.engine.backward(y)
0180:         self.engine._currently_computing_gradients = False
0181: 
0182:     def test_cyclic_graph_detection(self):
0183:         """Test detection of cycles in computational graph"""
0184:         x = Tensor([1.0], requires_grad=True)
0185:         y = Tensor([2.0], requires_grad=True)
0186:         
0187:         # Manually create a cycle in the graph
0188:         node_x = self.engine._nodes[id(x)]
0189:         node_y = self.engine._nodes[id(y)]
0190:         
0191:         edge1 = Edge(node_x, node_y)
0192:         edge2 = Edge(node_y, node_x)
0193:         
0194:         node_x.out_edges.append(edge1)
0195:         node_y.in_edges.append(edge1)
0196:         node_y.out_edges.append(edge2)
0197:         node_x.in_edges.append(edge2)
0198:         
0199:         with pytest.raises(RuntimeError, match="Cycle detected in computation graph"):
0200:             self.engine.backward(x)
0201: 
0202:     def test_gradient_shape_mismatch(self):
0203:         """Test detection of gradient shape mismatches"""
0204:         x = Tensor([[1.0]], requires_grad=True)  # Shape (1, 1)
0205:         y = Tensor([2.0], requires_grad=True)    # Shape (1,)
0206:         
0207:         # Create edge with obviously wrong shape
0208:         node_x = self.engine._nodes[id(x)]
0209:         node_y = self.engine._nodes[id(y)]
0210:         
0211:         edge = Edge(node_x, node_y)
0212:         edge.grad = np.array([[1.0, 2.0]])  # Wrong shape (1, 2)
0213:         
0214:         # Add the edge to both nodes and the engine
0215:         node_x.out_edges.append(edge)
0216:         node_y.in_edges.append(edge)
0217:         self.engine._edges.add(edge)
0218:         
0219:         warnings = self.engine.validate_graph()
0220:         assert any("shape mismatch" in w for w in warnings), "Should detect shape mismatch"

// File: C:\Users\aluja\Desktop\DLpy\tests\test_cnn.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: from DLpy.nn import Conv2d
0005: from DLpy.ops.cnn import (
0006:     Conv2dFunction, ConvMode,
0007:     _compute_conv_output_shape,
0008:     _unfold,
0009:     _fold,
0010:     _bilinear_interpolate,
0011:     _generate_grid,
0012:     _deform_grid,
0013:     _get_deformable_offsets, 
0014:     _compute_conv_grad_input_padding,
0015:     _col2im_dilated
0016: )
0017: 
0018: class TestConv2d:
0019:     """Tests for Conv2d module."""
0020:     
0021:     def test_conv2d_asymmetric_kernel(self):
0022:         """Test Conv2d with asymmetric kernel."""
0023:         batch_size = 2
0024:         in_channels = 3
0025:         out_channels = 16
0026:         height = width = 32
0027:         kernel_size = (3, 5)  # Asymmetric kernel
0028:         
0029:         conv = Conv2d(in_channels, out_channels, kernel_size)
0030:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0031:         output = conv(x)
0032:         
0033:         # Check output shape
0034:         expected_height = height - kernel_size[0] + 1
0035:         expected_width = width - kernel_size[1] + 1
0036:         assert output.shape == (batch_size, out_channels, expected_height, expected_width)
0037: 
0038:     def test_conv2d_asymmetric_stride(self):
0039:         """Test Conv2d with different strides for height and width."""
0040:         batch_size = 2
0041:         in_channels = 3
0042:         out_channels = 16
0043:         height = width = 32
0044:         kernel_size = 3
0045:         stride = (2, 3)  # Different strides
0046:         
0047:         conv = Conv2d(in_channels, out_channels, kernel_size, stride=stride)
0048:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0049:         output = conv(x)
0050:         
0051:         expected_height = (height - kernel_size) // stride[0] + 1
0052:         expected_width = (width - kernel_size) // stride[1] + 1
0053:         assert output.shape == (batch_size, out_channels, expected_height, expected_width)
0054: 
0055:     def test_conv2d_asymmetric_padding(self):
0056:         """Test Conv2d with different padding for height and width."""
0057:         batch_size = 2
0058:         in_channels = 3
0059:         out_channels = 16
0060:         height = width = 32
0061:         kernel_size = 3
0062:         padding = (1, 2)  # Different padding
0063:         
0064:         conv = Conv2d(in_channels, out_channels, kernel_size, padding=padding)
0065:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0066:         output = conv(x)
0067:         
0068:         expected_height = height + 2*padding[0] - kernel_size + 1
0069:         expected_width = width + 2*padding[1] - kernel_size + 1
0070:         assert output.shape == (batch_size, out_channels, expected_height, expected_width)
0071: 
0072:     def test_conv2d_dilated(self):
0073:         """Test dilated convolution."""
0074:         batch_size = 2
0075:         in_channels = 3
0076:         out_channels = 16
0077:         height = width = 32
0078:         kernel_size = 3
0079:         dilation = 2
0080:         
0081:         conv = Conv2d(in_channels, out_channels, kernel_size, dilation=dilation)
0082:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0083:         output = conv(x)
0084:         
0085:         effective_kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)
0086:         expected_height = height - effective_kernel_size + 1
0087:         expected_width = width - effective_kernel_size + 1
0088:         assert output.shape == (batch_size, out_channels, expected_height, expected_width)
0089: 
0090:     def test_col2im_dilated(self):
0091:         """Test column to image conversion with dilation"""
0092:         batch_size = 2
0093:         in_channels = 3
0094:         height = width = 8
0095:         kernel_size = 3
0096:         
0097:         # Create sample columns
0098:         cols = np.random.randn(in_channels * kernel_size * kernel_size, 
0099:                             batch_size * height * width)
0100:         
0101:         # Test with different dilation rates
0102:         dilations = [(1, 1), (2, 2), (1, 2)]
0103:         for dilation in dilations:
0104:             output = _col2im_dilated(cols, 
0105:                                     (batch_size, in_channels, height, width),
0106:                                     (kernel_size, kernel_size),
0107:                                     (1, 1),  # stride
0108:                                     dilation)
0109:             
0110:             assert output.shape == (batch_size, in_channels, height, width)
0111: 
0112:     def test_conv2d_groups(self):
0113:         """Test grouped convolution."""
0114:         batch_size = 2
0115:         in_channels = 4
0116:         out_channels = 4
0117:         height = width = 32
0118:         kernel_size = 3
0119:         groups = 2
0120:         
0121:         conv = Conv2d(in_channels, out_channels, kernel_size, groups=groups)
0122:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0123:         output = conv(x)
0124:         
0125:         expected_height = height - kernel_size + 1
0126:         expected_width = width - kernel_size + 1
0127:         assert output.shape == (batch_size, out_channels, expected_height, expected_width)
0128: 
0129:     def test_conv2d_output_shape_calculation(self):
0130:         """Test the calc_output_shape static method"""
0131:         input_shape = (1, 3, 32, 32)
0132:         out_channels = 16
0133:         kernel_size = (3, 3)
0134:         stride = (2, 2)
0135:         padding = (1, 1)
0136:         dilation = (1, 1)
0137:         
0138:         output_shape = Conv2d.calc_output_shape(
0139:             input_shape, out_channels, kernel_size, stride, padding, dilation
0140:         )
0141:         assert output_shape == (1, 16, 16, 16)
0142: 
0143:     def test_conv2d_invalid_groups(self):
0144:         """Test error handling for invalid group configurations"""
0145:         with pytest.raises(ValueError):
0146:             # in_channels not divisible by groups
0147:             Conv2d(5, 10, kernel_size=3, groups=2)
0148:             
0149:         with pytest.raises(ValueError):
0150:             # out_channels not divisible by groups
0151:             Conv2d(4, 5, kernel_size=3, groups=2)
0152: 
0153:     def test_conv2d_extra_repr(self):
0154:         """Test string representation of Conv2d layer"""
0155:         conv = Conv2d(3, 16, kernel_size=3, stride=2, padding=1, dilation=2, groups=1)
0156:         repr_str = conv.extra_repr()
0157:         assert '3, 16' in repr_str
0158:         assert 'kernel_size=(3, 3)' in repr_str
0159:         assert 'stride=(2, 2)' in repr_str
0160: 
0161: class TestConv2dFunction:
0162:     """Tests for Conv2dFunction and related helper functions."""
0163:     
0164:     def test_deformable_conv_forward(self):
0165:         """Test forward pass of deformable convolution."""
0166:         batch_size = 2
0167:         in_channels = 3
0168:         out_channels = 16
0169:         height = width = 8
0170:         kernel_size = 3
0171:         
0172:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0173:         weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size))
0174:         offset = Tensor(np.random.randn(batch_size, 2*kernel_size*kernel_size, height-2, width-2))
0175:         bias = Tensor(np.random.randn(out_channels))
0176:         
0177:         output = Conv2dFunction.apply(x, weight, bias, (1, 1), (0, 0), (1, 1), 1,
0178:                                     ConvMode.DEFORMABLE, offset)
0179: 
0180:     def test_modulated_deform_conv(self):
0181:         """Test modulated deformable convolution."""
0182:         batch_size = 2
0183:         in_channels = 3
0184:         out_channels = 16
0185:         height = width = 8
0186:         kernel_size = 3
0187:         
0188:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0189:         weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size))
0190:         offset = Tensor(np.random.randn(batch_size, 2*kernel_size*kernel_size, height-2, width-2))
0191:         mask = Tensor(np.random.randn(batch_size, kernel_size*kernel_size, height-2, width-2))
0192:         bias = Tensor(np.random.randn(out_channels))
0193:         
0194:         output = Conv2dFunction.apply(x, weight, bias, (1, 1), (0, 0), (1, 1), 1,
0195:                                     ConvMode.DEFORMABLE, offset, mask)
0196: 
0197:     def test_bilinear_interpolate(self):
0198:         """Test bilinear interpolation function"""
0199:         input_tensor = np.random.randn(1, 3, 4, 4)  # Change batch size to 1
0200:         points = np.array([[[-1.0, -1.0]]])  # Use normalized coordinates in [-1, 1] range
0201:         
0202:         output = _bilinear_interpolate(input_tensor, points)
0203:         assert output.shape == (1, 3, 1)  # (batch_size, channels, num_points)
0204:         
0205:     def test_compute_conv_output_shape(self):
0206:         """Test output shape computation for different configurations"""
0207:         input_size = 32
0208:         kernel_size = 3
0209:         stride = 2
0210:         padding = 1
0211:         dilation = 1
0212:         
0213:         output_size = _compute_conv_output_shape(
0214:             input_size, kernel_size, stride, padding, dilation
0215:         )
0216:         expected = ((32 + 2*1 - 1*(3-1) - 1) // 2) + 1
0217:         assert output_size == expected
0218: 
0219:     def test_unfold_fold_operations(self):
0220:         """Test unfold and fold operations are inverses"""
0221:         input_tensor = np.random.randn(2, 3, 8, 8)
0222:         kernel_size = (3, 3)
0223:         stride = (1, 1)
0224:         padding = (1, 1)
0225:         dilation = (1, 1)
0226:         
0227:         unfolded = _unfold(input_tensor, kernel_size, dilation, padding, stride)
0228:         folded = _fold(unfolded, (8, 8), kernel_size, dilation, padding, stride)
0229:         
0230:         assert np.allclose(input_tensor, folded, atol=1e-6)
0231:     
0232:     def test_backward_standard_comprehensive(self):
0233:         """Test standard convolution backward pass with various configurations"""
0234:         batch_size = 2
0235:         in_channels = 4
0236:         out_channels = 8
0237:         height = width = 8
0238:         kernel_size = 3
0239:         
0240:         # Test different stride/padding combinations
0241:         configs = [
0242:             ((1, 1), (0, 0)),  # Basic case
0243:             ((2, 2), (1, 1)),  # With stride and padding
0244:             ((1, 2), (2, 1)),  # Asymmetric stride and padding
0245:         ]
0246:         
0247:         for stride, padding in configs:
0248:             # Set requires_grad=True for tensors that need gradients
0249:             x = Tensor(np.random.randn(batch_size, in_channels, height, width), requires_grad=True)
0250:             weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size), requires_grad=True)
0251:             bias = Tensor(np.random.randn(out_channels), requires_grad=True)
0252:             
0253:             # Forward pass
0254:             output = Conv2dFunction.apply(x, weight, bias, stride, padding)
0255:             
0256:             # Backward pass
0257:             grad_output = np.random.randn(*output.shape)
0258:             output.backward(grad_output)
0259:             
0260:             # Verify gradients exist and have correct shapes
0261:             assert x.grad is not None
0262:             assert weight.grad is not None
0263:             assert bias.grad is not None
0264:             
0265:             # Verify gradient shapes
0266:             assert x.grad.shape == x.shape
0267:             assert weight.grad.shape == weight.shape
0268:             assert bias.grad.shape == bias.shape
0269: 
0270:     def test_deformable_conv_with_mask(self):
0271:         """Test deformable convolution with modulation mask"""
0272:         batch_size = 2
0273:         in_channels = 3
0274:         out_channels = 16
0275:         height = width = 8
0276:         kernel_size = 3
0277:         
0278:         x = Tensor(np.random.randn(batch_size, in_channels, height, width), requires_grad=True)
0279:         weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size), requires_grad=True)
0280:         offset = Tensor(np.random.randn(batch_size, 2*kernel_size*kernel_size, height-2, width-2), requires_grad=True)
0281:         mask = Tensor(np.random.randn(batch_size, kernel_size*kernel_size, height-2, width-2), requires_grad=True)
0282:         bias = Tensor(np.random.randn(out_channels), requires_grad=True)  # Add bias
0283:         
0284:         output = Conv2dFunction.apply(x, weight, bias, (1, 1), (0, 0), (1, 1), 1,
0285:                                     ConvMode.DEFORMABLE, offset, mask)
0286:         
0287:         grad_output = np.random.randn(*output.shape)
0288:         output.backward(grad_output)
0289:     
0290:     def test_transposed_conv_output_padding(self):
0291:         """Test transposed convolution with output padding"""
0292:         batch_size = 2
0293:         in_channels = 3
0294:         out_channels = 16
0295:         height = width = 8
0296:         kernel_size = 3
0297:         stride = (2, 2)
0298:         padding = (1, 1)
0299: 
0300:         x = Tensor(np.random.randn(batch_size, in_channels, height, width))
0301:         # For transposed conv, weight shape should be (in_channels, out_channels/groups, kernel_size, kernel_size)
0302:         weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size))  # Not swapped, keep original format
0303: 
0304:         # Forward pass
0305:         H_out = (height - 1) * stride[0] - 2 * padding[0] + kernel_size
0306:         W_out = (width - 1) * stride[1] - 2 * padding[1] + kernel_size
0307:         
0308:         # Test with different output paddings
0309:         output_paddings = [(0, 0), (1, 1), (1, 0)]
0310:         for output_padding in output_paddings:
0311:             # Add output padding to expected dimensions
0312:             expected_H = H_out + output_padding[0]
0313:             expected_W = W_out + output_padding[1]
0314:             
0315:             output = Conv2dFunction.apply(x, weight, None, stride, padding, (1, 1), 1,
0316:                                     ConvMode.TRANSPOSED, output_padding=output_padding)
0317:             
0318:             assert output.shape == (batch_size, out_channels, expected_H, expected_W)
0319:         
0320:     def test_conv2d_edge_cases(self):
0321:         """Test convolution with edge cases"""
0322:         # Test 1x1 convolution
0323:         x = Tensor(np.random.randn(2, 3, 8, 8))
0324:         weight = Tensor(np.random.randn(4, 3, 1, 1))
0325:         output = Conv2dFunction.apply(x, weight)
0326:         assert output.shape == (2, 4, 8, 8)
0327:         
0328:         # Test with large kernel
0329:         x = Tensor(np.random.randn(2, 3, 16, 16))
0330:         weight = Tensor(np.random.randn(4, 3, 7, 7))
0331:         output = Conv2dFunction.apply(x, weight)
0332:         assert output.shape == (2, 4, 10, 10)
0333:     
0334:     def test_conv2d_validation_errors(self):
0335:         """Test proper error handling"""
0336:         with pytest.raises(ValueError):
0337:             # Test invalid group configuration
0338:             x = Tensor(np.random.randn(2, 3, 8, 8))
0339:             weight = Tensor(np.random.randn(4, 2, 3, 3))  # Wrong in_channels per group
0340:             Conv2dFunction.apply(x, weight, groups=2)
0341: 
0342: class TestHelperFunctions:
0343:     """Tests for CNN helper functions."""
0344:     
0345:     def test_compute_output_shape(self):
0346:         """Test output shape computation."""
0347:         input_size = 32
0348:         kernel_size = 3
0349:         stride = 1
0350:         padding = 0
0351:         dilation = 1
0352:         
0353:         output_size = _compute_conv_output_shape(
0354:             input_size, kernel_size, stride, padding, dilation
0355:         )
0356:         assert output_size == 30  # 32 - 3 + 1
0357: 
0358:     def test_unfold_operation(self):
0359:         """Test im2col (unfold) operation."""
0360:         batch_size = 2
0361:         channels = 3
0362:         height = width = 8
0363:         kernel_size = (3, 3)
0364:         
0365:         input_tensor = np.random.randn(batch_size, channels, height, width)
0366:         unfolded = _unfold(input_tensor, kernel_size, (1, 1), (0, 0), (1, 1))
0367:         
0368:         # Check unfolded shape
0369:         expected_unfold_shape = (channels * kernel_size[0] * kernel_size[1],
0370:                                batch_size * (height - kernel_size[0] + 1) * 
0371:                                (width - kernel_size[1] + 1))
0372:         assert unfolded.shape == expected_unfold_shape
0373: 
0374:     def test_fold_operation(self):
0375:         """Test col2im (fold) operation."""
0376:         batch_size = 2
0377:         channels = 3
0378:         height = width = 8
0379:         kernel_size = (3, 3)
0380:         
0381:         # Create random input and unfold it
0382:         input_tensor = np.random.randn(batch_size, channels, height, width)
0383:         unfolded = _unfold(input_tensor, kernel_size, (1, 1), (0, 0), (1, 1))
0384:         
0385:         # Fold back
0386:         folded = _fold(unfolded, (height, width), kernel_size, (1, 1), (0, 0), (1, 1))
0387:         
0388:         # Check folded shape
0389:         assert folded.shape == input_tensor.shape
0390: 
0391:     def test_bilinear_interpolate(self):
0392:         """Test bilinear interpolation function"""
0393:         input_tensor = np.random.randn(1, 3, 4, 4)  # Changed batch size to 1
0394:         points = np.array([[[0.5, 0.5]]])  # Shape should be (1, 1, 2)
0395:         
0396:         output = _bilinear_interpolate(input_tensor, points)
0397:         assert output.shape == (1, 3, 1)  # (N, C, P)
0398: 
0399:     def test_generate_grid(self):
0400:         """Test sampling grid generation."""
0401:         batch_size = 2
0402:         height = 8
0403:         width = 8
0404:         
0405:         grid = _generate_grid(batch_size, height, width)
0406:         assert grid.shape == (batch_size, height, width, 2)
0407:         assert np.all(grid >= -1) and np.all(grid <= 1)
0408: 
0409:     def test_deform_grid(self):
0410:         """Test grid deformation."""
0411:         batch_size = 2
0412:         height = 8
0413:         width = 8
0414:         
0415:         grid = _generate_grid(batch_size, height, width)
0416:         offset = np.random.randn(batch_size, 2, height, width) * 0.1
0417:         
0418:         deformed = _deform_grid(grid, offset)
0419:         assert deformed.shape == (batch_size, height, width, 2)
0420:         assert np.all(deformed >= -1) and np.all(deformed <= 1)
0421: 
0422:     def test_numerical_gradient_deformable(self):
0423:         """Test numerical gradient computation for deformable convolution."""
0424:         batch_size = 2
0425:         in_channels = 2
0426:         out_channels = 3
0427:         height = width = 5
0428:         kernel_size = 3
0429:         
0430:         x = Tensor(np.random.randn(batch_size, in_channels, height, width), requires_grad=True)
0431:         weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size), 
0432:                        requires_grad=True)
0433:         offset = Tensor(np.random.randn(batch_size, 2*kernel_size*kernel_size, height-2, width-2), 
0434:                        requires_grad=True)
0435:         setattr(weight, 'offset', offset)
0436:         bias = Tensor(np.random.randn(out_channels), requires_grad=True)
0437:         
0438:         def compute_loss(x, w, b):
0439:             return np.sum(Conv2dFunction.apply(x, w, b, (1, 1), (0, 0), (1, 1), 1, 
0440:                                              ConvMode.DEFORMABLE).data)
0441:         
0442:         # Compute analytical gradients
0443:         output = Conv2dFunction.apply(x, weight, bias, (1, 1), (0, 0), (1, 1), 1, 
0444:                                     ConvMode.DEFORMABLE)
0445:         output.backward(np.ones_like(output.data))
0446:         
0447:         # Verify offset gradients exist and have correct shape
0448:         assert offset.grad is not None
0449:         assert offset.grad.shape == offset.shape
0450:     
0451:     def test_get_deformable_offsets(self):
0452:         """Test deformable convolution offset computation"""
0453:         batch_size, in_channels = 2, 3
0454:         height, width = 8, 8
0455:         kernel_size = (3, 3)
0456:         
0457:         # Create sample offset tensor
0458:         offset_tensor = np.random.randn(batch_size, 2*kernel_size[0]*kernel_size[1], height-2, width-2)
0459:         input_shape = (batch_size, in_channels, height, width)
0460:         
0461:         sampling_locations = _get_deformable_offsets(offset_tensor, kernel_size, input_shape)
0462:         assert sampling_locations.shape == (batch_size, (height-2)*(width-2), kernel_size[0]*kernel_size[1], 2)
0463: 
0464:     def test_compute_conv_grad_input_padding(self):
0465:         """Test gradient input padding computation"""
0466:         grad_output_size = 8
0467:         input_size = 10
0468:         kernel_size = 3
0469:         stride = 1
0470:         padding = 1
0471:         dilation = 1
0472:         
0473:         grad_padding = _compute_conv_grad_input_padding(
0474:             grad_output_size, input_size, kernel_size, stride, padding, dilation
0475:         )
0476:         assert isinstance(grad_padding, int)
0477: 
0478:     def test_generate_grid(self):
0479:         """Test sampling grid generation for different align_corners settings"""
0480:         batch_size = 2
0481:         height = 4
0482:         width = 4
0483:         
0484:         # Test with align_corners=True
0485:         grid_aligned = _generate_grid(batch_size, height, width, align_corners=True)
0486:         assert grid_aligned.shape == (batch_size, height, width, 2)
0487:         # Check grid bounds
0488:         assert np.all(grid_aligned >= -1) and np.all(grid_aligned <= 1)
0489:         
0490:         # Test with align_corners=False
0491:         grid_unaligned = _generate_grid(batch_size, height, width, align_corners=False)
0492:         assert grid_unaligned.shape == (batch_size, height, width, 2)
0493:         assert np.all(grid_unaligned >= -1) and np.all(grid_unaligned <= 1)

// File: C:\Users\aluja\Desktop\DLpy\tests\test_context.py
// ----------------------------------------
0001: import pytest
0002: from DLpy.core import Context, Tensor
0003: import numpy as np
0004: 
0005: class TestContext:
0006:     """Tests for Context class functionality"""
0007:     
0008:     def test_save_and_retrieve_tensors(self):
0009:         """Test saving and retrieving tensors"""
0010:         ctx = Context()
0011:         tensor1 = Tensor([1.0])
0012:         tensor2 = Tensor([2.0])
0013:         
0014:         ctx.save_for_backward(tensor1, tensor2)
0015:         saved = ctx.saved_tensors
0016:         
0017:         assert len(saved) == 2
0018:         assert np.array_equal(saved[0].data, tensor1.data)
0019:         assert np.array_equal(saved[1].data, tensor2.data)
0020: 
0021:     def test_save_and_retrieve_arguments(self):
0022:         """Test saving and retrieving non-tensor arguments"""
0023:         ctx = Context()
0024:         ctx.save_arguments(arg1="test", arg2=42)
0025:         
0026:         args = ctx.saved_arguments
0027:         assert args["arg1"] == "test"
0028:         assert args["arg2"] == 42
0029:         assert isinstance(args, dict)
0030: 
0031:     def test_intermediate_values(self):
0032:         """Test storing and retrieving intermediate values"""
0033:         ctx = Context()
0034:         
0035:         # Store various types of values
0036:         ctx.store_intermediate("scalar", 42)
0037:         ctx.store_intermediate("list", [1, 2, 3])
0038:         ctx.store_intermediate("tensor", Tensor([1.0]))
0039:         
0040:         # Retrieve and verify values
0041:         assert ctx.get_intermediate("scalar") == 42
0042:         assert ctx.get_intermediate("list") == [1, 2, 3]
0043:         assert isinstance(ctx.get_intermediate("tensor"), Tensor)
0044:         
0045:         # Test retrieving non-existent key
0046:         with pytest.raises(KeyError):
0047:             ctx.get_intermediate("nonexistent")
0048: 
0049:     def test_clear_functionality(self):
0050:         """Test clearing all stored data"""
0051:         ctx = Context()
0052:         
0053:         # Store various types of data
0054:         ctx.save_for_backward(Tensor([1.0]))
0055:         ctx.save_arguments(arg1="test")
0056:         ctx.store_intermediate("key", "value")
0057:         
0058:         # Clear all data
0059:         ctx.clear()
0060:         
0061:         # Verify everything is cleared
0062:         assert len(ctx._saved_tensors) == 0
0063:         assert len(ctx._non_tensor_args) == 0
0064:         assert len(ctx._intermediate_values) == 0

// File: C:\Users\aluja\Desktop\DLpy\tests\test_function.py
// ----------------------------------------
0001: import pytest
0002: from DLpy.core import Function, Tensor
0003: import numpy as np
0004: 
0005: class TestFunction:
0006:     """Tests for Function base class and utilities"""
0007:     
0008:     class TestFunction(Function):
0009:         """Simple test function implementation"""
0010:         
0011:         @staticmethod
0012:         def forward(ctx, x, y=None):
0013:             ctx.save_for_backward(x)
0014:             ctx.save_arguments(y=y)
0015:             return Tensor(x.data * 2)
0016:             
0017:         @staticmethod
0018:         def backward(ctx, grad_output, grad_dict):
0019:             x, = ctx.saved_tensors
0020:             y = ctx.saved_arguments["y"]
0021:             
0022:             if x.requires_grad:
0023:                 grad_dict[id(x)] = grad_output * 2
0024: 
0025:     def test_function_application(self):
0026:         """Test applying a function to inputs"""
0027:         x = Tensor([1.0], requires_grad=True)
0028:         result = self.TestFunction.apply(x, y=2.0)
0029:         
0030:         assert isinstance(result, Tensor)
0031:         assert np.array_equal(result.data, [2.0])
0032:         assert result.requires_grad
0033:         assert result._backward_fn is not None
0034: 
0035:     def test_verify_backward(self):
0036:         """Test gradient verification utility"""
0037:         def forward_fn(x):
0038:             return x * 2
0039:             
0040:         def correct_backward_fn(ctx, grad_output):
0041:             return grad_output * 2
0042:             
0043:         def incorrect_backward_fn(ctx, grad_output):
0044:             return grad_output * 3
0045:         
0046:         # Test with correct gradients
0047:         x = np.array([1.0])
0048:         assert Function.verify_backward(forward_fn, correct_backward_fn, (x,))
0049:         
0050:         # Test with incorrect gradients
0051:         assert not Function.verify_backward(forward_fn, incorrect_backward_fn, (x,))
0052: 
0053:     def test_abstract_methods(self):
0054:         """Test that abstract methods raise NotImplementedError"""
0055:         
0056:         class IncompleteFunction(Function):
0057:             pass
0058:             
0059:         with pytest.raises(TypeError):
0060:             IncompleteFunction()

// File: C:\Users\aluja\Desktop\DLpy\tests\test_loss.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: from DLpy.ops.loss import (
0005:     MSELoss,
0006:     CrossEntropyLoss,
0007:     BinaryCrossEntropyLoss,
0008:     L1Loss,
0009:     HuberLoss,
0010:     KLDivLoss,
0011:     CosineSimilarityLoss,
0012:     HingeLoss,
0013:     FocalLoss
0014: )
0015: 
0016: class TestMSELoss:
0017:     """Tests for Mean Squared Error Loss"""
0018:     
0019:     def test_forward(self):
0020:         """Test forward pass of MSE loss"""
0021:         predictions = Tensor([[1.0, 2.0], [3.0, 4.0]])
0022:         targets = Tensor([[2.0, 1.0], [4.0, 3.0]])
0023:         
0024:         # Test mean reduction
0025:         loss = MSELoss.apply(predictions, targets, 'mean')
0026:         expected = np.mean((predictions.data - targets.data) ** 2)
0027:         assert np.allclose(loss.data, expected)
0028:         
0029:         # Test sum reduction
0030:         loss = MSELoss.apply(predictions, targets, 'sum')
0031:         expected = np.sum((predictions.data - targets.data) ** 2)
0032:         assert np.allclose(loss.data, expected)
0033:         
0034:     def test_backward(self):
0035:         """Test backward pass of MSE loss"""
0036:         predictions = Tensor([[1.0]], requires_grad=True)
0037:         targets = Tensor([[2.0]])
0038:         
0039:         loss = MSELoss.apply(predictions, targets, 'mean')
0040:         loss.backward()
0041:         
0042:         # For MSE, gradient should be 2(pred - target)/N
0043:         expected_grad = 2 * (predictions.data - targets.data) / np.prod(predictions.shape)
0044:         assert np.allclose(predictions.grad, expected_grad)
0045: 
0046: class TestCrossEntropyLoss:
0047:     """Tests for Cross Entropy Loss"""
0048:     
0049:     def test_forward(self):
0050:         """Test forward pass of cross entropy loss"""
0051:         predictions = Tensor([[1.0, 0.0], [0.0, 1.0]])
0052:         targets = Tensor([[1.0, 0.0], [0.0, 1.0]])  # One-hot encoded
0053:         
0054:         loss = CrossEntropyLoss.apply(predictions, targets)
0055:         assert loss.data >= 0  # Loss should be non-negative
0056:         
0057:     def test_numerical_stability(self):
0058:         """Test numerical stability with large inputs"""
0059:         predictions = Tensor([[1000., -1000.], [-1000., 1000.]])
0060:         targets = Tensor([[1., 0.], [0., 1.]])
0061:         
0062:         loss = CrossEntropyLoss.apply(predictions, targets)
0063:         assert not np.isnan(loss.data)
0064:         assert not np.isinf(loss.data)
0065:         
0066:     def test_gradient(self):
0067:         """Test gradient computation"""
0068:         predictions = Tensor([[1.0, 0.0]], requires_grad=True)
0069:         targets = Tensor([[1.0, 0.0]])
0070:         
0071:         loss = CrossEntropyLoss.apply(predictions, targets)
0072:         loss.backward()
0073:         
0074:         assert predictions.grad is not None
0075:         assert not np.isnan(predictions.grad).any()
0076:         assert not np.isinf(predictions.grad).any()
0077: 
0078: class TestBinaryCrossEntropyLoss:
0079:     """Tests for Binary Cross Entropy Loss"""
0080:     
0081:     def test_forward(self):
0082:         """Test forward pass of binary cross entropy loss"""
0083:         predictions = Tensor([0.7, 0.3])
0084:         targets = Tensor([1.0, 0.0])
0085:         
0086:         loss = BinaryCrossEntropyLoss.apply(predictions, targets)
0087:         assert loss.data >= 0  # Loss should be non-negative
0088:         
0089:     def test_gradient(self):
0090:         """Test gradient computation"""
0091:         predictions = Tensor([0.7], requires_grad=True)
0092:         targets = Tensor([1.0])
0093:         
0094:         loss = BinaryCrossEntropyLoss.apply(predictions, targets)
0095:         loss.backward()
0096:         
0097:         assert predictions.grad is not None
0098:         assert not np.isnan(predictions.grad).any()
0099:         
0100:     def test_reductions(self):
0101:         """Test different reduction methods"""
0102:         predictions = Tensor([[0.7, 0.3], [0.2, 0.8]])
0103:         targets = Tensor([[1.0, 0.0], [0.0, 1.0]])
0104:         
0105:         loss_none = BinaryCrossEntropyLoss.apply(predictions, targets, 'none')
0106:         loss_mean = BinaryCrossEntropyLoss.apply(predictions, targets, 'mean')
0107:         loss_sum = BinaryCrossEntropyLoss.apply(predictions, targets, 'sum')
0108:         
0109:         assert loss_none.shape == predictions.shape
0110:         # Check if scalar by ensuring it's a 0-dimensional array or float
0111:         assert loss_mean.data.ndim == 0 or isinstance(loss_mean.data, float)
0112:         assert loss_sum.data.ndim == 0 or isinstance(loss_sum.data, float)
0113: 
0114: class TestL1Loss:
0115:     """Tests for L1 Loss"""
0116:     
0117:     def test_forward(self):
0118:         """Test forward pass of L1 loss"""
0119:         predictions = Tensor([[1.0, 2.0], [3.0, 4.0]])
0120:         targets = Tensor([[2.0, 1.0], [4.0, 3.0]])
0121:         
0122:         loss = L1Loss.apply(predictions, targets)
0123:         expected = np.mean(np.abs(predictions.data - targets.data))
0124:         assert np.allclose(loss.data, expected)
0125:         
0126:     def test_backward(self):
0127:         """Test backward pass of L1 loss"""
0128:         predictions = Tensor([1.0], requires_grad=True)
0129:         targets = Tensor([2.0])
0130:         
0131:         loss = L1Loss.apply(predictions, targets)
0132:         loss.backward()
0133:         
0134:         # Gradient should be sign(pred - target)
0135:         expected_grad = np.sign(predictions.data - targets.data)
0136:         assert np.allclose(predictions.grad, expected_grad)
0137: 
0138: class TestHuberLoss:
0139:     """Tests for Huber Loss"""
0140:     
0141:     def test_forward(self):
0142:         """Test forward pass of Huber loss"""
0143:         predictions = Tensor([1.0, 2.0])
0144:         targets = Tensor([0.0, 4.0])
0145:         delta = 1.0
0146:         
0147:         loss = HuberLoss.apply(predictions, targets, delta)
0148:         
0149:         # Manually calculate expected loss
0150:         diff = predictions.data - targets.data
0151:         expected = np.mean(np.where(np.abs(diff) <= delta,
0152:                                   0.5 * diff ** 2,
0153:                                   delta * np.abs(diff) - 0.5 * delta ** 2))
0154:         
0155:         assert np.allclose(loss.data, expected)
0156:         
0157:     def test_backward(self):
0158:         """Test backward pass of Huber loss"""
0159:         predictions = Tensor([0.0], requires_grad=True)
0160:         targets = Tensor([2.0])
0161:         delta = 1.0
0162:         
0163:         loss = HuberLoss.apply(predictions, targets, delta)
0164:         loss.backward()
0165:         
0166:         assert predictions.grad is not None
0167:         assert not np.isnan(predictions.grad).any()
0168: 
0169: class TestKLDivLoss:
0170:     """Tests for KL Divergence Loss"""
0171:     
0172:     def test_forward(self):
0173:         """Test forward pass of KL divergence loss"""
0174:         predictions = Tensor([[0.5, 0.5]])
0175:         targets = Tensor([[0.8, 0.2]])
0176:         
0177:         loss = KLDivLoss.apply(predictions, targets)
0178:         assert loss.data >= 0  # KL divergence is always non-negative
0179:         
0180:     def test_numerical_stability(self):
0181:         """Test numerical stability with small probabilities"""
0182:         predictions = Tensor([[0.999, 0.001]])
0183:         targets = Tensor([[0.001, 0.999]])
0184:         
0185:         loss = KLDivLoss.apply(predictions, targets)
0186:         assert not np.isnan(loss.data)
0187:         assert not np.isinf(loss.data)
0188:     
0189:     def test_kldiv_loss_log_target(self):
0190:         """Test KL divergence loss with log target"""
0191:         predictions = Tensor([[-1.0, -2.0]], requires_grad=True)
0192:         targets = Tensor([[-1.0, -1.0]])  # log probabilities
0193:         
0194:         loss = KLDivLoss.apply(predictions, targets, log_target=True)
0195:         loss.backward()
0196:         
0197:         assert predictions.grad is not None
0198:         assert not np.isnan(loss.data)
0199: 
0200: class TestCosineSimilarityLoss:
0201:     """Tests for Cosine Similarity Loss"""
0202:     
0203:     def test_forward(self):
0204:         """Test forward pass of cosine similarity loss"""
0205:         x1 = Tensor([[1.0, 0.0]])
0206:         x2 = Tensor([[0.0, 1.0]])
0207:         
0208:         loss = CosineSimilarityLoss.apply(x1, x2)
0209:         # Orthogonal vectors should have cos_sim = 0, so loss = 1 - 0 = 1
0210:         assert np.allclose(loss.data, 1.0), f"Expected 1.0, got {loss.data}"
0211:         
0212:     def test_identical_vectors(self):
0213:         """Test with identical vectors"""
0214:         x = Tensor([[1.0, 1.0]])
0215:         loss = CosineSimilarityLoss.apply(x, x)
0216:         # For identical vectors, cosine similarity is 1, so loss = 1 - 1 = 0
0217:         assert np.allclose(loss.data, 0.0, atol=1e-7)
0218:         
0219:     def test_numerical_stability(self):
0220:         """Test numerical stability with zero vectors"""
0221:         x1 = Tensor([[0.0, 0.0]])
0222:         x2 = Tensor([[1.0, 1.0]])
0223:         
0224:         loss = CosineSimilarityLoss.apply(x1, x2)
0225:         assert not np.isnan(loss.data)
0226:     
0227:     def test_cosine_similarity_zero_input(self):
0228:         """Test cosine similarity with zero vectors"""
0229:         x1 = Tensor([[0.0, 0.0]], requires_grad=True)
0230:         x2 = Tensor([[1.0, 1.0]], requires_grad=True)
0231:         
0232:         loss = CosineSimilarityLoss.apply(x1, x2, eps=1e-8)
0233:         loss.backward()
0234:         
0235:         assert not np.isnan(loss.data)
0236:         assert not np.isnan(x1.grad).any()
0237:         assert not np.isnan(x2.grad).any()
0238: 
0239: class TestHingeLoss:
0240:     """Tests for Hinge Loss"""
0241:     
0242:     def test_forward(self):
0243:         """Test forward pass of hinge loss"""
0244:         predictions = Tensor([0.5, -0.5])
0245:         targets = Tensor([1.0, 0.0])
0246:         
0247:         loss = HingeLoss.apply(predictions, targets)
0248:         assert loss.data >= 0  # Hinge loss is non-negative
0249:         
0250:     def test_perfect_prediction(self):
0251:         """Test with perfect predictions"""
0252:         predictions = Tensor([1.0])
0253:         targets = Tensor([1.0])
0254:         
0255:         loss = HingeLoss.apply(predictions, targets)
0256:         assert np.allclose(loss.data, 0.0)  # Loss should be zero
0257:         
0258:     def test_margin(self):
0259:         """Test different margin values"""
0260:         predictions = Tensor([0.5])
0261:         targets = Tensor([1.0])
0262:         
0263:         loss1 = HingeLoss.apply(predictions, targets, margin=1.0)
0264:         loss2 = HingeLoss.apply(predictions, targets, margin=2.0)
0265:         assert loss2.data > loss1.data  # Larger margin should give larger loss
0266: 
0267: class TestFocalLoss:
0268:     """Tests for Focal Loss"""
0269:     
0270:     def test_forward(self):
0271:         """Test forward pass of focal loss"""
0272:         predictions = Tensor([0.7, 0.3])
0273:         targets = Tensor([1.0, 0.0])
0274:         
0275:         loss = FocalLoss.apply(predictions, targets)
0276:         assert loss.data >= 0  # Focal loss is non-negative
0277:         
0278:     def test_gamma_effect(self):
0279:         """Test effect of gamma parameter"""
0280:         predictions = Tensor([0.7])
0281:         targets = Tensor([1.0])
0282:         
0283:         loss1 = FocalLoss.apply(predictions, targets, gamma=0.0)  # Equivalent to BCE
0284:         loss2 = FocalLoss.apply(predictions, targets, gamma=2.0)  # Standard focal loss
0285:         
0286:         # Focal loss should be smaller than BCE for easy examples
0287:         assert loss2.data < loss1.data
0288:         
0289:     def test_numerical_stability(self):
0290:         """Test numerical stability with extreme probabilities"""
0291:         predictions = Tensor([0.999, 0.001])
0292:         targets = Tensor([1.0, 0.0])
0293:         
0294:         loss = FocalLoss.apply(predictions, targets)
0295:         assert not np.isnan(loss.data)
0296:         assert not np.isinf(loss.data)
0297:     
0298:     def test_focal_loss_extreme_probabilities(self):
0299:         """Test focal loss with extreme probability values"""
0300:         predictions = Tensor([[0.999, 0.001]], requires_grad=True)
0301:         targets = Tensor([[1.0, 0.0]])
0302:         
0303:         loss = FocalLoss.apply(predictions, targets, gamma=2.0)
0304:         loss.backward()
0305:         
0306:         assert not np.isnan(loss.data)
0307:         assert not np.isnan(predictions.grad).any()
0308: 
0309: class TestEdgeCases:
0310:     """Tests for edge cases and error conditions"""
0311:     
0312:     def test_shape_mismatch(self):
0313:         """Test shape mismatch handling"""
0314:         predictions = Tensor([[1.0, 2.0]])
0315:         targets = Tensor([1.0])
0316:         
0317:         with pytest.raises(ValueError):
0318:             MSELoss.apply(predictions, targets)
0319:             
0320:     def test_invalid_reduction(self):
0321:         """Test invalid reduction method"""
0322:         predictions = Tensor([1.0])
0323:         targets = Tensor([1.0])
0324:         
0325:         with pytest.raises(ValueError):
0326:             MSELoss.apply(predictions, targets, reduction='invalid')
0327:             
0328:     def test_negative_probabilities(self):
0329:         """Test handling of negative probabilities"""
0330:         predictions = Tensor([-0.1, 1.1])
0331:         targets = Tensor([0.0, 1.0])
0332:         
0333:         with pytest.raises(ValueError):
0334:             BinaryCrossEntropyLoss.apply(predictions, targets)

// File: C:\Users\aluja\Desktop\DLpy\tests\test_modules.py
// ----------------------------------------
0001: import pytest
0002: from DLpy.nn.modules import Module
0003: from DLpy.core import Tensor
0004: import numpy as np
0005: from DLpy.nn.linear import Linear  
0006: 
0007: class TestModuleEdgeCases:
0008:     """Tests for edge cases in Module functionality"""
0009:     
0010:     def test_premature_parameter_registration(self):
0011:         """Test parameter registration before initialization"""
0012:         with pytest.raises(TypeError):
0013:             class BadModule(Module):
0014:                 def __init__(self):
0015:                     self.param = Tensor([1.0])  # Before super().__init__()
0016:             BadModule()
0017: 
0018:     def test_invalid_module_addition(self):
0019:         """Test adding invalid modules"""
0020:         module = Module()
0021:         
0022:         # Test adding None module
0023:         module.add_module('none_module', None)
0024:         assert module._modules['none_module'] is None
0025:         
0026:         # Test adding invalid type
0027:         with pytest.raises(TypeError):
0028:             module.add_module('invalid', "not a module")
0029:             
0030:         # Test adding before initialization
0031:         with pytest.raises(TypeError):
0032:             class BadModule(Module):
0033:                 def __init__(self):
0034:                     self.add_module('test', Module())  # Before super().__init__()
0035:             BadModule()
0036: 
0037:     def test_attribute_access(self):
0038:         """Test attribute access edge cases"""
0039:         # Test accessing non-existent attribute
0040:         module = Module()
0041:         with pytest.raises(AttributeError):
0042:             _ = module.nonexistent_attr
0043:         
0044:         # Test accessing training attribute before initialization
0045:         class BadModule(Module):
0046:             def __init__(self):
0047:                 # Access training before super().__init__()
0048:                 try:
0049:                     _ = self._parameters
0050:                 except AttributeError:
0051:                     pass  # Expected
0052:                     
0053:                 # Now try to get the training attribute which should fail
0054:                 _ = self.training
0055:                 super().__init__()
0056:                 
0057:         with pytest.raises(AttributeError):
0058:             BadModule()
0059: 
0060:     def test_module_buffer_operations(self):
0061:         """Test buffer operations in detail"""
0062:         class TestModule(Module):
0063:             def __init__(self):
0064:                 super().__init__()
0065:                 self.register_buffer('running_mean', Tensor([0.0]))
0066:                 self.register_buffer('running_var', None)
0067:                 
0068:         module = TestModule()
0069:         assert 'running_mean' in module._buffers
0070:         assert module._buffers['running_var'] is None
0071:         
0072:         # Test buffer replacement
0073:         module.register_buffer('running_mean', Tensor([1.0]))
0074:         assert np.array_equal(module._buffers['running_mean'].data, [1.0])
0075: 
0076:     def test_module_state_dict(self):
0077:         """Test state dict functionality"""
0078:         class ComplexModule(Module):
0079:             def __init__(self):
0080:                 super().__init__()
0081:                 self.linear = Linear(2, 2)
0082:                 self.register_buffer('running_stats', Tensor([0.0]))
0083:                 
0084:         module = ComplexModule()
0085:         # Test parameter access
0086:         params = dict(module.named_parameters())
0087:         assert 'linear.weight' in params
0088:         assert 'linear.bias' in params
0089: 
0090:     def test_nested_module_training(self):
0091:         """Test training mode propagation in nested modules"""
0092:         class NestedModule(Module):
0093:             def __init__(self):
0094:                 super().__init__()
0095:                 self.sub1 = Linear(2, 2)
0096:                 self.sub2 = Linear(2, 2)
0097:                 
0098:         module = NestedModule()
0099:         module.train(False)
0100:         assert not module.training
0101:         assert not module.sub1.training
0102:         assert not module.sub2.training

// File: C:\Users\aluja\Desktop\DLpy\tests\test_nn.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: from DLpy.nn.linear import Linear
0005: from DLpy.nn.modules import Module
0006: 
0007: 
0008: class TestLinearLayer:
0009:     """Test suite for the Linear layer implementation."""
0010:     
0011:     def test_linear_layer_creation(self):
0012:         """Test that linear layers are created with correct shapes and initialization."""
0013:         in_features, out_features = 5, 3
0014:         layer = Linear(in_features, out_features)
0015:         
0016:         # Test weight dimensions
0017:         assert layer.weight.shape == (in_features, out_features)
0018:         assert layer.weight.requires_grad
0019:         
0020:         # Test bias dimensions
0021:         assert layer.bias is not None
0022:         assert layer.bias.shape == (out_features,)
0023:         assert layer.bias.requires_grad
0024:         
0025:         # Test layer without bias
0026:         layer_no_bias = Linear(in_features, out_features, bias=False)
0027:         assert layer_no_bias.bias is None
0028:         
0029:     def test_linear_forward(self):
0030:         """Test the forward pass of the linear layer."""
0031:         # Create a simple linear layer with known weights for testing
0032:         layer = Linear(2, 3)
0033:         layer.weight.data = np.array([[1., 2., 3.], [4., 5., 6.]])
0034:         layer.bias.data = np.array([0.1, 0.2, 0.3])
0035:         
0036:         # Create input tensor
0037:         x = Tensor([[1., 2.]])  # Batch size 1, 2 features
0038:         
0039:         # Compute expected output manually
0040:         expected_output = np.array([[9.1, 12.2, 15.3]])  # (1×2) @ (2×3) + bias
0041:         
0042:         # Get actual output
0043:         output = layer(x)
0044:         
0045:         # Compare results
0046:         assert isinstance(output, Tensor)
0047:         assert output.shape == (1, 3)
0048:         assert np.allclose(output.data, expected_output)
0049:         
0050:     def test_linear_backward(self):
0051:         """Test the backward pass and gradient computation of the linear layer."""
0052:         # Create a layer with specific weights for testing
0053:         layer = Linear(2, 1)
0054:         layer.weight.data = np.array([[1.], [2.]])
0055:         layer.bias.data = np.array([0.])
0056:         
0057:         # Forward pass
0058:         x = Tensor([[1., 2.]], requires_grad=True)
0059:         output = layer(x)
0060:         
0061:         # Backward pass
0062:         output.backward(np.array([[1.]]))
0063:         
0064:         # Check input gradients
0065:         expected_input_grad = np.array([[1., 2.]])  # Gradient w.r.t input
0066:         assert np.allclose(x.grad, expected_input_grad)
0067:         
0068:         # Check weight gradients
0069:         expected_weight_grad = np.array([[1.], [2.]])  # Gradient w.r.t weights
0070:         assert np.allclose(layer.weight.grad, expected_weight_grad)
0071:         
0072:         # Check bias gradients
0073:         expected_bias_grad = np.array([1.])  # Gradient w.r.t bias
0074:         assert np.allclose(layer.bias.grad, expected_bias_grad)
0075:         
0076:     def test_linear_batch_processing(self):
0077:         """Test that the linear layer correctly handles batched inputs."""
0078:         layer = Linear(3, 2)
0079:         batch_size = 4
0080:         x = Tensor(np.random.randn(batch_size, 3))
0081:         
0082:         output = layer(x)
0083:         assert output.shape == (batch_size, 2)
0084:         
0085:     def test_weight_initialization(self):
0086:         """Test that weights are properly initialized using He initialization."""
0087:         in_features, out_features = 100, 100
0088:         layer = Linear(in_features, out_features)
0089:         
0090:         # Check if weights follow He initialization statistics
0091:         weights = layer.weight.data
0092:         mean = np.mean(weights)
0093:         std = np.std(weights)
0094:         
0095:         # He initialization should have mean ≈ 0 and std ≈ sqrt(2/in_features)
0096:         expected_std = np.sqrt(2.0 / in_features)
0097:         assert abs(mean) < 0.1  # Mean should be close to 0
0098:         assert abs(std - expected_std) < 0.1  # Std should be close to expected
0099: 
0100: 
0101: class TestModule:
0102:     """Test suite for the base Module class."""
0103:     
0104:     class SimpleModule(Module):
0105:         """A simple module for testing purposes."""
0106:         def __init__(self):
0107:             super().__init__()
0108:             self.linear1 = Linear(2, 3)
0109:             self.linear2 = Linear(3, 1)
0110:             self.register_buffer('running_mean', Tensor(np.zeros(3)))
0111:             
0112:         def forward(self, x):
0113:             x = self.linear1(x)
0114:             return self.linear2(x)
0115:     
0116:     def test_module_parameter_registration(self):
0117:         """Test that parameters are correctly registered and tracked."""
0118:         model = self.SimpleModule()
0119:         
0120:         # Count parameters
0121:         params = list(model.parameters())
0122:         assert len(params) == 4  # 2 weights + 2 biases
0123:         
0124:         # Check named parameters
0125:         named_params = dict(model.named_parameters())
0126:         assert 'linear1.weight' in named_params
0127:         assert 'linear1.bias' in named_params
0128:         assert 'linear2.weight' in named_params
0129:         assert 'linear2.bias' in named_params
0130:         
0131:     def test_module_buffer_registration(self):
0132:         """Test that buffers are correctly registered."""
0133:         model = self.SimpleModule()
0134:         assert 'running_mean' in model._buffers
0135:         assert isinstance(model._buffers['running_mean'], Tensor)
0136:         
0137:     def test_module_train_eval_modes(self):
0138:         """Test switching between training and evaluation modes."""
0139:         model = self.SimpleModule()
0140:         
0141:         # Test train mode
0142:         model.train()
0143:         assert model.training
0144:         assert model.linear1.training
0145:         assert model.linear2.training
0146:         
0147:         # Test eval mode
0148:         model.eval()
0149:         assert not model.training
0150:         assert not model.linear1.training
0151:         assert not model.linear2.training
0152:         
0153:     def test_module_repr(self):
0154:         """Test the string representation of modules."""
0155:         model = self.SimpleModule()
0156:         repr_str = repr(model)
0157:         
0158:         # Check that repr includes important information
0159:         assert 'SimpleModule' in repr_str
0160:         assert 'linear1' in repr_str
0161:         assert 'linear2' in repr_str
0162: 
0163: 
0164: class TestEndToEnd:
0165:     """End-to-end tests for neural network components."""
0166:     
0167:     def test_simple_network(self):
0168:         """Test a simple network with multiple layers."""
0169:         # Create a simple network
0170:         class SimpleNet(Module):
0171:             def __init__(self):
0172:                 super().__init__()
0173:                 self.linear1 = Linear(2, 3)
0174:                 self.linear2 = Linear(3, 1)
0175:                 
0176:             def forward(self, x):
0177:                 x = self.linear1(x)
0178:                 return self.linear2(x)
0179:         
0180:         # Create model and input
0181:         model = SimpleNet()
0182:         x = Tensor([[1., 2.]], requires_grad=True)
0183:         
0184:         # Forward pass
0185:         output = model(x)
0186:         assert output.shape == (1, 1)
0187:         
0188:         # Backward pass
0189:         output.backward(np.array([[1.]]))
0190:         
0191:         # Check that all parameters have gradients
0192:         for param in model.parameters():
0193:             assert param.grad is not None

// File: C:\Users\aluja\Desktop\DLpy\tests\test_ops.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: from DLpy.ops import (
0005:     Add, Multiply, Power, Divide, Log, Exp, Sum, Mean, Max, Transpose,
0006:     Greater, GreaterEqual, Less, LessEqual, Equal, NotEqual
0007: )
0008: 
0009: class TestBasicOps:
0010:     """Tests for basic arithmetic operations"""
0011:     
0012:     def test_add_edge_cases(self):
0013:         """Test edge cases for Add operation"""
0014:         # Test broadcasting
0015:         x = Tensor([[1.0]], requires_grad=True)
0016:         y = Tensor([1.0, 2.0], requires_grad=True)
0017:         
0018:         with pytest.raises(ValueError):
0019:             _ = Add.apply(x, y)
0020:         
0021:         # Test gradient accumulation
0022:         x = Tensor([1.0], requires_grad=True)
0023:         y = Tensor([2.0], requires_grad=True)
0024:         z = Add.apply(x, y)
0025:         z.backward()
0026:         
0027:         assert np.array_equal(x.grad, [1.0])
0028:         assert np.array_equal(y.grad, [1.0])
0029: 
0030:     def test_multiply_edge_cases(self):
0031:         """Test edge cases for Multiply operation"""
0032:         # Test scalar multiplication
0033:         x = Tensor([1.0], requires_grad=True)
0034:         y = Tensor(2.0, requires_grad=True)
0035:         z = Multiply.apply(x, y)
0036:         z.backward()
0037:         
0038:         assert np.array_equal(x.grad, [2.0])
0039:         assert np.array_equal(y.grad, [1.0])
0040:     
0041:     def test_add_broadcasting_complex(self):
0042:         """Test complex broadcasting scenarios in Add operation"""
0043:         # Test broadcasting with different dimensions
0044:         x = Tensor([[1.0]], requires_grad=True)
0045:         y = Tensor([1.0, 2.0, 3.0], requires_grad=True)
0046:         with pytest.raises(ValueError):
0047:             _ = x + y  # Incompatible shapes
0048:             
0049:         # Test broadcasting with scalar
0050:         x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
0051:         y = Tensor(2.0, requires_grad=True)
0052:         z = x + y
0053:         z.backward(np.ones_like(x.data))
0054:         assert np.sum(y.grad) == np.prod(x.shape)  # Sum of gradients equals number of elements
0055: 
0056:     def test_multiply_broadcasting_complex(self):
0057:         """Test complex broadcasting scenarios in Multiply operation"""
0058:         # Test scalar multiplication with matrix
0059:         x = Tensor(2.0, requires_grad=True)
0060:         y = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
0061:         z = x * y
0062:         z.backward(np.ones_like(y.data))
0063: 
0064:         # Check scalar gradient - should be sum of all elements in y
0065:         assert x.grad.shape == (1,)
0066:         assert np.allclose(x.grad, [10.0])  # sum([1,2,3,4])
0067: 
0068:         # Check matrix gradient - should be uniformly scaled by x
0069:         assert y.grad.shape == y.data.shape
0070:         assert np.allclose(y.grad, np.full_like(y.data, 2.0))
0071: 
0072:         # Test broadcasting matrix with different shapes
0073:         a = Tensor([[1.0], [2.0]], requires_grad=True)  # Shape: (2,1)
0074:         b = Tensor([[1.0, 2.0]], requires_grad=True)    # Shape: (1,2)
0075:         c = a * b  # Should broadcast to shape (2,2)
0076: 
0077:         assert c.data.shape == (2, 2)
0078:         expected = np.array([[1.0, 2.0], [2.0, 4.0]])
0079:         assert np.allclose(c.data, expected)
0080: 
0081:         # Test gradient propagation with broadcasting
0082:         c.backward(np.ones_like(c.data))
0083:         assert a.grad.shape == (2, 1)
0084:         assert b.grad.shape == (1, 2)
0085:         # Correct expected gradients
0086:         assert np.allclose(a.grad, np.array([[3.0], [3.0]]))  # Correct sum of gradients for each row
0087:         assert np.allclose(b.grad, np.array([[3.0, 3.0]]))    # Correct sum of gradients for each column
0088: 
0089:     def test_add_empty_tensors(self):
0090:         x = Tensor([], requires_grad=True)
0091:         y = Tensor([], requires_grad=True)
0092:         z = Add.apply(x, y)
0093:         assert z.shape == (0,)
0094:     
0095:     def test_multiply_empty_tensors(self):
0096:         x = Tensor([], requires_grad=True)
0097:         y = Tensor([], requires_grad=True)
0098:         z = Multiply.apply(x, y)
0099:         assert z.shape == (0,)
0100:     
0101:     def test_add_broadcasting_edge_cases(self):
0102:         """Test edge cases of broadcasting in Add operation"""
0103:         # Test scalar + matrix
0104:         scalar = Tensor(2.0, requires_grad=True)
0105:         matrix = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
0106:         result = Add.apply(scalar, matrix)
0107:         result.backward(np.ones_like(matrix.data))
0108:         
0109:         assert result.shape == matrix.shape
0110:         assert scalar.grad == 4.0  # Sum of gradients
0111:         assert np.all(matrix.grad == 1.0)
0112:         
0113:         # Test incompatible shapes
0114:         with pytest.raises(ValueError):
0115:             Add.apply(Tensor([1.0, 2.0]), Tensor([[1.0]]))
0116: 
0117:     def test_multiply_zero_gradient(self):
0118:         """Test multiply operation with zero gradient"""
0119:         x = Tensor([1.0, 2.0], requires_grad=True)
0120:         y = Tensor([3.0, 4.0], requires_grad=True)
0121:         z = Multiply.apply(x, y)
0122:         z.backward(np.zeros_like(z.data))
0123:         
0124:         assert np.all(x.grad == 0)
0125:         assert np.all(y.grad == 0)
0126: 
0127: class TestPowerOperations:
0128:     """Tests for power and division operations"""
0129:     
0130:     def test_power_scalar(self):
0131:         """Test power operation with scalar exponent"""
0132:         x = Tensor([2.0, 3.0], requires_grad=True)
0133:         y = x ** 2
0134:         y.backward(np.array([1.0, 1.0]))
0135:         
0136:         assert np.allclose(y.data, [4.0, 9.0])
0137:         assert np.allclose(x.grad, [4.0, 6.0])  # d/dx(x^2) = 2x
0138:         
0139:     def test_power_negative(self):
0140:         """Test power operation with negative exponent"""
0141:         x = Tensor([2.0, 4.0], requires_grad=True)
0142:         y = x ** (-1)
0143:         y.backward(np.array([1.0, 1.0]))
0144:         
0145:         assert np.allclose(y.data, [0.5, 0.25])
0146:         assert np.allclose(x.grad, [-0.25, -0.0625])  # d/dx(x^-1) = -x^-2
0147:         
0148:     def test_division(self):
0149:         """Test division operation"""
0150:         x = Tensor([6.0, 8.0], requires_grad=True)
0151:         y = Tensor([2.0, 4.0], requires_grad=True)
0152:         z = x / y
0153:         z.backward(np.array([1.0, 1.0]))
0154:         
0155:         assert np.allclose(z.data, [3.0, 2.0])
0156:         assert np.allclose(x.grad, [0.5, 0.25])  # d/dx(x/y) = 1/y
0157:         assert np.allclose(y.grad, [-1.5, -0.5])  # d/dy(x/y) = -x/y^2
0158:         
0159:     def test_division_by_zero(self):
0160:         """Test division by zero raises error"""
0161:         x = Tensor([1.0, 2.0])
0162:         y = Tensor([1.0, 0.0])
0163:         with pytest.raises(ValueError):
0164:             _ = x / y
0165: 
0166: class TestElementwiseOperations:
0167:     """Tests for element-wise operations"""
0168:     
0169:     def test_log(self):
0170:         """Test natural logarithm"""
0171:         x = Tensor([1.0, np.e], requires_grad=True)
0172:         y = x.log()
0173:         y.backward(np.array([1.0, 1.0]))
0174:         
0175:         assert np.allclose(y.data, [0.0, 1.0])
0176:         assert np.allclose(x.grad, [1.0, 1/np.e])  # d/dx(log(x)) = 1/x
0177:         
0178:     def test_exp(self):
0179:         """Test exponential function"""
0180:         x = Tensor([0.0, 1.0], requires_grad=True)
0181:         y = x.exp()
0182:         y.backward(np.array([1.0, 1.0]))
0183:         
0184:         assert np.allclose(y.data, [1.0, np.e])
0185:         assert np.allclose(y.data, x.grad)  # d/dx(exp(x)) = exp(x)
0186: 
0187: class TestReductionOperations:
0188:     """Tests for reduction operations"""
0189:     
0190:     def test_sum(self):
0191:         """Test sum reduction"""
0192:         x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
0193:         
0194:         # Test sum all elements
0195:         y1 = x.sum()
0196:         y1.backward()
0197:         assert np.allclose(y1.data, 10.0)
0198:         assert np.allclose(x.grad, np.ones_like(x.data))
0199:         
0200:         # Reset gradients
0201:         x.grad = None
0202:         
0203:         # Test sum along axis
0204:         y2 = x.sum(axis=0)
0205:         y2.backward(np.array([1.0, 1.0]))
0206:         assert np.allclose(y2.data, [4.0, 6.0])
0207:         assert np.allclose(x.grad, np.ones_like(x.data))
0208:         
0209:     def test_mean(self):
0210:         """Test mean reduction"""
0211:         x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
0212:         y = x.mean()
0213:         y.backward()
0214:         
0215:         assert np.allclose(y.data, 2.5)
0216:         assert np.allclose(x.grad, np.full_like(x.data, 0.25))  # 1/n for each element
0217:         
0218:     def test_max(self):
0219:         """Test max reduction"""
0220:         x = Tensor([[1.0, 4.0], [3.0, 2.0]], requires_grad=True)
0221:         y = x.max()
0222:         y.backward()
0223:         
0224:         assert np.allclose(y.data, 4.0)
0225:         expected_grad = np.array([[0.0, 1.0], [0.0, 0.0]])
0226:         assert np.allclose(x.grad, expected_grad)
0227: 
0228: class TestMatrixOperations:
0229:     """Tests for matrix operations"""
0230:     
0231:     def test_transpose(self):
0232:         """Test matrix transpose"""
0233:         x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
0234:         y = x.t()
0235:         y.backward(np.ones_like(y.data))
0236:         
0237:         assert np.allclose(y.data, [[1.0, 3.0], [2.0, 4.0]])
0238:         assert np.allclose(x.grad, np.ones_like(x.data))
0239:         
0240:     def test_transpose_3d(self):
0241:         """Test 3D tensor transpose"""
0242:         x = Tensor(np.arange(8).reshape(2, 2, 2), requires_grad=True)
0243:         y = x.transpose(1, 2, 0)
0244:         y.backward(np.ones_like(y.data))
0245:         
0246:         assert y.data.shape == (2, 2, 2)
0247:         assert np.allclose(x.grad, np.ones_like(x.data))
0248: 
0249: class TestComparisonOperations:
0250:     """Tests for comparison operations"""
0251:     
0252:     def test_greater(self):
0253:         """Test greater than operation"""
0254:         x = Tensor([1.0, 2.0, 3.0])
0255:         y = Tensor([2.0, 2.0, 2.0])
0256:         result = x > y
0257:         assert np.allclose(result.data, [False, False, True])
0258:         
0259:     def test_less_equal(self):
0260:         """Test less than or equal operation"""
0261:         x = Tensor([1.0, 2.0, 3.0])
0262:         y = Tensor([2.0, 2.0, 2.0])
0263:         result = x <= y
0264:         assert np.allclose(result.data, [True, True, False])
0265:         
0266:     def test_equal(self):
0267:         """Test equality operation"""
0268:         x = Tensor([1.0, 2.0, 3.0])
0269:         y = Tensor([1.0, 2.0, 2.0])
0270:         result = x == y
0271:         assert np.allclose(result.data, [True, True, False])
0272:         
0273:     def test_not_equal(self):
0274:         """Test inequality operation"""
0275:         x = Tensor([1.0, 2.0, 3.0])
0276:         y = Tensor([1.0, 2.0, 2.0])
0277:         result = x != y
0278:         assert np.allclose(result.data, [False, False, True])
0279: 
0280: class TestEdgeCases:
0281:     """Tests for edge cases and error conditions"""
0282:     
0283:     def test_log_negative(self):
0284:         """Test log of negative number raises error"""
0285:         x = Tensor([-1.0])
0286:         with pytest.raises(ValueError):
0287:             _ = x.log()
0288:             
0289:     def test_power_non_scalar(self):
0290:         """Test power with non-scalar exponent raises error"""
0291:         x = Tensor([2.0])
0292:         y = Tensor([1.0, 2.0])
0293:         with pytest.raises(ValueError):
0294:             _ = x ** y
0295:             
0296:     def test_reduction_keepdims(self):
0297:         """Test reduction operations with keepdims=True"""
0298:         x = Tensor([[1.0, 2.0], [3.0, 4.0]])
0299:         y = x.sum(axis=0, keepdims=True)
0300:         assert y.shape == (1, 2)
0301:         
0302:     def test_broadcasting_division(self):
0303:         """Test division with broadcasting"""
0304:         x = Tensor([[1.0, 2.0], [3.0, 4.0]])
0305:         y = Tensor([2.0])
0306:         z = x / y
0307:         assert z.shape == x.shape
0308:         assert np.allclose(z.data, [[0.5, 1.0], [1.5, 2.0]])

// File: C:\Users\aluja\Desktop\DLpy\tests\test_optim.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: from DLpy.optim import SGD, Adam, RMSprop, AdaGrad, AdaDelta, AdaMax
0005: 
0006: class TestOptimizers:
0007:     """Base test class for all optimizers."""
0008:     
0009:     def setup_method(self):
0010:         """Setup method run before each test."""
0011:         # Create a simple parameter tensor
0012:         self.param = Tensor([1.0, 2.0, 3.0], requires_grad=True)
0013:         self.grad = np.array([0.1, 0.2, 0.3])
0014:         
0015:     def _test_basic_update(self, optimizer_class, **kwargs):
0016:         """Helper method to test basic parameter updates."""
0017:         optimizer = optimizer_class([self.param], **kwargs)
0018:         
0019:         # Initial parameter values
0020:         initial_params = self.param.data.copy()
0021:         
0022:         # Set gradient and perform optimization step
0023:         self.param.grad = self.grad
0024:         optimizer.step()
0025:         
0026:         # Check that parameters were updated
0027:         assert not np.array_equal(self.param.data, initial_params)
0028:         
0029:     def _test_zero_grad(self, optimizer_class, **kwargs):
0030:         """Helper method to test zero_grad functionality."""
0031:         optimizer = optimizer_class([self.param], **kwargs)
0032:         
0033:         # Set some gradient
0034:         self.param.grad = self.grad
0035:         
0036:         # Zero out gradients
0037:         optimizer.zero_grad()
0038:         
0039:         # Check that gradients are zeroed
0040:         assert np.all(self.param.grad == 0)
0041: 
0042: class TestSGD(TestOptimizers):
0043:     """Tests for SGD optimizer."""
0044:     
0045:     def test_basic_sgd(self):
0046:         """Test basic SGD functionality."""
0047:         self._test_basic_update(SGD, lr=0.1)
0048:         
0049:     def test_sgd_momentum(self):
0050:         """Test SGD with momentum."""
0051:         optimizer = SGD([self.param], lr=0.1, momentum=0.9)
0052:         
0053:         # First update
0054:         self.param.grad = self.grad
0055:         optimizer.step()
0056:         first_update = self.param.data.copy()
0057:         
0058:         # Second update with same gradient
0059:         optimizer.step()
0060:         
0061:         # With momentum, second update should be larger
0062:         first_step = np.abs(first_update - np.array([1.0, 2.0, 3.0]))
0063:         second_step = np.abs(self.param.data - first_update)
0064:         assert np.all(second_step > first_step)
0065:         
0066:     def test_sgd_nesterov(self):
0067:         """Test SGD with Nesterov momentum."""
0068:         self._test_basic_update(SGD, lr=0.1, momentum=0.9, nesterov=True)
0069:         
0070:     def test_sgd_weight_decay(self):
0071:         """Test SGD with weight decay."""
0072:         optimizer = SGD([self.param], lr=0.1, weight_decay=0.1)
0073:         self.param.grad = self.grad
0074:         optimizer.step()
0075:         
0076:         # Parameters should decrease more with weight decay
0077:         no_decay_param = Tensor([1.0, 2.0, 3.0], requires_grad=True)
0078:         no_decay_optimizer = SGD([no_decay_param], lr=0.1)
0079:         no_decay_param.grad = self.grad
0080:         no_decay_optimizer.step()
0081:         
0082:         assert np.all(np.abs(self.param.data) < np.abs(no_decay_param.data))
0083: 
0084: class TestAdam(TestOptimizers):
0085:     """Tests for Adam optimizer."""
0086:     
0087:     def test_basic_adam(self):
0088:         """Test basic Adam functionality."""
0089:         self._test_basic_update(Adam, lr=0.001)
0090:         
0091:     def test_adam_bias_correction(self):
0092:         """Test Adam bias correction."""
0093:         optimizer = Adam([self.param], lr=0.001)
0094:         
0095:         initial_param = self.param.data.copy()
0096:         updates = []
0097:         
0098:         # Perform several updates and track parameter changes
0099:         for _ in range(6):
0100:             self.param.grad = self.grad  # Keep constant gradient for testing
0101:             optimizer.step()
0102:             updates.append(np.linalg.norm(self.param.data - initial_param))
0103:         
0104:         # Test that:
0105:         # 1. Parameters are actually being updated
0106:         assert not np.array_equal(self.param.data, initial_param)
0107:         
0108:         # 2. Updates are being affected by bias correction
0109:         # (not necessarily smaller, but should be different)
0110:         assert len(set(updates)) > 1  # Updates should not all be identical
0111:         
0112:         # 3. State contains expected bias correction terms
0113:         assert 'step' in optimizer.state[id(self.param)]
0114:         assert 'exp_avg' in optimizer.state[id(self.param)]
0115:         assert 'exp_avg_sq' in optimizer.state[id(self.param)]
0116:         
0117:     def test_adam_amsgrad(self):
0118:         """Test Adam with AMSGrad."""
0119:         self._test_basic_update(Adam, lr=0.001, amsgrad=True)
0120: 
0121: class TestRMSprop(TestOptimizers):
0122:     """Tests for RMSprop optimizer."""
0123:     
0124:     def test_basic_rmsprop(self):
0125:         """Test basic RMSprop functionality."""
0126:         self._test_basic_update(RMSprop, lr=0.01)
0127:         
0128:     def test_rmsprop_momentum(self):
0129:         """Test RMSprop with momentum."""
0130:         self._test_basic_update(RMSprop, lr=0.01, momentum=0.9)
0131:         
0132:     def test_rmsprop_centered(self):
0133:         """Test centered RMSprop."""
0134:         self._test_basic_update(RMSprop, lr=0.01, centered=True)
0135: 
0136: class TestAdaGrad(TestOptimizers):
0137:     """Tests for AdaGrad optimizer."""
0138:     
0139:     def test_basic_adagrad(self):
0140:         """Test basic AdaGrad functionality."""
0141:         self._test_basic_update(AdaGrad, lr=0.01)
0142:         
0143:     def test_adagrad_lr_decay(self):
0144:         """Test AdaGrad learning rate decay."""
0145:         optimizer = AdaGrad([self.param], lr=0.01, lr_decay=0.1)
0146:         
0147:         initial_param = self.param.data.copy()
0148:         effective_lrs = []
0149:         
0150:         # Perform several updates and track effective learning rates
0151:         for _ in range(6):
0152:             self.param.grad = self.grad  # Keep constant gradient for testing
0153:             prev_param = self.param.data.copy()
0154:             optimizer.step()
0155:             
0156:             # Calculate effective learning rate from parameter update
0157:             param_update = np.linalg.norm(self.param.data - prev_param)
0158:             grad_norm = np.linalg.norm(self.grad)
0159:             effective_lrs.append(param_update / grad_norm if grad_norm != 0 else 0)
0160:         
0161:         # Test that:
0162:         # 1. Parameters are actually being updated
0163:         assert not np.array_equal(self.param.data, initial_param)
0164:         
0165:         # 2. Accumulated sum in state is increasing
0166:         assert np.all(optimizer.state[id(self.param)]['sum'] > 0)
0167:         
0168:         # 3. Effective learning rates should show some variation
0169:         assert len(set(map(lambda x: round(x, 6), effective_lrs))) > 1
0170:         
0171:     def test_adagrad_reset(self):
0172:         """Test AdaGrad state reset."""
0173:         optimizer = AdaGrad([self.param], lr=0.01)
0174:         
0175:         # Perform some updates
0176:         self.param.grad = self.grad
0177:         optimizer.step()
0178:         
0179:         # Reset state
0180:         optimizer.reset_state()
0181:         
0182:         # Check that state was reset
0183:         for state in optimizer.state.values():
0184:             assert state['step'] == 0
0185:             assert np.all(state['sum'] == 0)
0186: 
0187: class TestOptimizerEdgeCases:
0188:     """Tests for optimizer edge cases and error conditions."""
0189:     
0190:     def test_invalid_learning_rates(self):
0191:         """Test that invalid learning rates raise errors."""
0192:         param = Tensor([1.0], requires_grad=True)
0193:         
0194:         with pytest.raises(ValueError):
0195:             SGD([param], lr=-0.1)
0196:         with pytest.raises(ValueError):
0197:             Adam([param], lr=-0.1)
0198:         with pytest.raises(ValueError):
0199:             RMSprop([param], lr=-0.1)
0200:         with pytest.raises(ValueError):
0201:             AdaGrad([param], lr=-0.1)
0202:             
0203:     def test_no_gradients(self):
0204:         """Test optimizer behavior with no gradients."""
0205:         param = Tensor([1.0], requires_grad=True)
0206:         optimizers = [
0207:             SGD([param], lr=0.1),
0208:             Adam([param], lr=0.001),
0209:             RMSprop([param], lr=0.01),
0210:             AdaGrad([param], lr=0.01)
0211:         ]
0212:         
0213:         # Parameter should not change if there's no gradient
0214:         for optimizer in optimizers:
0215:             initial_param = param.data.copy()
0216:             optimizer.step()
0217:             assert np.array_equal(param.data, initial_param)
0218:             
0219:     def test_param_groups(self):
0220:         """Test adding parameter groups."""
0221:         param1 = Tensor([1.0], requires_grad=True)
0222:         param2 = Tensor([2.0], requires_grad=True)
0223:         
0224:         optimizer = SGD([param1], lr=0.1)
0225:         optimizer.add_param_group({'params': [param2]})
0226:         
0227:         # Both parameters should be updated
0228:         param1.grad = np.array([0.1])
0229:         param2.grad = np.array([0.2])
0230:         optimizer.step()
0231:         
0232:         assert not np.array_equal(param1.data, [1.0])
0233:         assert not np.array_equal(param2.data, [2.0])
0234:     
0235:     def test_optimizer_state_dict(self):
0236:         """Test state dict functionality"""
0237:         param = Tensor([1.0], requires_grad=True)
0238:         optimizer = SGD([param], lr=0.1)
0239:         
0240:         # Save state
0241:         state = optimizer.state_dict()
0242:         assert 'state' in state
0243:         
0244:         # Load state
0245:         new_optimizer = SGD([param], lr=0.1)
0246:         new_optimizer.load_state_dict(state)
0247:         
0248:         assert new_optimizer.state == optimizer.state
0249: 
0250:     def test_optimizer_add_param_group_validation(self):
0251:         """Test param group validation"""
0252:         param = Tensor([1.0], requires_grad=True)
0253:         optimizer = SGD([param], lr=0.1)
0254:         
0255:         # Test adding single tensor
0256:         optimizer.add_param_group({'params': Tensor([2.0], requires_grad=True)})
0257:         
0258:         # Test adding list of tensors instead of set
0259:         param_list = [Tensor([3.0], requires_grad=True)]
0260:         optimizer.add_param_group({'params': param_list})
0261:         
0262:         assert len(optimizer._params) == 3
0263: 
0264: class TestAdaDelta:
0265:     """Tests for AdaDelta optimizer."""
0266:     
0267:     def setup_method(self):
0268:         """Setup method run before each test."""
0269:         self.param = Tensor([1.0, 2.0, 3.0], requires_grad=True)
0270:         self.grad = np.array([0.1, 0.2, 0.3])
0271:         
0272:     def test_basic_adadelta(self):
0273:         """Test basic AdaDelta functionality."""
0274:         optimizer = AdaDelta([self.param])
0275:         
0276:         # Initial parameter values
0277:         initial_params = self.param.data.copy()
0278:         
0279:         # First update
0280:         self.param.grad = self.grad
0281:         optimizer.step()
0282:         
0283:         # Parameters should be updated
0284:         assert not np.array_equal(self.param.data, initial_params)
0285:         
0286:         # State should be initialized
0287:         state = optimizer.state[id(self.param)]
0288:         assert 'square_avg' in state
0289:         assert 'acc_delta' in state
0290:         assert state['step'] == 1
0291:         
0292:     def test_adadelta_no_lr(self):
0293:         """Test AdaDelta works without learning rate."""
0294:         optimizer = AdaDelta([self.param])
0295:         self.param.grad = self.grad
0296:         
0297:         # Should not raise error despite no learning rate
0298:         optimizer.step()
0299:         
0300:     def test_adadelta_convergence(self):
0301:         """Test AdaDelta converges to minimum."""
0302:         param = Tensor([1.0], requires_grad=True)
0303:         optimizer = AdaDelta([param], eps=1e-5)
0304:         
0305:         # Simple quadratic function: f(x) = (x-1)^2
0306:         for _ in range(100):
0307:             # Gradient of (x-1)^2 is 2(x-1)
0308:             param.grad = np.array([2.0 * (param.data[0] - 1.0)])
0309:             optimizer.step()
0310:             
0311:         # Should converge close to x=1
0312:         assert np.abs(param.data[0] - 1.0) < 0.1
0313:         
0314:     def test_adadelta_parameter_validation(self):
0315:         """Test parameter validation in AdaDelta."""
0316:         with pytest.raises(ValueError):
0317:             AdaDelta([self.param], rho=-0.1)
0318:         with pytest.raises(ValueError):
0319:             AdaDelta([self.param], rho=1.1)
0320:         with pytest.raises(ValueError):
0321:             AdaDelta([self.param], eps=-1e-6)
0322:         with pytest.raises(ValueError):
0323:             AdaDelta([self.param], weight_decay=-0.1)
0324: 
0325: class TestAdaMax:
0326:     """Tests for AdaMax optimizer."""
0327:     
0328:     def setup_method(self):
0329:         """Setup method run before each test."""
0330:         self.param = Tensor([1.0, 2.0, 3.0], requires_grad=True)
0331:         self.grad = np.array([0.1, 0.2, 0.3])
0332:         
0333:     def test_basic_adamax(self):
0334:         """Test basic AdaMax functionality."""
0335:         optimizer = AdaMax([self.param])
0336:         
0337:         # Initial parameter values
0338:         initial_params = self.param.data.copy()
0339:         
0340:         # First update
0341:         self.param.grad = self.grad
0342:         optimizer.step()
0343:         
0344:         # Parameters should be updated
0345:         assert not np.array_equal(self.param.data, initial_params)
0346:         
0347:         # State should be initialized
0348:         state = optimizer.state[id(self.param)]
0349:         assert 'exp_avg' in state
0350:         assert 'exp_inf' in state
0351:         assert state['step'] == 1
0352:         
0353:     def test_adamax_bias_correction(self):
0354:         """Test AdaMax bias correction."""
0355:         optimizer = AdaMax([self.param], lr=0.01)
0356:         
0357:         updates = []
0358:         initial_param = self.param.data.copy()
0359:         
0360:         # Perform several updates and track parameter changes
0361:         for _ in range(5):
0362:             self.param.grad = self.grad
0363:             optimizer.step()
0364:             updates.append(np.linalg.norm(self.param.data - initial_param))
0365:             
0366:         # Updates should vary due to bias correction
0367:         assert len(set(updates)) > 1
0368:         
0369:     def test_adamax_convergence(self):
0370:         """Test AdaMax converges to minimum."""
0371:         param = Tensor([2.0], requires_grad=True)
0372:         optimizer = AdaMax([param], lr=0.1)
0373:         
0374:         # Simple quadratic function: f(x) = (x-1)^2
0375:         for _ in range(100):
0376:             # Gradient of (x-1)^2 is 2(x-1)
0377:             param.grad = np.array([2.0 * (param.data[0] - 1.0)])
0378:             optimizer.step()
0379:             
0380:         # Should converge close to x=1
0381:         assert np.abs(param.data[0] - 1.0) < 0.1
0382:         
0383:     def test_adamax_parameter_validation(self):
0384:         """Test parameter validation in AdaMax."""
0385:         with pytest.raises(ValueError):
0386:             AdaMax([self.param], lr=-0.1)
0387:         with pytest.raises(ValueError):
0388:             AdaMax([self.param], eps=-1e-8)
0389:         with pytest.raises(ValueError):
0390:             AdaMax([self.param], betas=(-0.1, 0.999))
0391:         with pytest.raises(ValueError):
0392:             AdaMax([self.param], betas=(0.9, 1.1))
0393:         with pytest.raises(ValueError):
0394:             AdaMax([self.param], weight_decay=-0.1)
0395: 
0396: class TestAdvancedOptimizerEdgeCases:
0397:     """Tests for edge cases in advanced optimizers."""
0398:     
0399:     def test_zero_gradients(self):
0400:         """Test handling of zero gradients."""
0401:         param = Tensor([1.0], requires_grad=True)
0402:         optimizers = [
0403:             AdaDelta([param]),
0404:             AdaMax([param])
0405:         ]
0406:         
0407:         for optimizer in optimizers:
0408:             # Parameter should not change if there's no gradient
0409:             initial_param = param.data.copy()
0410:             optimizer.step()
0411:             assert np.array_equal(param.data, initial_param)
0412:             
0413:     def test_state_dict(self):
0414:         """Test state dict functionality."""
0415:         param = Tensor([1.0], requires_grad=True)
0416:         optimizers = [
0417:             AdaDelta([param]),
0418:             AdaMax([param])
0419:         ]
0420:         
0421:         for optimizer in optimizers:
0422:             # Perform an update
0423:             param.grad = np.array([0.1])
0424:             optimizer.step()
0425:             
0426:             # Save state
0427:             state = optimizer.state_dict()
0428:             assert 'state' in state
0429:             assert 'defaults' in state
0430:             
0431:             # Create new optimizer and load state
0432:             if isinstance(optimizer, AdaDelta):
0433:                 new_optimizer = AdaDelta([param])
0434:             else:
0435:                 new_optimizer = AdaMax([param])
0436:             
0437:             new_optimizer.load_state_dict(state)
0438:             
0439:             # States should match
0440:             assert new_optimizer.state.keys() == optimizer.state.keys()
0441:             for key in optimizer.state:
0442:                 assert all(np.array_equal(optimizer.state[key][k], 
0443:                                         new_optimizer.state[key][k])
0444:                          for k in optimizer.state[key]
0445:                          if isinstance(optimizer.state[key][k], np.ndarray))
0446:                          
0447:     def test_sparse_gradients(self):
0448:         """Test handling of sparse gradients."""
0449:         param = Tensor([1.0, 0.0, 2.0, 0.0], requires_grad=True)
0450:         optimizers = [
0451:             AdaDelta([param]),
0452:             AdaMax([param])
0453:         ]
0454:         
0455:         # Create sparse gradient (most values zero)
0456:         sparse_grad = np.array([0.1, 0.0, 0.0, 0.3])
0457:         
0458:         for optimizer in optimizers:
0459:             param.grad = sparse_grad
0460:             optimizer.step()
0461:             
0462:             # Check that only non-zero gradient entries caused updates
0463:             state = optimizer.state[id(param)]
0464:             if isinstance(optimizer, AdaDelta):
0465:                 square_avg = state['square_avg']
0466:                 assert square_avg[1] == 0  # No update where gradient was 0
0467:                 assert square_avg[2] == 0
0468:                 assert square_avg[0] != 0  # Update where gradient was non-zero
0469:                 assert square_avg[3] != 0
0470:             else:  # AdaMax
0471:                 exp_avg = state['exp_avg']
0472:                 assert exp_avg[1] == 0  # No update where gradient was 0
0473:                 assert exp_avg[2] == 0
0474:                 assert exp_avg[0] != 0  # Update where gradient was non-zero
0475:                 assert exp_avg[3] != 0
0476: 
0477:     def test_momentum_behavior(self):
0478:         """Test momentum-like behavior in optimizers with varying gradients."""
0479:         # Initialize a single parameter
0480:         param_adadelta = Tensor([1.0], requires_grad=True)
0481:         param_adamax = Tensor([1.0], requires_grad=True)
0482: 
0483:         optimizers = [
0484:             AdaDelta([param_adadelta], rho=0.9),
0485:             AdaMax([param_adamax], betas=(0.9, 0.999))
0486:         ]
0487: 
0488:         for optimizer, param in zip(optimizers, [param_adadelta, param_adamax]):
0489:             # Apply varying gradients
0490:             updates = []
0491:             for i in range(1, 6):
0492:                 param.grad = np.array([0.1 * i])  # Gradients increase each step
0493:                 optimizer.step()
0494:                 updates.append(float(param.data[0]))
0495: 
0496:             # Compute differences between consecutive updates
0497:             diffs = np.diff(updates)
0498: 
0499:             # Debugging output (optional)
0500:             print(f"{type(optimizer).__name__} updates: {updates}")
0501:             print(f"{type(optimizer).__name__} diffs: {diffs}")
0502: 
0503:             # Assert that diffs are not all close to zero
0504:             assert not np.allclose(diffs, diffs[0]), (
0505:                 f"{type(optimizer).__name__} does not exhibit momentum-like behavior"
0506:             )
0507: 
0508:     def test_numerical_stability(self):
0509:         """Test numerical stability with extreme values."""
0510:         param = Tensor([1e-6, 1e6], requires_grad=True)
0511:         optimizers = [
0512:             AdaDelta([param], eps=1e-7),
0513:             AdaMax([param], eps=1e-7)
0514:         ]
0515:         
0516:         for optimizer in optimizers:
0517:             # Test with very small and very large gradients
0518:             param.grad = np.array([1e-8, 1e8])
0519:             
0520:             try:
0521:                 optimizer.step()
0522:             except Exception as e:
0523:                 pytest.fail(f"Optimizer {type(optimizer).__name__} failed with extreme values: {str(e)}")
0524:                 
0525:             # Check for NaN or inf values
0526:             assert not np.any(np.isnan(param.data))
0527:             assert not np.any(np.isinf(param.data))
0528: 
0529:     def test_weight_decay(self):
0530:         """Test weight decay in optimizers."""
0531:         weight_decay = 0.2
0532:         
0533:         # Test both optimizers with and without weight decay
0534:         for optimizer_class in [AdaDelta, AdaMax]:
0535:             # Without weight decay
0536:             param_no_decay = Tensor([1.0, 2.0], requires_grad=True)
0537:             opt_no_decay = optimizer_class([param_no_decay])
0538:             param_no_decay.grad = np.array([0.1, 0.2])
0539:             opt_no_decay.step()
0540:             result_no_decay = param_no_decay.data.copy()
0541:             
0542:             # With weight decay
0543:             param_with_decay = Tensor([1.0, 2.0], requires_grad=True)
0544:             opt_with_decay = optimizer_class([param_with_decay], weight_decay=weight_decay)
0545:             param_with_decay.grad = np.array([0.1, 0.2])
0546:             opt_with_decay.step()
0547:             result_with_decay = param_with_decay.data.copy()
0548:             
0549:             # Parameters should be smaller with weight decay
0550:             assert np.all(np.abs(result_with_decay) < np.abs(result_no_decay)), \
0551:                 f"{type(optimizer_class).__name__} does not correctly apply weight decay"

// File: C:\Users\aluja\Desktop\DLpy\tests\test_tensor.py
// ----------------------------------------
0001: import pytest
0002: import numpy as np
0003: from DLpy.core import Tensor
0004: 
0005: class TestReshapeOp:
0006:     """Tests for Reshape operation"""
0007: 
0008:     def test_reshape_edge_cases(self):
0009:         """Test edge cases for Reshape operation"""
0010:         x = Tensor([1.0, 2.0], requires_grad=True)
0011:         
0012:         # Test invalid shape
0013:         with pytest.raises(ValueError):
0014:             _ = x.reshape(3)  # Invalid shape
0015:         
0016:         # Test gradients with different shapes
0017:         y = x.reshape(2, 1)
0018:         y.backward(np.array([[1.0], [1.0]]))
0019:         assert np.array_equal(x.grad, [1.0, 1.0])
0020: 
0021: class TestAdvancedOperations:
0022:     """Additional tests for basic operations"""
0023:     
0024:     def test_broadcasting_edge_cases(self):
0025:         """Test broadcasting with different dimensions"""
0026:         # Test broadcasting scalar to matrix
0027:         x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
0028:         y = Tensor(2.0, requires_grad=True)
0029:         z = x * y
0030:         z.backward(np.ones_like(x.data))
0031:         assert y.grad.shape == (1,)
0032:         assert np.array_equal(x.grad, [[2.0, 2.0], [2.0, 2.0]])
0033:         
0034:         # Test broadcasting vector to matrix
0035:         x = Tensor([[1.0], [2.0]], requires_grad=True)
0036:         y = Tensor([1.0, 2.0], requires_grad=True)
0037:         z = x + y  # Should broadcast to [[1,2], [2,3]]
0038:         z.backward(np.ones((2, 2)))
0039:         assert x.grad.shape == (2, 1)
0040:         assert y.grad.shape == (2,)
0041:         
0042:     def test_zero_gradient_handling(self):
0043:         """Test operations with zero gradients"""
0044:         x = Tensor([1.0, 2.0], requires_grad=True)
0045:         y = Tensor([3.0, 4.0], requires_grad=True)
0046:         z = x * y
0047:         z.backward(np.zeros_like(z.data))
0048:         assert np.all(x.grad == 0)
0049:         assert np.all(y.grad == 0)
0050:         
0051:     def test_non_differentiable_inputs(self):
0052:         """Test operations with non-differentiable inputs"""
0053:         x = Tensor([1.0, 2.0], requires_grad=False)
0054:         y = Tensor([3.0, 4.0], requires_grad=True)
0055:         z = x * y
0056:         z.backward(np.ones_like(z.data))
0057:         assert x.grad is None  # Non-differentiable input should have no gradient
0058:         assert np.array_equal(y.grad, [1.0, 2.0])
0059: 
0060:     def test_tensor_creation_edge_cases(self):
0061:         """Test edge cases in tensor creation"""
0062:         # Test with different dtypes
0063:         t1 = Tensor([1, 2, 3], dtype=np.int32)
0064:         assert t1.dtype == np.int32
0065:         
0066:         # Test with nested lists
0067:         t2 = Tensor([[1, 2], [3, 4]])
0068:         assert t2.shape == (2, 2)
0069:         
0070:         # Test with another tensor
0071:         t3 = Tensor(t2)
0072:         assert np.array_equal(t3.data, t2.data)
0073: 
0074:     def test_backward_edge_cases(self):
0075:         """Test edge cases in backward pass"""
0076:         # Test backward with scalar tensor
0077:         x = Tensor(2.0, requires_grad=True)
0078:         y = x * 2
0079:         y.backward(np.array(3.0))
0080:         assert x.grad is not None
0081:         
0082:         # Test backward with non-scalar tensor without gradient
0083:         x = Tensor([1.0, 2.0, 3.0], requires_grad=True)
0084:         y = x * 2
0085:         with pytest.raises(RuntimeError):
0086:             y.backward()  # Should raise error for non-scalar
0087: 
0088:     def test_repr_and_str(self):
0089:         """Test string representations"""
0090:         t = Tensor([1.0, 2.0], requires_grad=True)
0091:         assert 'Tensor' in repr(t)
0092:         assert 'requires_grad=True' in repr(t)

// ----------------------------------------
// Total Python files found: 45
