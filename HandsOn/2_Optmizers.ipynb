{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Optimization: The Heart of Deep Learning Training\n",
    "\n",
    "Welcome to our exploration of optimization in deep learning! In this notebook, we'll uncover how neural networks actually learn from data. While our previous notebook showed us what tensors are and how to manipulate them, this notebook will show us how to systematically adjust these tensors to make our models smarter.\n",
    "\n",
    "## Table of Contents\n",
    "1. [The Optimization Problem: A Mathematical Foundation](#optimization-foundation)\n",
    "2. [Understanding Convexity and its Implications](#convexity)\n",
    "3. [Gradient-Based Methods](#gradient-methods)\n",
    "4. [Stochastic Methods and Mini-batching](#stochastic-methods)\n",
    "5. [Modern Optimization Algorithms](#modern-optimizers)\n",
    "6. [Practical Considerations](#practical)\n",
    "7. [Beyond Gradient Descent](#beyond)\n",
    "8. [Conclusion](#conclusion)\n",
    "\n",
    "<a id=\"optimization-foundation\"></a>\n",
    "\n",
    "# Understanding Optimization: From Foundations to Deep Learning\n",
    "\n",
    "Building on our understanding of tensors as the fundamental building blocks of neural networks, we now turn to optimization - the mathematical framework that enables neural networks to learn. While tensors provide the structure for representing and transforming data, optimization provides the machinery for systematically adjusting these transformations to minimize error.\n",
    "\n",
    "This exploration bridges classical optimization theory with modern deep learning practice. We'll examine how traditional methods like gradient descent extend to handle the high-dimensional, non-convex landscapes characteristic of neural networks, and how this understanding led to the development of specialized optimizers like Adam and RMSprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_optimization_characteristics():\n",
    "    \"\"\"\n",
    "    Demonstrates key challenges in neural network optimization through\n",
    "    carefully constructed examples that highlight specific mathematical properties.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # 1. Demonstrate high dimensionality effects\n",
    "    dims = [2, 10, 100, 1000]\n",
    "    distances = []\n",
    "    for d in dims:\n",
    "        # Generate random points in d dimensions\n",
    "        points = np.random.randn(1000, d)\n",
    "        # Compute pairwise distances\n",
    "        dists = np.linalg.norm(points[:, None] - points, axis=2)\n",
    "        distances.append(dists[~np.eye(dists.shape[0], dtype=bool)])\n",
    "    \n",
    "    axes[0].boxplot(distances)\n",
    "    axes[0].set_xticklabels(dims)\n",
    "    axes[0].set_title('Distance Concentration\\nin High Dimensions')\n",
    "    axes[0].set_xlabel('Dimensionality')\n",
    "    axes[0].set_ylabel('Pairwise Distances')\n",
    "    \n",
    "    # 2. Non-convex loss surface\n",
    "    x = np.linspace(-2, 2, 100)\n",
    "    y = np.linspace(-2, 2, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = (1 - X)**2 + 100*(Y - X**2)**2  # Rosenbrock function\n",
    "    \n",
    "    axes[1].contour(X, Y, Z, levels=np.logspace(-1, 3, 20))\n",
    "    axes[1].set_title('Non-Convex Loss Surface\\n(Rosenbrock Function)')\n",
    "    \n",
    "    # 3. Stochastic gradient estimation\n",
    "    def true_gradient(x):\n",
    "        return 2*x\n",
    "    \n",
    "    x_vals = np.linspace(-2, 2, 100)\n",
    "    true_grads = true_gradient(x_vals)\n",
    "    noisy_grads = true_grads + np.random.normal(0, 0.5, size=true_grads.shape)\n",
    "    \n",
    "    axes[2].plot(x_vals, true_grads, 'b-', label='True Gradient')\n",
    "    axes[2].plot(x_vals, noisy_grads, 'r.', alpha=0.3, label='Mini-batch Estimates')\n",
    "    axes[2].set_title('Stochastic Gradient\\nEstimation')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_optimization_characteristics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Optimization Problem: A Mathematical Foundation\n",
    "\n",
    "At its core, training a machine learning model is an optimization problem. We're trying to find the best set of parameters that minimize some objective function (often called the loss function). Let's build intuition about what this means.\n",
    "\n",
    "### 1.1 The Basic Setup\n",
    "\n",
    "Consider a simple neural network with parameters $\\theta$ (which could be weights and biases). Our goal is to minimize some loss function $\\mathcal{L}(\\theta)$:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\underset{\\theta}{\\arg\\min} \\,\\, \\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta^*$ represents the optimal parameters\n",
    "- $\\mathcal{L}(\\theta)$ measures how poorly our model performs\n",
    "- The $\\arg\\min$ notation means \"find the argument ($\\theta$) that minimizes the function\"\n",
    "\n",
    "### 1.2 Why is this Hard?\n",
    "\n",
    "Several challenges make optimization in deep learning particularly difficult:\n",
    "\n",
    "1. **Non-Convexity**: The loss landscape is typically non-convex, meaning there are many local minima and saddle points.\n",
    "\n",
    "2. **High Dimensionality**: Modern networks can have millions or billions of parameters.\n",
    "\n",
    "3. **Stochasticity**: We usually work with random mini-batches of data, making our gradient estimates noisy.\n",
    "\n",
    "4. **Ill-Conditioning**: Different parameters might need very different scales of updates.\n",
    "\n",
    "Let's visualize some of these challenges with a simple 2D example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_loss_landscape():\n",
    "    # Create a grid of points\n",
    "    x = np.linspace(-4, 4, 100)\n",
    "    y = np.linspace(-4, 4, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Non-convex function with multiple local minima\n",
    "    Z = np.sin(X) * np.cos(Y) + 0.1 * (X**2 + Y**2)\n",
    "    \n",
    "    # Create 3D surface plot\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Surface plot\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    surf = ax1.plot_surface(X, Y, Z, cmap='viridis')\n",
    "    ax1.set_title('Loss Landscape (3D View)')\n",
    "    ax1.set_xlabel('Parameter 1')\n",
    "    ax1.set_ylabel('Parameter 2')\n",
    "    ax1.set_zlabel('Loss')\n",
    "    \n",
    "    # Contour plot\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    contour = ax2.contour(X, Y, Z, levels=20)\n",
    "    ax2.clabel(contour, inline=True)\n",
    "    ax2.set_title('Loss Landscape (Contour View)')\n",
    "    ax2.set_xlabel('Parameter 1')\n",
    "    ax2.set_ylabel('Parameter 2')\n",
    "    \n",
    "    plt.colorbar(surf, ax=ax1, shrink=0.5, aspect=5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_landscape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Types of Optimization Problems\n",
    "\n",
    "Before diving into specific algorithms, it's helpful to categorize optimization problems:\n",
    "\n",
    "1. **Convex vs Non-Convex**\n",
    "   - Convex problems have a unique global minimum\n",
    "   - Non-convex problems may have multiple local minima\n",
    "\n",
    "2. **Constrained vs Unconstrained**\n",
    "   - Constrained: Parameters must satisfy certain conditions\n",
    "   - Unconstrained: Parameters can take any value\n",
    "\n",
    "3. **Smooth vs Non-Smooth**\n",
    "   - Smooth: Functions are differentiable\n",
    "   - Non-smooth: Functions may have \"kinks\" or discontinuities\n",
    "\n",
    "4. **Deterministic vs Stochastic**\n",
    "   - Deterministic: Fixed objective function\n",
    "   - Stochastic: Objective function involves randomness\n",
    "\n",
    "Deep learning typically deals with non-convex, unconstrained, smooth, and stochastic optimization problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"convexity\"></a>\n",
    "## 2. Understanding Convexity and its Implications\n",
    "\n",
    "Convexity is a fundamental concept in optimization that helps us understand when we can guarantee finding global minima. While deep learning problems are typically non-convex, understanding convexity helps us build intuition about optimization algorithms.\n",
    "\n",
    "### 2.1 Definition of Convexity\n",
    "\n",
    "A function $f$ is convex if for any two points $x_1$ and $x_2$, and any $t \\in [0,1]$:\n",
    "\n",
    "$$\n",
    "f(tx_1 + (1-t)x_2) \\leq tf(x_1) + (1-t)f(x_2)\n",
    "$$\n",
    "\n",
    "In simpler terms: the line segment between any two points on the function lies above the function's graph.\n",
    "\n",
    "Let's visualize this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_convexity_example():\n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Generate x values\n",
    "    x = np.linspace(-4, 4, 100)\n",
    "    \n",
    "    # Convex function (quadratic)\n",
    "    y1 = x**2\n",
    "    ax1.plot(x, y1, 'b-', label='f(x) = x²')\n",
    "    ax1.set_title('Convex Function')\n",
    "    \n",
    "    # Non-convex function (sine)\n",
    "    y2 = np.sin(x) + 0.1 * x**2\n",
    "    ax2.plot(x, y2, 'r-', label='f(x) = sin(x) + 0.1x²')\n",
    "    ax2.set_title('Non-convex Function')\n",
    "    \n",
    "    # Add line segments to demonstrate convexity\n",
    "    x1, x2 = -3, 3\n",
    "    t = np.linspace(0, 1, 100)\n",
    "    \n",
    "    # For convex function\n",
    "    y1_1, y1_2 = x1**2, x2**2\n",
    "    x_line = t*x1 + (1-t)*x2\n",
    "    y_line = t*y1_1 + (1-t)*y1_2\n",
    "    y_true = x_line**2\n",
    "    ax1.plot([x1, x2], [y1_1, y1_2], 'g--', label='Line segment')\n",
    "    \n",
    "    # For non-convex function\n",
    "    y2_1, y2_2 = np.sin(x1) + 0.1*x1**2, np.sin(x2) + 0.1*x2**2\n",
    "    y_line2 = t*y2_1 + (1-t)*y2_2\n",
    "    ax2.plot([x1, x2], [y2_1, y2_2], 'g--', label='Line segment')\n",
    "    \n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('f(x)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_convexity_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Properties of Convex Functions\n",
    "\n",
    "1. **Local Minimum = Global Minimum**\n",
    "   - In convex optimization, any local minimum is guaranteed to be global\n",
    "   - This makes optimization much easier\n",
    "\n",
    "2. **First-Order Condition**\n",
    "   - For differentiable convex functions, if $\\nabla f(x) = 0$, then $x$ is a global minimum\n",
    "   - This is why gradient-based methods work well for convex problems\n",
    "\n",
    "3. **Second-Order Condition**\n",
    "   - For twice-differentiable functions, positive semi-definite Hessian implies convexity\n",
    "   - This gives us information about the curvature of the function\n",
    "\n",
    "### 2.3 Why Deep Learning is Non-Convex\n",
    "\n",
    "Let's understand why neural networks lead to non-convex optimization problems:\n",
    "\n",
    "1. **Composition of Functions**\n",
    "   - Neural networks stack multiple layers of transformations\n",
    "   - Even if each layer's operation is simple, the composition creates non-convexity\n",
    "\n",
    "2. **Parameter Symmetry**\n",
    "   - Different parameter configurations can give identical networks\n",
    "   - This creates multiple equivalent minima\n",
    "\n",
    "Here's a simple example showing how even a basic neural network creates non-convexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_simple_nn_loss():\n",
    "    # Create a simple dataset\n",
    "    X = np.linspace(-4, 4, 100)\n",
    "    y = np.sin(X)\n",
    "    \n",
    "    # Define a simple neural network with one hidden unit\n",
    "    def nn_output(w1, w2, x):\n",
    "        return w1 * np.maximum(0, w2 * x)  # ReLU activation\n",
    "    \n",
    "    # Create loss landscape\n",
    "    w1 = np.linspace(-2, 2, 50)\n",
    "    w2 = np.linspace(-2, 2, 50)\n",
    "    W1, W2 = np.meshgrid(w1, w2)\n",
    "    \n",
    "    # Compute MSE loss\n",
    "    Z = np.zeros_like(W1)\n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            pred = nn_output(W1[i,j], W2[i,j], X)\n",
    "            Z[i,j] = np.mean((pred - y)**2)\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Surface plot\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    surf = ax1.plot_surface(W1, W2, Z, cmap='viridis')\n",
    "    ax1.set_title('Loss Landscape of Simple Neural Network')\n",
    "    ax1.set_xlabel('Weight 1')\n",
    "    ax1.set_ylabel('Weight 2')\n",
    "    ax1.set_zlabel('Loss')\n",
    "    \n",
    "    # Contour plot\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    contour = ax2.contour(W1, W2, Z, levels=20)\n",
    "    ax2.clabel(contour, inline=True)\n",
    "    ax2.set_title('Loss Landscape (Contour View)')\n",
    "    ax2.set_xlabel('Weight 1')\n",
    "    ax2.set_ylabel('Weight 2')\n",
    "    \n",
    "    plt.colorbar(surf, ax=ax1, shrink=0.5, aspect=5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_simple_nn_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gradient-methods\"></a>\n",
    "## 3. Gradient-Based Methods\n",
    "\n",
    "Now that we understand the optimization landscape, let's explore how gradient-based methods navigate it. We'll start with the simplest approach and build up to more sophisticated algorithms.\n",
    "\n",
    "### 3.1 Gradient Descent\n",
    "\n",
    "The most basic approach is gradient descent, which updates parameters in the direction of steepest descent:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\nabla \\mathcal{L}(\\theta_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta_t$ is the parameter vector at step $t$\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\nabla \\mathcal{L}(\\theta_t)$ is the gradient of the loss with respect to parameters\n",
    "\n",
    "Let's implement a basic gradient descent optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Basic gradient descent optimizer.\n",
    "        \n",
    "        Args:\n",
    "            params: List of parameters to optimize\n",
    "            learning_rate: Step size for updates\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Performs one optimization step.\"\"\"\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                # Update rule: θ = θ - η∇L\n",
    "                param.data -= self.lr * param.grad\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Clears the gradients of all parameters.\"\"\"\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how gradient descent works on a simple 2D function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_gradient_descent():\n",
    "    # Create a simple 2D function\n",
    "    def f(x, y):\n",
    "        return x**2 + 2*y**2\n",
    "    \n",
    "    def grad_f(x, y):\n",
    "        return np.array([2*x, 4*y])\n",
    "    \n",
    "    # Create grid of points\n",
    "    x = np.linspace(-2, 2, 100)\n",
    "    y = np.linspace(-2, 2, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    \n",
    "    # Run gradient descent\n",
    "    path_x, path_y = [-1.5], [1.5]  # Starting point\n",
    "    lr = 0.1\n",
    "    \n",
    "    for _ in range(20):\n",
    "        grad = grad_f(path_x[-1], path_y[-1])\n",
    "        path_x.append(path_x[-1] - lr * grad[0])\n",
    "        path_y.append(path_y[-1] - lr * grad[1])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contour(X, Y, Z, levels=20)\n",
    "    plt.colorbar(label='f(x, y)')\n",
    "    plt.plot(path_x, path_y, 'r.-', label='Gradient descent path')\n",
    "    plt.plot(path_x[0], path_y[0], 'go', label='Start')\n",
    "    plt.plot(path_x[-1], path_y[-1], 'ro', label='End')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Gradient Descent Optimization Path')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "visualize_gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Challenges with Basic Gradient Descent\n",
    "\n",
    "Several issues make basic gradient descent suboptimal:\n",
    "\n",
    "1. **Learning Rate Selection**\n",
    "   - Too large: overshooting, oscillation\n",
    "   - Too small: slow convergence\n",
    "\n",
    "2. **Ill-Conditioning**\n",
    "   - Different parameters may require different scales of updates\n",
    "   - The loss surface might be much steeper in some directions than others\n",
    "\n",
    "3. **Saddle Points**\n",
    "   - Points where the gradient is zero but it's not a minimum\n",
    "   - Can cause optimization to get stuck\n",
    "\n",
    "Let's visualize these challenges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_learning_rate_effects():\n",
    "    # Create a simple 1D function\n",
    "    x = np.linspace(-2, 2, 200)\n",
    "    y = x**2  # Simple quadratic function\n",
    "    \n",
    "    def gradient(x):\n",
    "        return 2*x\n",
    "    \n",
    "    # Different learning rates\n",
    "    learning_rates = [0.1, 0.5, 1.2]\n",
    "    starting_point = 1.8\n",
    "    \n",
    "    # Plot setup\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot the function\n",
    "    axes[0].plot(x, y, 'b-', label='f(x) = x²')\n",
    "    axes[0].grid(True)\n",
    "    axes[0].set_title('Optimization Paths with Different Learning Rates')\n",
    "    axes[0].set_xlabel('x')\n",
    "    axes[0].set_ylabel('f(x)')\n",
    "    \n",
    "    # Plot the gradient descent paths\n",
    "    colors = ['g', 'r', 'purple']\n",
    "    for lr, color in zip(learning_rates, colors):\n",
    "        current_x = starting_point\n",
    "        path_x = [current_x]\n",
    "        path_y = [current_x**2]\n",
    "        \n",
    "        for _ in range(10):\n",
    "            current_x = current_x - lr * gradient(current_x)\n",
    "            path_x.append(current_x)\n",
    "            path_y.append(current_x**2)\n",
    "        \n",
    "        axes[0].plot(path_x, path_y, f'{color}.-', \n",
    "                    label=f'η = {lr}')\n",
    "        axes[0].plot(path_x[0], path_y[0], f'{color}o')\n",
    "    \n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot the convergence rates\n",
    "    for lr, color in zip(learning_rates, colors):\n",
    "        current_x = starting_point\n",
    "        values = [abs(current_x)]\n",
    "        \n",
    "        for _ in range(20):\n",
    "            current_x = current_x - lr * gradient(current_x)\n",
    "            values.append(abs(current_x))\n",
    "        \n",
    "        axes[1].plot(values, f'{color}.-', \n",
    "                    label=f'η = {lr}')\n",
    "    \n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True)\n",
    "    axes[1].set_title('Convergence Rates')\n",
    "    axes[1].set_xlabel('Iteration')\n",
    "    axes[1].set_ylabel('|x|')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_learning_rate_effects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Momentum Methods\n",
    "\n",
    "To address some of the challenges with basic gradient descent, momentum methods were introduced. The key idea is to maintain a \"velocity\" term that helps smooth out oscillations and accelerate convergence. Think of it like a ball rolling down a hill – it builds up momentum in consistent directions.\n",
    "\n",
    "The classical momentum update equations are:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{t+1} &= \\beta v_t + \\nabla \\mathcal{L}(\\theta_t) \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\eta v_{t+1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $v_t$ is the velocity at time t\n",
    "- $\\beta$ is the momentum coefficient (typically around 0.9)\n",
    "- $\\eta$ is still our learning rate\n",
    "\n",
    "Let's implement and visualize momentum in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MomentumOptimizer:\n",
    "    def __init__(self, params, learning_rate=0.01, momentum=0.9):\n",
    "        \"\"\"\n",
    "        Gradient descent with momentum.\n",
    "        \n",
    "        Args:\n",
    "            params: List of parameters to optimize\n",
    "            learning_rate: Step size for updates\n",
    "            momentum: Momentum coefficient (β)\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocity = [np.zeros_like(p.data) for p in params]\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Performs one optimization step.\"\"\"\n",
    "        for i, param in enumerate(self.params):\n",
    "            if param.grad is not None:\n",
    "                # Update velocity: v = βv + ∇L\n",
    "                self.velocity[i] = (self.momentum * self.velocity[i] + \n",
    "                                  param.grad)\n",
    "                # Update parameters: θ = θ - ηv\n",
    "                param.data -= self.lr * self.velocity[i]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Clears the gradients of all parameters.\"\"\"\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "def visualize_momentum_effect():\n",
    "    # Create a pathological loss surface (like a narrow valley)\n",
    "    def f(x, y):\n",
    "        return 10 * x**2 + y**2\n",
    "    \n",
    "    def grad_f(x, y):\n",
    "        return np.array([20*x, 2*y])\n",
    "    \n",
    "    # Create grid of points\n",
    "    x = np.linspace(-2, 2, 100)\n",
    "    y = np.linspace(-2, 2, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    \n",
    "    # Run optimizers\n",
    "    start_x, start_y = -1.5, 1.5\n",
    "    \n",
    "    # Without momentum\n",
    "    path_x_gd, path_y_gd = [start_x], [start_y]\n",
    "    lr = 0.05\n",
    "    \n",
    "    # With momentum\n",
    "    path_x_momentum, path_y_momentum = [start_x], [start_y]\n",
    "    velocity_x, velocity_y = 0, 0\n",
    "    beta = 0.9\n",
    "    \n",
    "    for _ in range(40):\n",
    "        # Regular gradient descent\n",
    "        grad = grad_f(path_x_gd[-1], path_y_gd[-1])\n",
    "        path_x_gd.append(path_x_gd[-1] - lr * grad[0])\n",
    "        path_y_gd.append(path_y_gd[-1] - lr * grad[1])\n",
    "        \n",
    "        # Momentum update\n",
    "        grad = grad_f(path_x_momentum[-1], path_y_momentum[-1])\n",
    "        velocity_x = beta * velocity_x + grad[0]\n",
    "        velocity_y = beta * velocity_y + grad[1]\n",
    "        path_x_momentum.append(path_x_momentum[-1] - lr * velocity_x)\n",
    "        path_y_momentum.append(path_y_momentum[-1] - lr * velocity_y)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contour(X, Y, Z, levels=20)\n",
    "    plt.colorbar(label='f(x, y)')\n",
    "    \n",
    "    plt.plot(path_x_gd, path_y_gd, 'r.-', \n",
    "            label='Gradient descent', alpha=0.7)\n",
    "    plt.plot(path_x_momentum, path_y_momentum, 'b.-', \n",
    "            label='Momentum', alpha=0.7)\n",
    "    \n",
    "    plt.plot(start_x, start_y, 'go', label='Start')\n",
    "    plt.plot(0, 0, 'ro', label='Optimum')\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Gradient Descent vs Momentum: Pathological Curvature')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "visualize_momentum_effect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization shows how momentum helps in two key ways:\n",
    "1. It reduces oscillation in directions of high curvature\n",
    "2. It maintains velocity in consistent directions, helping escape plateaus\n",
    "\n",
    "### 3.4 Adaptive Learning Rate Methods\n",
    "\n",
    "While momentum helps with oscillations, it doesn't address the fact that different parameters might need different learning rates. This led to the development of adaptive methods, where each parameter gets its own learning rate based on the history of gradients.\n",
    "\n",
    "The most popular adaptive methods are:\n",
    "\n",
    "1. **AdaGrad**: Accumulates squared gradients from the beginning\n",
    "   $$\n",
    "   \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\odot \\nabla \\mathcal{L}(\\theta_t)\n",
    "   $$\n",
    "   where $G_t$ is the sum of squared gradients up to step t\n",
    "\n",
    "2. **RMSProp**: Uses an exponentially decaying average of squared gradients\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   v_t &= \\beta v_{t-1} + (1-\\beta)(\\nabla \\mathcal{L}(\\theta_t))^2 \\\\\n",
    "   \\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{v_t + \\epsilon}} \\odot \\nabla \\mathcal{L}(\\theta_t)\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "3. **Adam**: Combines momentum with adaptive learning rates\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   m_t &= \\beta_1 m_{t-1} + (1-\\beta_1)\\nabla \\mathcal{L}(\\theta_t) \\\\\n",
    "   v_t &= \\beta_2 v_{t-1} + (1-\\beta_2)(\\nabla \\mathcal{L}(\\theta_t))^2 \\\\\n",
    "   \\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t} \\\\\n",
    "   \\hat{v}_t &= \\frac{v_t}{1-\\beta_2^t} \\\\\n",
    "   \\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t + \\epsilon}} \\odot \\hat{m}_t\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "Let's implement Adam and see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, params, learning_rate=0.001, betas=(0.9, 0.999), eps=1e-8):\n",
    "        \"\"\"\n",
    "        Adam optimizer.\n",
    "        \n",
    "        Args:\n",
    "            params: List of parameters to optimize\n",
    "            learning_rate: Step size\n",
    "            betas: Coefficients for computing running averages\n",
    "            eps: Term added to denominator for numerical stability\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.lr = learning_rate\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Initialize momentum and velocity\n",
    "        self.m = [np.zeros_like(p.data) for p in params]  # First moment\n",
    "        self.v = [np.zeros_like(p.data) for p in params]  # Second moment\n",
    "        self.t = 0  # Time step\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Performs one optimization step.\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        for i, param in enumerate(self.params):\n",
    "            if param.grad is not None:\n",
    "                g = param.grad\n",
    "                \n",
    "                # Update biased first moment estimate\n",
    "                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * g\n",
    "                # Update biased second raw moment estimate\n",
    "                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * g**2\n",
    "                \n",
    "                # Compute bias-corrected moments\n",
    "                m_hat = self.m[i] / (1 - self.beta1**self.t)\n",
    "                v_hat = self.v[i] / (1 - self.beta2**self.t)\n",
    "                \n",
    "                # Update parameters\n",
    "                param.data -= (self.lr * m_hat / \n",
    "                             (np.sqrt(v_hat) + self.eps))\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Clears the gradients of all parameters.\"\"\"\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "def compare_optimizers():\n",
    "    \"\"\"Compare different optimizers on a challenging function.\"\"\"\n",
    "    # Create a challenging function (like a curved valley)\n",
    "    def f(x, y):\n",
    "        return 10 * (y - x**2)**2 + (1-x)**2  # Rosenbrock function\n",
    "    \n",
    "    def grad_f(x, y):\n",
    "        dx = -40 * x * (y - x**2) - 2 * (1-x)\n",
    "        dy = 20 * (y - x**2)\n",
    "        return np.array([dx, dy])\n",
    "    \n",
    "    # Create grid of points\n",
    "    x = np.linspace(-2, 2, 100)\n",
    "    y = np.linspace(-1, 3, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    \n",
    "    # Starting point\n",
    "    start_x, start_y = -1.5, 2.5\n",
    "    \n",
    "    # Run different optimizers\n",
    "    optimizers = {\n",
    "        'GD': {'lr': 0.0002, 'path': ([start_x], [start_y])},\n",
    "        'Momentum': {'lr': 0.0002, 'beta': 0.9, \n",
    "                    'path': ([start_x], [start_y])},\n",
    "        'Adam': {'lr': 0.01, 'path': ([start_x], [start_y])}\n",
    "    }\n",
    "    \n",
    "    # Run optimization\n",
    "    for _ in range(200):\n",
    "        for name, opt in optimizers.items():\n",
    "            x, y = opt['path'][0][-1], opt['path'][1][-1]\n",
    "            grad = grad_f(x, y)\n",
    "            \n",
    "            if name == 'GD':\n",
    "                new_x = x - opt['lr'] * grad[0]\n",
    "                new_y = y - opt['lr'] * grad[1]\n",
    "            \n",
    "            elif name == 'Momentum':\n",
    "                if 'velocity' not in opt:\n",
    "                    opt['velocity'] = np.zeros(2)\n",
    "                \n",
    "                opt['velocity'] = (opt['beta'] * opt['velocity'] + \n",
    "                                 opt['lr'] * grad)\n",
    "                new_x = x - opt['velocity'][0]\n",
    "                new_y = y - opt['velocity'][1]\n",
    "            \n",
    "            elif name == 'Adam':\n",
    "                if 'm' not in opt:\n",
    "                    opt['m'] = np.zeros(2)  # First moment\n",
    "                    opt['v'] = np.zeros(2)  # Second moment\n",
    "                    opt['t'] = 0\n",
    "                \n",
    "                opt['t'] += 1\n",
    "                beta1, beta2 = 0.9, 0.999\n",
    "                \n",
    "                # Update moments\n",
    "                opt['m'] = beta1 * opt['m'] + (1-beta1) * grad\n",
    "                opt['v'] = beta2 * opt['v'] + (1-beta2) * grad**2\n",
    "                \n",
    "                # Bias correction\n",
    "                m_hat = opt['m'] / (1 - beta1**opt['t'])\n",
    "                v_hat = opt['v'] / (1 - beta2**opt['t'])\n",
    "                \n",
    "                # Update\n",
    "                new_x = x - opt['lr'] * m_hat[0] / (np.sqrt(v_hat[0]) + 1e-8)\n",
    "                new_y = y - opt['lr'] * m_hat[1] / (np.sqrt(v_hat[1]) + 1e-8)\n",
    "            \n",
    "            opt['path'][0].append(new_x)\n",
    "            opt['path'][1].append(new_y)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.contour(X, Y, Z, levels=50)\n",
    "    plt.colorbar(label='f(x, y)')\n",
    "    \n",
    "    colors = {'GD': 'r', 'Momentum': 'b', 'Adam': 'g'}\n",
    "\n",
    "    for name, opt in optimizers.items():\n",
    "    path_x, path_y = opt['path']\n",
    "    plt.plot(path_x, path_y, f\"{colors[name]}.-\", \n",
    "            label=name, alpha=0.7)\n",
    "    plt.plot(path_x[0], path_y[0], f\"{colors[name]}o\")\n",
    "\n",
    "    plt.plot(1, 1, 'ko', label='Global minimum')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Optimizer Comparison on Rosenbrock Function')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "compare_optimizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Understanding the Results\n",
    "\n",
    "The Rosenbrock function (also known as the banana function due to its shape) provides an excellent test case for optimization algorithms because it has a global minimum inside a long, narrow, parabolic valley. While finding the valley is easy, converging to the global minimum is difficult.\n",
    "\n",
    "Let's analyze how each optimizer performs:\n",
    "\n",
    "1. **Gradient Descent (Red Path)**\n",
    "   - Takes small, cautious steps\n",
    "   - Often zigzags across the valley\n",
    "   - Struggles with the different scales of the parameters\n",
    "   - Slowest to converge\n",
    "\n",
    "2. **Momentum (Blue Path)**\n",
    "   - Builds up velocity in the direction of the valley\n",
    "   - Reduces oscillation compared to pure gradient descent\n",
    "   - Can overshoot and need to correct course\n",
    "   - Better than GD but still not optimal\n",
    "\n",
    "3. **Adam (Green Path)**\n",
    "   - Adapts learning rates for each parameter\n",
    "   - Combines benefits of momentum with adaptive scaling\n",
    "   - Handles the curved valley most efficiently\n",
    "   - Generally converges fastest\n",
    "\n",
    "### 3.6 When to Use Each Optimizer\n",
    "\n",
    "The choice of optimizer often depends on your specific problem:\n",
    "\n",
    "1. **SGD with Momentum**\n",
    "   - Good for problems with consistent gradient directions\n",
    "   - Often works well for CNN architectures\n",
    "   - Can achieve slightly better final accuracy in some cases\n",
    "   - Learning rate scheduling is often important\n",
    "\n",
    "2. **Adam**\n",
    "   - Excellent default choice for most problems\n",
    "   - Particularly good for:\n",
    "     - Training deep networks\n",
    "     - Sparse gradients (NLP tasks)\n",
    "     - Noisy gradients\n",
    "   - May generalize slightly worse than SGD in some cases\n",
    "\n",
    "3. **RMSprop**\n",
    "   - Good alternative to Adam\n",
    "   - Sometimes preferred for RNN architectures\n",
    "   - Can be more robust in some cases\n",
    "   - Often requires less memory than Adam\n",
    "\n",
    "Let's look at a practical comparison of convergence rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_convergence_comparison():\n",
    "    \"\"\"Compare convergence rates of different optimizers.\"\"\"\n",
    "    # Create a simple quadratic problem\n",
    "    def f(x):\n",
    "        return x[0]**2 + 10*x[1]**2\n",
    "    \n",
    "    def grad_f(x):\n",
    "        return np.array([2*x[0], 20*x[1]])\n",
    "    \n",
    "    # Initialize starting point and optimizers\n",
    "    x0 = np.array([1.0, 1.0])\n",
    "    n_iterations = 100\n",
    "    \n",
    "    optimizers = {\n",
    "        'GD': {'lr': 0.05, 'x': x0.copy(), 'loss': []},\n",
    "        'Momentum': {\n",
    "            'lr': 0.05, 'beta': 0.9, \n",
    "            'velocity': np.zeros_like(x0),\n",
    "            'x': x0.copy(), 'loss': []\n",
    "        },\n",
    "        'Adam': {\n",
    "            'lr': 0.1, 'x': x0.copy(), 'loss': [],\n",
    "            'm': np.zeros_like(x0), 'v': np.zeros_like(x0),\n",
    "            'beta1': 0.9, 'beta2': 0.999, 't': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Run optimization\n",
    "    for _ in range(n_iterations):\n",
    "        for name, opt in optimizers.items():\n",
    "            x = opt['x']\n",
    "            grad = grad_f(x)\n",
    "            \n",
    "            if name == 'GD':\n",
    "                x = x - opt['lr'] * grad\n",
    "            \n",
    "            elif name == 'Momentum':\n",
    "                opt['velocity'] = (opt['beta'] * opt['velocity'] + \n",
    "                                 opt['lr'] * grad)\n",
    "                x = x - opt['velocity']\n",
    "            \n",
    "            elif name == 'Adam':\n",
    "                opt['t'] += 1\n",
    "                opt['m'] = (opt['beta1'] * opt['m'] + \n",
    "                          (1-opt['beta1']) * grad)\n",
    "                opt['v'] = (opt['beta2'] * opt['v'] + \n",
    "                          (1-opt['beta2']) * grad**2)\n",
    "                \n",
    "                m_hat = opt['m'] / (1 - opt['beta1']**opt['t'])\n",
    "                v_hat = opt['v'] / (1 - opt['beta2']**opt['t'])\n",
    "                \n",
    "                x = x - opt['lr'] * m_hat / (np.sqrt(v_hat) + 1e-8)\n",
    "            \n",
    "            opt['x'] = x\n",
    "            opt['loss'].append(f(x))\n",
    "    \n",
    "    # Plot convergence\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for name, opt in optimizers.items():\n",
    "        plt.plot(opt['loss'], label=name)\n",
    "    \n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.title('Convergence Comparison of Optimizers')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_convergence_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convergence plot reveals several important patterns:\n",
    "\n",
    "1. **Initial Progress**\n",
    "   - Adam typically makes rapid progress in early iterations\n",
    "   - Momentum takes time to build up speed\n",
    "   - Basic gradient descent is consistently slower\n",
    "\n",
    "2. **Final Convergence**\n",
    "   - All methods eventually find the minimum\n",
    "   - The adaptive methods (Adam) might oscillate slightly more at the end\n",
    "   - Momentum can achieve very precise convergence with patience\n",
    "\n",
    "3. **Stability**\n",
    "   - Adam shows the most stable convergence path\n",
    "   - Momentum can have larger oscillations\n",
    "   - Basic gradient descent is stable but slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"stochastic-methods\"></a>\n",
    "## 4. Stochastic Methods and Mini-batching\n",
    "\n",
    "In practice, we rarely compute gradients over the entire dataset. Instead, we use mini-batches of data to estimate gradients. This introduces noise but brings several benefits:\n",
    "\n",
    "1. **Computational Efficiency**\n",
    "   - Processing small batches is faster than full dataset passes\n",
    "   - Enables training on datasets too large to fit in memory\n",
    "\n",
    "2. **Regularization Effect**\n",
    "   - Noise in gradients can help escape poor local minima\n",
    "   - Can improve generalization performance\n",
    "\n",
    "3. **Online Learning**\n",
    "   - Can update model with new data as it arrives\n",
    "   - Useful for streaming scenarios\n",
    "\n",
    "Let's implement a simple example of stochastic vs batch gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compare_batch_sizes():\n",
    "    \"\"\"Compare different batch sizes in gradient descent.\"\"\"\n",
    "    # Generate synthetic dataset\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    X = np.random.randn(n_samples, 2)\n",
    "    y = 3*X[:, 0] + 2*X[:, 1] + np.random.randn(n_samples) * 0.1\n",
    "    \n",
    "    # Initialize parameters\n",
    "    theta = np.zeros(2)\n",
    "    learning_rate = 0.01\n",
    "    n_epochs = 50\n",
    "    \n",
    "    batch_sizes = [1, 32, n_samples]  # SGD, mini-batch, full batch\n",
    "    histories = {}\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        theta_history = [theta.copy()]\n",
    "        loss_history = []\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Shuffle data\n",
    "            idx = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[idx]\n",
    "            y_shuffled = y[idx]\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Compute gradient\n",
    "                pred = X_batch @ theta\n",
    "                grad = X_batch.T @ (pred - y_batch) / len(X_batch)\n",
    "                \n",
    "                # Update parameters\n",
    "                theta = theta - learning_rate * grad\n",
    "                theta_history.append(theta.copy())\n",
    "            \n",
    "            # Compute full loss for monitoring\n",
    "            pred = X @ theta\n",
    "            loss = np.mean((pred - y)**2)\n",
    "            loss_history.append(loss)\n",
    "        \n",
    "        histories[batch_size] = {\n",
    "            'theta': theta_history,\n",
    "            'loss': loss_history\n",
    "        }\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot parameter trajectories\n",
    "    for batch_size, hist in histories.items():\n",
    "        theta_hist = np.array(hist['theta'])\n",
    "        label = 'SGD' if batch_size == 1 else \\\n",
    "                'Mini-batch' if batch_size == 32 else 'Full batch'\n",
    "        ax1.plot(theta_hist[:, 0], theta_hist[:, 1], '.-', \n",
    "                label=f'{label} (batch={batch_size})', alpha=0.5)\n",
    "    \n",
    "    ax1.plot(3, 2, 'r*', label='True parameters')\n",
    "    ax1.set_xlabel('θ₁')\n",
    "    ax1.set_ylabel('θ₂')\n",
    "    ax1.set_title('Parameter Trajectories')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot loss curves\n",
    "    for batch_size, hist in histories.items():\n",
    "        label = 'SGD' if batch_size == 1 else \\\n",
    "                'Mini-batch' if batch_size == 32 else 'Full batch'\n",
    "        ax2.plot(hist['loss'], \n",
    "                label=f'{label} (batch={batch_size})')\n",
    "    \n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss (log scale)')\n",
    "    ax2.set_title('Convergence Comparison')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_batch_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison reveals important tradeoffs:\n",
    "\n",
    "1. **Stochastic Gradient Descent (batch_size=1)**\n",
    "   - Noisiest parameter updates\n",
    "   - Most frequent parameter updates\n",
    "   - Can make rapid initial progress\n",
    "   - May never fully converge to optimum\n",
    "\n",
    "2. **Mini-batch Gradient Descent (batch_size=32)**\n",
    "   - Balance between update frequency and gradient accuracy\n",
    "   - More stable than SGD but still noisy\n",
    "   - Often best practical choice\n",
    "\n",
    "3. **Full Batch Gradient Descent (batch_size=n_samples)**\n",
    "   - Most accurate gradient estimates\n",
    "   - Smoothest optimization path\n",
    "   - Computationally expensive\n",
    "   - Can get stuck in poor local minima\n",
    "\n",
    "### 4.1 Choosing Batch Size\n",
    "\n",
    "Several factors influence the choice of batch size:\n",
    "\n",
    "1. **Hardware Constraints**\n",
    "   - GPU/TPU memory limits\n",
    "   - Parallelization efficiency\n",
    "   - Memory bandwidth\n",
    "\n",
    "2. **Statistical Efficiency**\n",
    "   - Larger batches give more reliable gradients\n",
    "   - Smaller batches provide regularization effect\n",
    "   - Diminishing returns beyond certain sizes\n",
    "\n",
    "3. **Optimization Dynamics**\n",
    "   - Larger batches allow larger learning rates\n",
    "   - Smaller batches might require learning rate scaling\n",
    "   - Batch size affects momentum dynamics\n",
    "\n",
    "Here's a practical guide for batch size selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def batch_size_effects():\n",
    "    \"\"\"Visualize how batch size affects gradient noise and convergence.\"\"\"\n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    X = np.random.randn(n_samples, 2)\n",
    "    y = 3*X[:, 0] + 2*X[:, 1] + np.random.randn(n_samples) * 0.1\n",
    "    \n",
    "    batch_sizes = [1, 8, 32, 128]\n",
    "    \n",
    "    # Compute gradients at a fixed point\n",
    "    theta = np.array([2.5, 1.5])  # Some point during optimization\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, batch_size in enumerate(batch_sizes, 1):\n",
    "        gradients = []\n",
    "        \n",
    "        # Compute multiple gradient estimates\n",
    "        for _ in range(100):\n",
    "            idx = np.random.choice(n_samples, batch_size)\n",
    "            X_batch = X[idx]\n",
    "            y_batch = y[idx]\n",
    "            \n",
    "            pred = X_batch @ theta\n",
    "            grad = X_batch.T @ (pred - y_batch) / batch_size\n",
    "            gradients.append(grad)\n",
    "        \n",
    "        gradients = np.array(gradients)\n",
    "        \n",
    "        # Plot gradient distribution\n",
    "        plt.subplot(1, len(batch_sizes), i)\n",
    "        plt.scatter(gradients[:, 0], gradients[:, 1], \n",
    "                   alpha=0.5, s=20)\n",
    "        plt.title(f'Batch Size = {batch_size}')\n",
    "        plt.xlabel('∂L/∂θ₁')\n",
    "        plt.ylabel('∂L/∂θ₂')\n",
    "        \n",
    "        # Add true gradient\n",
    "        true_grad = X.T @ (X @ theta - y) / n_samples\n",
    "        plt.plot(true_grad[0], true_grad[1], 'r*', \n",
    "                label='True gradient', markersize=10)\n",
    "        \n",
    "        if i == 1:\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.grid(True)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "batch_size_effects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization shows how batch size affects gradient estimates:\n",
    "\n",
    "1. **Small Batches (1 or 8)**\n",
    "   - High variance in gradient estimates\n",
    "   - More exploration of parameter space\n",
    "   - Might require smaller learning rates\n",
    "\n",
    "2. **Medium Batches (32)**\n",
    "   - Good balance of noise and accuracy\n",
    "   - Efficient hardware utilization\n",
    "   - Common choice in practice\n",
    "\n",
    "3. **Large Batches (128+)**\n",
    "   - More accurate gradient estimates\n",
    "   - Might need specialized training techniques\n",
    "   - Can lead to poorer generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"beyond\"></a>\n",
    "## 5. Beyond Gradient Descent\n",
    "\n",
    "While gradient-based methods dominate deep learning optimization, other approaches can be valuable in certain scenarios:\n",
    "\n",
    "### 5.1 Evolutionary Strategies\n",
    "\n",
    "Evolutionary algorithms provide a gradient-free alternative, particularly useful when:\n",
    "- The objective function is non-differentiable\n",
    "- The problem has discrete parameters\n",
    "- Parallelization is highly important\n",
    "\n",
    "Let's implement a simple evolutionary strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EvolutionaryOptimizer:\n",
    "    def __init__(self, param_shape, pop_size=50, sigma=0.1):\n",
    "        \"\"\"\n",
    "        Simple evolutionary strategy optimizer.\n",
    "        \n",
    "        Args:\n",
    "            param_shape: Shape of parameters to optimize\n",
    "            pop_size: Number of random variations to try\n",
    "            sigma: Standard deviation of noise for mutations\n",
    "        \"\"\"\n",
    "        self.param_shape = param_shape\n",
    "        self.pop_size = pop_size\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def optimize(self, objective_fn, n_iterations=100):\n",
    "        \"\"\"\n",
    "        Optimize using random parameter perturbations.\n",
    "        \n",
    "        Args:\n",
    "            objective_fn: Function to minimize\n",
    "            n_iterations: Number of generations\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Best parameters found and history of best scores\n",
    "        \"\"\"\n",
    "        # Initialize random parameters\n",
    "        theta = np.random.randn(*self.param_shape)\n",
    "        best_score = float('inf')\n",
    "        history = []\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            # Generate population of random perturbations\n",
    "            noise = np.random.randn(self.pop_size, *self.param_shape)\n",
    "            \n",
    "            # Evaluate fitness for each perturbation\n",
    "            scores = np.array([\n",
    "                objective_fn(theta + self.sigma * noise[i])\n",
    "                for i in range(self.pop_size)\n",
    "            ])\n",
    "            \n",
    "            # Compute weighted average of perturbations\n",
    "            weights = self.compute_weights(scores)\n",
    "            update = np.sum(weights[:, None] * noise, axis=0)\n",
    "            \n",
    "            # Update parameters\n",
    "            theta = theta + self.sigma * update\n",
    "            \n",
    "            # Track best score\n",
    "            current_score = objective_fn(theta)\n",
    "            if current_score < best_score:\n",
    "                best_score = current_score\n",
    "            history.append(best_score)\n",
    "            \n",
    "        return theta, history\n",
    "    \n",
    "    def compute_weights(self, scores):\n",
    "        \"\"\"\n",
    "        Compute weights for parameter updates based on scores.\n",
    "        Uses rank-based weights favoring better performing perturbations.\n",
    "        \"\"\"\n",
    "        n = len(scores)\n",
    "        ranks = (-scores).argsort().argsort()  # Convert scores to ranks\n",
    "        weights = np.maximum(0, np.log(n/2 + 1) - np.log(ranks + 1))\n",
    "        return weights / weights.sum()\n",
    "\n",
    "def compare_evolution_vs_gradient():\n",
    "    \"\"\"Compare evolutionary strategy with gradient descent.\"\"\"\n",
    "    # Define a challenging objective function\n",
    "    def objective(x):\n",
    "        \"\"\"Rastrigin function - has many local minima.\"\"\"\n",
    "        n = len(x)\n",
    "        return 10*n + sum(x**2 - 10*np.cos(2*np.pi*x))\n",
    "    \n",
    "    def gradient(x):\n",
    "        \"\"\"Gradient of Rastrigin function.\"\"\"\n",
    "        return 2*x + 20*np.pi*np.sin(2*np.pi*x)\n",
    "    \n",
    "    # Initialize optimizers\n",
    "    param_shape = (2,)  # 2D problem\n",
    "    evo_opt = EvolutionaryOptimizer(param_shape, pop_size=50, sigma=0.1)\n",
    "    \n",
    "    # Starting point for gradient descent\n",
    "    x_gd = np.random.randn(2)\n",
    "    lr = 0.01\n",
    "    \n",
    "    # Run optimization\n",
    "    n_iterations = 100\n",
    "    gd_history = []\n",
    "    \n",
    "    # Gradient descent\n",
    "    for _ in range(n_iterations):\n",
    "        score = objective(x_gd)\n",
    "        gd_history.append(score)\n",
    "        grad = gradient(x_gd)\n",
    "        x_gd = x_gd - lr * grad\n",
    "    \n",
    "    # Evolutionary strategy\n",
    "    _, evo_history = evo_opt.optimize(objective, n_iterations)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot optimization landscape\n",
    "    plt.subplot(121)\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    y = np.linspace(-5, 5, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = objective([X, Y])\n",
    "    \n",
    "    plt.contour(X, Y, Z, levels=50)\n",
    "    plt.colorbar(label='Objective value')\n",
    "    plt.xlabel('x₁')\n",
    "    plt.ylabel('x₂')\n",
    "    plt.title('Rastrigin Function\\n(Many Local Minima)')\n",
    "    \n",
    "    # Plot convergence\n",
    "    plt.subplot(122)\n",
    "    plt.plot(gd_history, 'b-', label='Gradient Descent')\n",
    "    plt.plot(evo_history, 'r-', label='Evolution Strategy')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Objective Value (log scale)')\n",
    "    plt.title('Optimization Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_evolution_vs_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Understanding Evolutionary Strategies\n",
    "\n",
    "Evolutionary strategies offer several unique advantages compared to gradient-based methods:\n",
    "\n",
    "1. **Gradient-Free Operation**\n",
    "   - No need for derivative computations\n",
    "   - Can work with non-differentiable objectives\n",
    "   - Handles discrete or discontinuous parameters\n",
    "\n",
    "2. **Global Optimization**\n",
    "   - Can escape local minima more easily\n",
    "   - Explores the parameter space more broadly\n",
    "   - Less sensitive to initialization\n",
    "\n",
    "3. **Natural Parallelization**\n",
    "   - Population members can be evaluated independently\n",
    "   - Scales well with parallel computing resources\n",
    "   - Good for distributed optimization\n",
    "\n",
    "Let's examine how evolutionary strategies work:\n",
    "\n",
    "1. **Population Generation**\n",
    "   - Create multiple parameter variations\n",
    "   - Each variation is a \"candidate solution\"\n",
    "   - Variations are random perturbations of current best\n",
    "\n",
    "2. **Fitness Evaluation**\n",
    "   - Evaluate objective function for each candidate\n",
    "   - No gradients needed\n",
    "   - Can use any performance metric\n",
    "\n",
    "3. **Selection and Update**\n",
    "   - Better performing candidates influence next generation more\n",
    "   - Updates combine information from multiple candidates\n",
    "   - Natural tendency to improve over time\n",
    "\n",
    "### 5.3 When to Use Evolutionary Strategies\n",
    "\n",
    "Evolutionary strategies are particularly useful in several scenarios:\n",
    "\n",
    "1. **Black-Box Optimization**\n",
    "   - When objective function derivatives are unavailable\n",
    "   - For optimizing external systems or simulations\n",
    "   - When evaluating discrete choices\n",
    "\n",
    "2. **Highly Non-Convex Problems**\n",
    "   - Many local minima\n",
    "   - Discontinuous objectives\n",
    "   - Multi-modal optimization\n",
    "\n",
    "3. **Distributed Computing**\n",
    "   - When massive parallelization is available\n",
    "   - For expensive fitness evaluations\n",
    "   - In cloud computing environments\n",
    "\n",
    "Let's implement a more sophisticated example showing these advantages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ParallelEvolutionaryOptimizer:\n",
    "    \"\"\"Enhanced evolutionary optimizer with parallel evaluation.\"\"\"\n",
    "    def __init__(self, param_shape, pop_size=50, sigma=0.1, n_elite=5):\n",
    "        self.param_shape = param_shape\n",
    "        self.pop_size = pop_size\n",
    "        self.sigma = sigma\n",
    "        self.n_elite = n_elite\n",
    "        \n",
    "    def optimize(self, objective_fn, n_iterations=100):\n",
    "        \"\"\"\n",
    "        Optimize using parallel evolution strategy.\n",
    "        \n",
    "        Args:\n",
    "            objective_fn: Function to minimize\n",
    "            n_iterations: Number of generations\n",
    "        \"\"\"\n",
    "        # Initialize population\n",
    "        population = np.random.randn(self.pop_size, *self.param_shape)\n",
    "        best_ever = {'params': None, 'score': float('inf')}\n",
    "        history = []\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            # Evaluate population in parallel\n",
    "            scores = np.array([objective_fn(p) for p in population])\n",
    "            \n",
    "            # Track best solution\n",
    "            min_idx = np.argmin(scores)\n",
    "            if scores[min_idx] < best_ever['score']:\n",
    "                best_ever['score'] = scores[min_idx]\n",
    "                best_ever['params'] = population[min_idx].copy()\n",
    "            \n",
    "            # Select elite members\n",
    "            elite_idx = np.argsort(scores)[:self.n_elite]\n",
    "            elite = population[elite_idx]\n",
    "            \n",
    "            # Generate new population\n",
    "            new_population = np.zeros_like(population)\n",
    "            \n",
    "            # Keep elite members\n",
    "            new_population[:self.n_elite] = elite\n",
    "            \n",
    "            # Generate offspring\n",
    "            for i in range(self.n_elite, self.pop_size):\n",
    "                # Select random elite parent\n",
    "                parent = elite[np.random.randint(self.n_elite)]\n",
    "                # Add random mutation\n",
    "                new_population[i] = parent + self.sigma * np.random.randn(*self.param_shape)\n",
    "            \n",
    "            population = new_population\n",
    "            history.append(best_ever['score'])\n",
    "        \n",
    "        return best_ever['params'], history\n",
    "\n",
    "def visualize_multimodal_optimization():\n",
    "    \"\"\"Visualize optimization on a highly multimodal function.\"\"\"\n",
    "    # Define a complex multimodal function\n",
    "    def multimodal_objective(x):\n",
    "        \"\"\"Sum of multiple Gaussian peaks.\"\"\"\n",
    "        centers = np.array([[-2, -2], [2, 2], [-2, 2], [2, -2], [0, 0]])\n",
    "        return -sum(np.exp(-3 * np.sum((x - c)**2)) for c in centers)\n",
    "    \n",
    "    # Create optimizers\n",
    "    param_shape = (2,)\n",
    "    evo_opt = ParallelEvolutionaryOptimizer(param_shape, pop_size=100)\n",
    "    \n",
    "    # Run optimization multiple times\n",
    "    n_runs = 5\n",
    "    evo_histories = []\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        _, history = evo_opt.optimize(multimodal_objective, n_iterations=50)\n",
    "        evo_histories.append(history)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot landscape\n",
    "    ax1 = plt.subplot(121)\n",
    "    x = np.linspace(-4, 4, 100)\n",
    "    y = np.linspace(-4, 4, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.array([[multimodal_objective(np.array([xi, yi])) \n",
    "                   for xi in x] for yi in y])\n",
    "    \n",
    "    plt.contour(X, Y, Z, levels=50)\n",
    "    plt.colorbar(label='Objective value')\n",
    "    plt.xlabel('x₁')\n",
    "    plt.ylabel('x₂')\n",
    "    plt.title('Multimodal Landscape\\nMultiple Global Optima')\n",
    "    \n",
    "    # Plot convergence\n",
    "    ax2 = plt.subplot(122)\n",
    "    for i, history in enumerate(evo_histories):\n",
    "        plt.plot(history, alpha=0.5, label=f'Run {i+1}')\n",
    "    \n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Best Fitness')\n",
    "    plt.title('Multiple Optimization Runs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_multimodal_optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Other Alternative Optimization Methods\n",
    "\n",
    "Beyond evolutionary strategies, several other alternative optimization approaches deserve mention:\n",
    "\n",
    "1. **Bayesian Optimization**\n",
    "   - Builds a probabilistic model of the objective\n",
    "   - Particularly good for expensive evaluations\n",
    "   - Common in hyperparameter optimization\n",
    "\n",
    "2. **Simulated Annealing**\n",
    "   - Inspired by physical annealing processes\n",
    "   - Gradually reduces random exploration\n",
    "   - Good for discrete optimization problems\n",
    "\n",
    "3. **Particle Swarm Optimization**\n",
    "   - Population-based method inspired by swarm behavior\n",
    "   - Maintains velocity information for each particle\n",
    "   - Often used in continuous optimization\n",
    "\n",
    "Here's a basic implementation of simulated annealing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class SimulatedAnnealing:\n",
    "    def __init__(self, initial_temp=100, cooling_rate=0.95):\n",
    "        self.temp = initial_temp\n",
    "        self.cooling_rate = cooling_rate\n",
    "    \n",
    "    def optimize(self, objective_fn, initial_state, n_iterations=1000):\n",
    "        \"\"\"\n",
    "        Optimize using simulated annealing.\n",
    "        \n",
    "        Args:\n",
    "            objective_fn: Function to minimize\n",
    "            initial_state: Starting parameters\n",
    "            n_iterations: Number of iterations\n",
    "        \"\"\"\n",
    "        current_state = initial_state.copy()\n",
    "        current_energy = objective_fn(current_state)\n",
    "        \n",
    "        best_state = current_state.copy()\n",
    "        best_energy = current_energy\n",
    "        \n",
    "        history = [current_energy]\n",
    "        \n",
    "        for i in range(n_iterations):\n",
    "            # Generate neighbor\n",
    "            neighbor = current_state + np.random.randn(*current_state.shape) * self.temp\n",
    "            neighbor_energy = objective_fn(neighbor)\n",
    "            \n",
    "            # Decide whether to accept neighbor\n",
    "            delta_e = neighbor_energy - current_energy\n",
    "            if delta_e < 0 or np.random.random() < np.exp(-delta_e / self.temp):\n",
    "                current_state = neighbor\n",
    "                current_energy = neighbor_energy\n",
    "                \n",
    "                if current_energy < best_energy:\n",
    "                    best_state = current_state.copy()\n",
    "                    best_energy = current_energy\n",
    "            \n",
    "            # Cool temperature\n",
    "            self.temp *= self.cooling_rate\n",
    "            history.append(best_energy)\n",
    "        \n",
    "        return best_state, history\n",
    "\n",
    "def compare_alternative_methods():\n",
    "    \"\"\"Compare different optimization approaches.\"\"\"\n",
    "    # Define test function (Ackley function)\n",
    "    def ackley(x):\n",
    "        return (-20 * np.exp(-0.2 * np.sqrt(0.5 * (x[0]**2 + x[1]**2))) \n",
    "                - np.exp(0.5 * (np.cos(2*np.pi*x[0]) + np.cos(2*np.pi*x[1]))) \n",
    "                + np.e + 20)\n",
    "    \n",
    "    # Initialize optimizers\n",
    "    param_shape = (2,)\n",
    "    evo_opt = ParallelEvolutionaryOptimizer(param_shape)\n",
    "    sa_opt = SimulatedAnnealing()\n",
    "    \n",
    "    # Run optimization\n",
    "    initial_state = np.random.randn(2) * 2\n",
    "    \n",
    "    _, evo_history = evo_opt.optimize(ackley, n_iterations=100)\n",
    "    _, sa_history = sa_opt.optimize(ackley, initial_state, n_iterations=100)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot landscape\n",
    "    ax1 = plt.subplot(121)\n",
    "    x = np.linspace(-4, 4, 100)\n",
    "    y = np.linspace(-4, 4, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.array([[ackley(np.array([xi, yi])) for xi in x] for yi in y])\n",
    "    \n",
    "    plt.contour(X, Y, Z, levels=50)\n",
    "    plt.colorbar(label='Objective value')\n",
    "    plt.xlabel('x₁')\n",
    "    plt.ylabel('x₂')\n",
    "    plt.title('Ackley Function')\n",
    "    \n",
    "    # Plot convergence\n",
    "    ax2 = plt.subplot(122)\n",
    "    plt.plot(evo_history, label='Evolution Strategy')\n",
    "    plt.plot(sa_history, label='Simulated Annealing')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Best Objective Value')\n",
    "    plt.title('Optimization Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_alternative_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Choosing the Right Optimization Method\n",
    "\n",
    "The choice of optimization method depends on several factors:\n",
    "\n",
    "1. **Problem Characteristics**\n",
    "   - Differentiability of objective\n",
    "   - Number of local minima\n",
    "   - Constraint handling requirements\n",
    "   - Parameter dimensionality\n",
    "\n",
    "2. **Computational Resources**\n",
    "   - Available parallelization\n",
    "   - Memory constraints\n",
    "   - Time constraints\n",
    "   - Function evaluation cost\n",
    "\n",
    "3. **Problem Scale**\n",
    "   - Data size and dimensionality greatly influence our choice of optimization method. For instance, when dealing with high-dimensional problems like deep neural networks with millions of parameters, gradient-based methods often prove most efficient. However, for lower-dimensional problems with complex landscapes, evolutionary or Bayesian approaches might be more appropriate.\n",
    "   - In real-world applications, we must also consider whether our problem requires online learning (updating with new data) or batch learning (fixed dataset).\n",
    "\n",
    "4. **Domain Knowledge**\n",
    "   - Understanding the structure of our optimization problem can guide our choice of method. For example, if we know our objective function is convex, we can confidently use gradient descent. If we know our problem has many local minima, we might prefer evolutionary strategies or simulated annealing.\n",
    "   - Domain expertise also helps in setting appropriate constraints and choosing meaningful parameter ranges.\n",
    "\n",
    "Let's implement a decision helper that demonstrates these considerations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class OptimizationAdvisor:\n",
    "    \"\"\"Helper class to suggest appropriate optimization methods.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.method_properties = {\n",
    "            'gradient_descent': {\n",
    "                'requires_gradients': True,\n",
    "                'handles_constraints': False,\n",
    "                'parallelizable': False,\n",
    "                'memory_efficient': True,\n",
    "                'handles_local_minima': False,\n",
    "                'online_learning': True\n",
    "            },\n",
    "            'adam': {\n",
    "                'requires_gradients': True,\n",
    "                'handles_constraints': False,\n",
    "                'parallelizable': False,\n",
    "                'memory_efficient': False,\n",
    "                'handles_local_minima': False,\n",
    "                'online_learning': True\n",
    "            },\n",
    "            'evolutionary': {\n",
    "                'requires_gradients': False,\n",
    "                'handles_constraints': True,\n",
    "                'parallelizable': True,\n",
    "                'memory_efficient': False,\n",
    "                'handles_local_minima': True,\n",
    "                'online_learning': False\n",
    "            },\n",
    "            'bayesian': {\n",
    "                'requires_gradients': False,\n",
    "                'handles_constraints': True,\n",
    "                'parallelizable': False,\n",
    "                'memory_efficient': True,\n",
    "                'handles_local_minima': True,\n",
    "                'online_learning': False\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def suggest_method(self, problem_description):\n",
    "        \"\"\"\n",
    "        Suggest optimization method based on problem characteristics.\n",
    "        \n",
    "        Args:\n",
    "            problem_description: Dict containing problem properties\n",
    "        \n",
    "        Returns:\n",
    "            List of recommended methods with explanations\n",
    "        \"\"\"\n",
    "        scores = self._score_methods(problem_description)\n",
    "        recommendations = []\n",
    "        \n",
    "        for method, score in sorted(scores.items(), \n",
    "                                  key=lambda x: x[1], \n",
    "                                  reverse=True):\n",
    "            explanation = self._generate_explanation(\n",
    "                method, problem_description)\n",
    "            recommendations.append({\n",
    "                'method': method,\n",
    "                'score': score,\n",
    "                'explanation': explanation\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _score_methods(self, problem):\n",
    "        \"\"\"Score each method based on problem requirements.\"\"\"\n",
    "        scores = {}\n",
    "        for method, properties in self.method_properties.items():\n",
    "            score = 0\n",
    "            # Add scoring logic based on problem requirements\n",
    "            if problem.get('has_gradients', True) == \\\n",
    "               properties['requires_gradients']:\n",
    "                score += 1\n",
    "            if problem.get('needs_constraints', False) == \\\n",
    "               properties['handles_constraints']:\n",
    "                score += 1\n",
    "            if problem.get('parallel_compute', False) == \\\n",
    "               properties['parallelizable']:\n",
    "                score += 1\n",
    "            if problem.get('memory_limited', False) == \\\n",
    "               properties['memory_efficient']:\n",
    "                score += 1\n",
    "            scores[method] = score\n",
    "        return scores\n",
    "    \n",
    "    def _generate_explanation(self, method, problem):\n",
    "        \"\"\"Generate explanation for method recommendation.\"\"\"\n",
    "        props = self.method_properties[method]\n",
    "        explanation = f\"Recommended {method} because:\\n\"\n",
    "        \n",
    "        if method == 'gradient_descent':\n",
    "            explanation += (\"- Simple and memory efficient\\n\"\n",
    "                          \"- Works well with convex problems\\n\"\n",
    "                          \"- Good for online learning\")\n",
    "        elif method == 'adam':\n",
    "            explanation += (\"- Adaptive learning rates\\n\"\n",
    "                          \"- Handles non-stationary objectives\\n\"\n",
    "                          \"- Good for deep learning\")\n",
    "        elif method == 'evolutionary':\n",
    "            explanation += (\"- No gradients required\\n\"\n",
    "                          \"- Highly parallelizable\\n\"\n",
    "                          \"- Good for multi-modal problems\")\n",
    "        elif method == 'bayesian':\n",
    "            explanation += (\"- Sample efficient\\n\"\n",
    "                          \"- Handles expensive evaluations\\n\"\n",
    "                          \"- Good for hyperparameter tuning\")\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "# Let's demonstrate the advisor with some example problems\n",
    "advisor = OptimizationAdvisor()\n",
    "\n",
    "# Example 1: Deep Learning Problem\n",
    "deep_learning_problem = {\n",
    "    'has_gradients': True,\n",
    "    'needs_constraints': False,\n",
    "    'parallel_compute': True,\n",
    "    'memory_limited': False\n",
    "}\n",
    "\n",
    "# Example 2: Hyperparameter Optimization\n",
    "hyperparameter_problem = {\n",
    "    'has_gradients': False,\n",
    "    'needs_constraints': True,\n",
    "    'parallel_compute': False,\n",
    "    'memory_limited': True\n",
    "}\n",
    "\n",
    "def print_recommendations(problem_type, problem):\n",
    "    print(f\"\\nRecommendations for {problem_type}:\")\n",
    "    recommendations = advisor.suggest_method(problem)\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"\\n{i}. {rec['method'].upper()} (Score: {rec['score']})\")\n",
    "        print(rec['explanation'])\n",
    "\n",
    "print_recommendations(\"Deep Learning\", deep_learning_problem)\n",
    "print_recommendations(\"Hyperparameter Optimization\", hyperparameter_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Practical Implementation Guidelines\n",
    "\n",
    "When implementing optimization in practice, consider these important guidelines:\n",
    "\n",
    "1. **Initialization Strategy**\n",
    "   \n",
    "Starting points can significantly impact optimization success. Here's a helper function to demonstrate different initialization strategies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def demonstrate_initialization_strategies():\n",
    "    \"\"\"Show the impact of different initialization strategies.\"\"\"\n",
    "    \n",
    "    def objective(x):\n",
    "        \"\"\"A challenging objective function with multiple basins.\"\"\"\n",
    "        return np.sin(5*x[0])*np.cos(5*x[1])*np.exp(-0.1*(x[0]**2 + x[1]**2))\n",
    "    \n",
    "    # Different initialization strategies\n",
    "    strategies = {\n",
    "        'random_normal': lambda n: np.random.randn(n, 2),\n",
    "        'random_uniform': lambda n: np.random.uniform(-2, 2, (n, 2)),\n",
    "        'grid': lambda n: np.array([(x, y) \n",
    "            for x in np.linspace(-2, 2, int(np.sqrt(n)))\n",
    "            for y in np.linspace(-2, 2, int(np.sqrt(n)))]),\n",
    "        'latin_hypercube': lambda n: scipy.stats.qmc.LatinHypercube(2).random(n)*4-2\n",
    "    }\n",
    "    \n",
    "    # Visualize different strategies\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    x = np.linspace(-2, 2, 100)\n",
    "    y = np.linspace(-2, 2, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = objective([X, Y])\n",
    "    \n",
    "    for i, (name, strategy) in enumerate(strategies.items()):\n",
    "        axes[i].contour(X, Y, Z, levels=50)\n",
    "        \n",
    "        # Generate and plot initialization points\n",
    "        points = strategy(25)\n",
    "        axes[i].scatter(points[:, 0], points[:, 1], \n",
    "                       color='red', alpha=0.6)\n",
    "        \n",
    "        axes[i].set_title(f'{name} Initialization')\n",
    "        axes[i].set_xlabel('x₁')\n",
    "        axes[i].set_ylabel('x₂')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_initialization_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Learning Rate Scheduling**\n",
    "\n",
    "Learning rate adaptation can significantly improve optimization performance. Here's an implementation of common scheduling strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LearningRateScheduler:\n",
    "    \"\"\"Implements various learning rate scheduling strategies.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def step_decay(initial_lr, epoch, drop=0.5, epochs_drop=10):\n",
    "        \"\"\"Step decay: reduce learning rate by half every epochs_drop.\"\"\"\n",
    "        return initial_lr * np.power(drop, np.floor(epoch/epochs_drop))\n",
    "    \n",
    "    @staticmethod\n",
    "    def exponential_decay(initial_lr, epoch, decay=0.95):\n",
    "        \"\"\"Exponential decay of learning rate.\"\"\"\n",
    "        return initial_lr * np.power(decay, epoch)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_decay(initial_lr, epoch, total_epochs):\n",
    "        \"\"\"Cosine annealing of learning rate.\"\"\"\n",
    "        return initial_lr * 0.5 * (1 + np.cos(epoch/total_epochs * np.pi))\n",
    "    \n",
    "    @staticmethod\n",
    "    def warm_up_linear_decay(initial_lr, epoch, \n",
    "                            warmup_epochs=5, total_epochs=100):\n",
    "        \"\"\"Linear warm up followed by linear decay.\"\"\"\n",
    "        if epoch < warmup_epochs:\n",
    "            return initial_lr * (epoch + 1) / warmup_epochs\n",
    "        return initial_lr * (1 - (epoch - warmup_epochs)/(total_epochs - warmup_epochs))\n",
    "\n",
    "def visualize_lr_schedules():\n",
    "    \"\"\"Visualize different learning rate scheduling strategies.\"\"\"\n",
    "    epochs = np.arange(100)\n",
    "    initial_lr = 0.1\n",
    "    \n",
    "    scheduler = LearningRateScheduler()\n",
    "    schedules = {\n",
    "        'Step Decay': lambda e: scheduler.step_decay(initial_lr, e),\n",
    "        'Exponential': lambda e: scheduler.exponential_decay(initial_lr, e),\n",
    "        'Cosine': lambda e: scheduler.cosine_decay(initial_lr, e, 100),\n",
    "        'Warm Up + Linear': lambda e: scheduler.warm_up_linear_decay(initial_lr, e)\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, schedule in schedules.items():\n",
    "        lr_schedule = [schedule(e) for e in epochs]\n",
    "        plt.plot(epochs, lr_schedule, label=name)\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Scheduling Strategies')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "\n",
    "visualize_lr_schedules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Monitoring and Early Stopping**\n",
    "\n",
    "Proper monitoring is crucial for optimization success. Here's a comprehensive monitoring system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class OptimizationMonitor:\n",
    "    \"\"\"\n",
    "    Monitors optimization progress and implements early stopping.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience=10, min_delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_value = float('inf')\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.history = {'loss': [], 'gradient_norm': [], \n",
    "                       'parameter_norm': []}\n",
    "    \n",
    "    def update(self, loss, gradient_norm, parameter_norm):\n",
    "        \"\"\"\n",
    "        Update monitoring statistics.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if should stop optimization\n",
    "        \"\"\"\n",
    "        self.history['loss'].append(loss)\n",
    "        self.history['gradient_norm'].append(gradient_norm)\n",
    "        self.history['parameter_norm'].append(parameter_norm)\n",
    "        \n",
    "        if loss < self.best_value - self.min_delta:\n",
    "            self.best_value = loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            \n",
    "        if self.wait >= self.patience:\n",
    "            self.stopped_epoch = len(self.history['loss'])\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def plot_statistics(self):\n",
    "        \"\"\"Visualize optimization statistics.\"\"\"\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[0].plot(self.history['loss'])\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_xlabel('Iteration')\n",
    "        axes[0].grid(True)\n",
    "        if self.stopped_epoch:\n",
    "            axes[0].axvline(self.stopped_epoch, \n",
    "                          color='r', linestyle='--',\n",
    "                          label='Early stopping')\n",
    "            axes[0].legend()\n",
    "        \n",
    "        # Plot gradient norm\n",
    "        axes[1].plot(self.history['gradient_norm'])\n",
    "        axes[1].set_ylabel('Gradient Norm')\n",
    "        axes[1].set_xlabel('Iteration')\n",
    "        axes[1].set_yscale('log')\n",
    "        axes[1].grid(True)\n",
    "        \n",
    "        # Plot parameter norm\n",
    "        axes[2].plot(self.history['parameter_norm'])\n",
    "        axes[2].set_ylabel('Parameter Norm')\n",
    "        axes[2].set_xlabel('Iteration')\n",
    "        axes[2].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate monitoring system\n",
    "def optimize_with_monitoring():\n",
    "    \"\"\"Example optimization with monitoring.\"\"\"\n",
    "    monitor = OptimizationMonitor(patience=20)\n",
    "    \n",
    "    # Simulate optimization\n",
    "    for i in range(100):\n",
    "        # Simulated loss that improves then plateaus\n",
    "        loss = 1.0/(1 + 0.1*i) + 0.1*np.random.randn()\n",
    "        gradient_norm = np.exp(-0.1*i) + 0.05*np.random.randn()\n",
    "        parameter_norm = 1 - np.exp(-0.05*i) + 0.02*np.random.randn()\n",
    "        \n",
    "        if monitor.update(loss, gradient_norm, parameter_norm):\n",
    "            print(f\"Early stopping triggered at iteration {i}\")\n",
    "            break\n",
    "    \n",
    "    monitor.plot_statistics()\n",
    "\n",
    "optimize_with_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Common Pitfalls and Solutions\n",
    "\n",
    "Understanding common optimization challenges and their solutions helps avoid common pitfalls:\n",
    "\n",
    "1. **Gradient Vanishing/Exploding**\n",
    "   - Problem: Gradients become too small or too large\n",
    "   - Solutions: \n",
    "     - Gradient clipping\n",
    "     - Proper initialization\n",
    "     - Layer normalization\n",
    "\n",
    "2. **Poor Conditioning**\n",
    "   - Problem: Different parameters require vastly different scales\n",
    "   - Solutions:\n",
    "     - Feature scaling\n",
    "     - Adaptive learning rates\n",
    "     - Preconditioning\n",
    "\n",
    "3. **Saddle Points**\n",
    "   - Problem: Optimization gets stuck at saddle points\n",
    "   - Solutions:\n",
    "     - Add noise to gradients\n",
    "     - Use momentum\n",
    "     - Implement trust region methods\n",
    "\n",
    "Let's implement some of these solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class OptimizationSafeguards:\n",
    "    \"\"\"\n",
    "    Implements various optimization safeguards and improvements.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clip_gradients(gradients, max_norm):\n",
    "        \"\"\"\n",
    "        Clip gradients to prevent explosion by scaling them when their norm exceeds max_norm.\n",
    "        \n",
    "        Args:\n",
    "            gradients: List of gradient arrays\n",
    "            max_norm: Maximum allowed gradient norm\n",
    "        \n",
    "        Returns:\n",
    "            List of clipped gradient arrays\n",
    "        \"\"\"\n",
    "        total_norm = np.sqrt(sum(np.sum(g**2) for g in gradients))\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        if clip_coef < 1:\n",
    "            return [g * clip_coef for g in gradients]\n",
    "        return gradients\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_gradient_noise(gradients, step, eta=1e-3):\n",
    "        \"\"\"\n",
    "        Add annealing Gaussian noise to gradients to help escape saddle points.\n",
    "        The noise magnitude decreases over time following a schedule.\n",
    "        \n",
    "        Args:\n",
    "            gradients: List of gradient arrays\n",
    "            step: Current optimization step\n",
    "            eta: Initial noise magnitude\n",
    "        \n",
    "        Returns:\n",
    "            List of gradients with added noise\n",
    "        \"\"\"\n",
    "        noise_std = eta/((1 + step)**0.55)\n",
    "        return [g + np.random.randn(*g.shape) * noise_std \n",
    "                for g in gradients]\n",
    "    \n",
    "    @staticmethod\n",
    "    def precondition_gradients(gradients, running_square_avg, \n",
    "                             beta=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Apply preconditioning to gradients to handle different parameter scales.\n",
    "        Uses a running average of squared gradients similar to RMSprop.\n",
    "        \n",
    "        Args:\n",
    "            gradients: List of gradient arrays\n",
    "            running_square_avg: List of running averages of squared gradients\n",
    "            beta: Decay rate for running averages\n",
    "            epsilon: Small constant for numerical stability\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (preconditioned gradients, updated running averages)\n",
    "        \"\"\"\n",
    "        # Update running averages of squared gradients\n",
    "        new_running_avg = [\n",
    "            beta * avg + (1 - beta) * (g**2)\n",
    "            for avg, g in zip(running_square_avg, gradients)\n",
    "        ]\n",
    "        \n",
    "        # Apply preconditioning\n",
    "        preconditioned = [\n",
    "            g / (np.sqrt(avg) + epsilon)\n",
    "            for g, avg in zip(gradients, new_running_avg)\n",
    "        ]\n",
    "        \n",
    "        return preconditioned, new_running_avg\n",
    "\n",
    "def demonstrate_safeguards():\n",
    "    \"\"\"\n",
    "    Demonstrate the effects of optimization safeguards on a challenging problem.\n",
    "    \"\"\"\n",
    "    # Create a pathological function with saddle points and different scales\n",
    "    def objective(x):\n",
    "        return (0.01 * x[0]**2 + 100 * x[1]**2 * \n",
    "                np.sin(0.1 * x[0]) * np.cos(0.1 * x[1]))\n",
    "    \n",
    "    def gradient(x):\n",
    "        dx0 = (0.02 * x[0] + \n",
    "               10 * x[1]**2 * np.cos(0.1 * x[0]) * np.cos(0.1 * x[1]))\n",
    "        dx1 = (200 * x[1] * np.sin(0.1 * x[0]) * np.cos(0.1 * x[1]) - \n",
    "               10 * x[1]**2 * np.sin(0.1 * x[0]) * np.sin(0.1 * x[1]))\n",
    "        return np.array([dx0, dx1])\n",
    "    \n",
    "    # Initialize optimization\n",
    "    safeguards = OptimizationSafeguards()\n",
    "    starting_points = np.random.randn(4, 2) * 5\n",
    "    \n",
    "    # Compare different combinations of safeguards\n",
    "    methods = {\n",
    "        'Basic GD': lambda g, t: g,\n",
    "        'Clipped': lambda g, t: safeguards.clip_gradients([g], 1.0)[0],\n",
    "        'With Noise': lambda g, t: safeguards.add_gradient_noise([g], t)[0],\n",
    "        'All Safeguards': lambda g, t: safeguards.add_gradient_noise(\n",
    "            safeguards.clip_gradients([g], 1.0), t)[0]\n",
    "    }\n",
    "    \n",
    "    # Run optimization\n",
    "    paths = {name: [] for name in methods}\n",
    "    \n",
    "    for name, transform in methods.items():\n",
    "        x = starting_points[list(methods.keys()).index(name)]\n",
    "        path = [x]\n",
    "        \n",
    "        for t in range(1000):\n",
    "            grad = gradient(x)\n",
    "            grad = transform(grad, t)\n",
    "            x = x - 0.01 * grad\n",
    "            path.append(x.copy())\n",
    "            \n",
    "            if np.linalg.norm(grad) < 1e-6:\n",
    "                break\n",
    "        \n",
    "        paths[name] = np.array(path)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Create contour plot of objective function\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    y = np.linspace(-5, 5, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = objective([X, Y])\n",
    "    \n",
    "    for i, (name, path) in enumerate(paths.items(), 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        plt.contour(X, Y, Z, levels=50)\n",
    "        plt.plot(path[:, 0], path[:, 1], '.-', label='Optimization path')\n",
    "        plt.plot(path[0, 0], path[0, 1], 'go', label='Start')\n",
    "        plt.plot(path[-1, 0], path[-1, 1], 'ro', label='End')\n",
    "        plt.title(f'{name} Gradient Descent')\n",
    "        plt.xlabel('x₁')\n",
    "        plt.ylabel('x₂')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_safeguards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Understanding the Safeguards\n",
    "\n",
    "Each optimization safeguard addresses specific challenges that can arise during optimization:\n",
    "\n",
    "1. **Gradient Clipping**\n",
    "   This technique prevents the \"exploding gradient\" problem by ensuring that gradient magnitudes stay within a reasonable range. When gradients become too large, they are scaled down proportionally while maintaining their direction. This is particularly important in deep learning where the chain rule can lead to exponentially large gradients.\n",
    "\n",
    "2. **Gradient Noise**\n",
    "   Adding carefully scaled noise to gradients serves multiple purposes:\n",
    "   - Helps escape saddle points by providing random perturbations\n",
    "   - Acts as a form of regularization\n",
    "   - Can improve generalization performance\n",
    "   The noise magnitude is annealed over time to allow for fine convergence in later stages of optimization.\n",
    "\n",
    "3. **Preconditioning**\n",
    "   This technique addresses the challenge of different parameters having different natural scales. By maintaining running averages of squared gradients, we can adaptively scale the updates for each parameter. This is similar to the ideas behind RMSprop and Adam optimizers.\n",
    "\n",
    "Let's examine how these safeguards affect optimization in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_safeguard_effects():\n",
    "    \"\"\"\n",
    "    Analyze the statistical effects of different optimization safeguards.\n",
    "    \"\"\"\n",
    "    # Generate synthetic gradients with pathological properties\n",
    "    n_samples = 1000\n",
    "    n_dims = 100\n",
    "    \n",
    "    # Create gradients with different scales\n",
    "    gradients = np.random.randn(n_samples, n_dims)\n",
    "    gradients[:, :n_dims//2] *= 0.01  # Small gradients\n",
    "    gradients[:, n_dims//2:] *= 100   # Large gradients\n",
    "    \n",
    "    safeguards = OptimizationSafeguards()\n",
    "    \n",
    "    # Analyze effects of different safeguards\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Original gradient distribution\n",
    "    plt.subplot(131)\n",
    "    plt.hist(gradients.flatten(), bins=50, alpha=0.5, \n",
    "             label='Original')\n",
    "    plt.title('Original Gradient Distribution')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Gradient Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Clipped gradients\n",
    "    clipped = safeguards.clip_gradients([gradients], 1.0)[0]\n",
    "    plt.subplot(132)\n",
    "    plt.hist(gradients.flatten(), bins=50, alpha=0.5, \n",
    "             label='Original')\n",
    "    plt.hist(clipped.flatten(), bins=50, alpha=0.5, \n",
    "             label='Clipped')\n",
    "    plt.title('Effect of Gradient Clipping')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Gradient Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Noisy gradients\n",
    "    noisy = safeguards.add_gradient_noise([gradients], 0)[0]\n",
    "    plt.subplot(133)\n",
    "    plt.hist(gradients.flatten(), bins=50, alpha=0.5, \n",
    "             label='Original')\n",
    "    plt.hist(noisy.flatten(), bins=50, alpha=0.5, \n",
    "             label='With Noise')\n",
    "    plt.title('Effect of Gradient Noise')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Gradient Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_safeguard_effects()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6.2 Implementing a Robust Optimizer\n",
    "\n",
    "Let's combine all these safeguards into a robust optimizer implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class RobustOptimizer:\n",
    "    \"\"\"\n",
    "    An optimizer that combines multiple safeguards for robust optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params, learning_rate=0.01, \n",
    "                 max_grad_norm=1.0, noise_eta=1e-3,\n",
    "                 beta=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Initialize the robust optimizer.\n",
    "        \n",
    "        Args:\n",
    "            params: List of parameter arrays to optimize\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "            max_grad_norm: Maximum allowed gradient norm\n",
    "            noise_eta: Initial gradient noise magnitude\n",
    "            beta: Decay rate for running averages\n",
    "            epsilon: Small constant for numerical stability\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.lr = learning_rate\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.noise_eta = noise_eta\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize running averages for preconditioning\n",
    "        self.running_square_avg = [\n",
    "            np.zeros_like(p) for p in params\n",
    "        ]\n",
    "        \n",
    "        self.step_count = 0\n",
    "        self.safeguards = OptimizationSafeguards()\n",
    "    \n",
    "    def step(self, gradients):\n",
    "        \"\"\"\n",
    "        Perform one optimization step with all safeguards.\n",
    "        \n",
    "        Args:\n",
    "            gradients: List of gradient arrays for each parameter\n",
    "        \"\"\"\n",
    "        # 1. Clip gradients\n",
    "        gradients = self.safeguards.clip_gradients(\n",
    "            gradients, self.max_grad_norm)\n",
    "        \n",
    "        # 2. Add noise\n",
    "        gradients = self.safeguards.add_gradient_noise(\n",
    "            gradients, self.step_count, self.noise_eta)\n",
    "        \n",
    "        # 3. Precondition gradients\n",
    "        gradients, self.running_square_avg = \\\n",
    "            self.safeguards.precondition_gradients(\n",
    "                gradients, self.running_square_avg,\n",
    "                self.beta, self.epsilon\n",
    "            )\n",
    "        \n",
    "        # 4. Update parameters\n",
    "        for param, grad in zip(self.params, gradients):\n",
    "            param -= self.lr * grad\n",
    "        \n",
    "        self.step_count += 1\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Reset gradients to zero.\"\"\"\n",
    "        # In practice, this would clear gradients in a deep learning framework\n",
    "        pass\n",
    "\n",
    "def demonstrate_robust_optimizer():\n",
    "    \"\"\"\n",
    "    Demonstrate the robust optimizer on a challenging problem.\n",
    "    \"\"\"\n",
    "    # Create a challenging optimization problem\n",
    "    def rosenbrock(x):\n",
    "        \"\"\"Rosenbrock function - challenging to optimize.\"\"\"\n",
    "        return sum(100.0*(x[1:] - x[:-1]**2.0)**2.0 + \n",
    "                  (1 - x[:-1])**2.0)\n",
    "    \n",
    "    def rosenbrock_grad(x):\n",
    "        \"\"\"Gradient of Rosenbrock function.\"\"\"\n",
    "        grad = np.zeros_like(x)\n",
    "        grad[0] = -400*x[0]*(x[1] - x[0]**2) - 2*(1 - x[0])\n",
    "        grad[-1] = 200*(x[-1] - x[-2]**2)\n",
    "        grad[1:-1] = (200*(x[1:-1] - x[:-2]**2) - \n",
    "                     400*x[1:-1]*(x[2:] - x[1:-1]**2) - \n",
    "                     2*(1 - x[1:-1]))\n",
    "        return grad\n",
    "    \n",
    "    # Initialize parameters and optimizer\n",
    "    x0 = np.random.randn(4)  # 4D Rosenbrock\n",
    "    optimizer = RobustOptimizer([x0], learning_rate=0.0001)\n",
    "    \n",
    "    # Optimize\n",
    "    trajectory = [x0.copy()]\n",
    "    losses = [rosenbrock(x0)]\n",
    "    \n",
    "    for t in range(10000):\n",
    "        grad = rosenbrock_grad(x0)\n",
    "        optimizer.step([grad])\n",
    "        \n",
    "        trajectory.append(x0.copy())\n",
    "        losses.append(rosenbrock(x0))\n",
    "        \n",
    "        if np.linalg.norm(grad) < 1e-6:\n",
    "            break\n",
    "    \n",
    "    trajectory = np.array(trajectory)\n",
    "    \n",
    "    # Visualize optimization progress\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(losses)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.title('Optimization Progress')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(trajectory)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Parameter Value')\n",
    "    plt.title('Parameter Trajectories')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_robust_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The robust optimizer combines all our safeguards to provide reliable optimization even in challenging scenarios. Let's analyze its behavior:\n",
    "\n",
    "1. **Initial Progress**\n",
    "   - Gradient clipping prevents large parameter jumps\n",
    "   - Added noise helps escape poor local minima\n",
    "   - Preconditioning helps handle different parameter scales\n",
    "\n",
    "2. **Middle Phase**\n",
    "   - Noise magnitude decreases as optimization progresses\n",
    "   - Running averages provide more stable parameter updates\n",
    "   - Clipping becomes less active as gradients naturally decrease\n",
    "\n",
    "3. **Final Convergence**\n",
    "   - Minimal noise allows precise convergence\n",
    "   - Preconditioning ensures all parameters converge at similar rates\n",
    "   - Gradient clipping no longer affects the small gradients\n",
    "\n",
    "This implementation demonstrates how combining multiple safeguards can create a robust optimization process that handles various numerical challenges while maintaining good convergence properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"beyond\"></a>\n",
    "# 7. Beyond Gradient Descent\n",
    "\n",
    "While gradient-based methods dominate machine learning optimization, there are important scenarios where alternative approaches shine. In this section, we'll explore methods that don't rely on gradients, with a focus on genetic algorithms (GA) and how they handle challenging non-convex optimization landscapes.\n",
    "\n",
    "## 7.1 When Gradient-Based Methods Fall Short\n",
    "\n",
    "Before diving into alternatives, let's understand when we might need them:\n",
    "\n",
    "1. **Non-differentiable Objectives**: Some problems involve discrete choices or discontinuous functions\n",
    "2. **Highly Non-convex Landscapes**: Multiple local minima where gradient descent gets stuck\n",
    "3. **Noisy or Black-box Functions**: When we can't compute reliable gradients\n",
    "4. **Constrained Optimization**: Complex constraints that are hard to enforce with gradients\n",
    "\n",
    "Let's create a challenging 3D test function to illustrate these scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def eggholder_function(x, y):\n",
    "    \"\"\"\n",
    "    The Eggholder function - a challenging non-convex optimization problem.\n",
    "    Global minimum: f(512, 404.2319) = -959.6407\n",
    "    \"\"\"\n",
    "    return -(y + 47) * np.sin(np.sqrt(abs(x/2 + (y + 47)))) \\\n",
    "           - x * np.sin(np.sqrt(abs(x - (y + 47))))\n",
    "\n",
    "def visualize_landscape():\n",
    "    x = np.linspace(-512, 512, 100)\n",
    "    y = np.linspace(-512, 512, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = eggholder_function(X, Y)\n",
    "    \n",
    "    # Create 3D surface plot\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Surface plot\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', \n",
    "                           linewidth=0, antialiased=True)\n",
    "    ax1.set_title('Eggholder Function Surface')\n",
    "    fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=5)\n",
    "    \n",
    "    # Contour plot\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    contour = ax2.contour(X, Y, Z, levels=30)\n",
    "    ax2.clabel(contour, inline=True, fontsize=8)\n",
    "    ax2.set_title('Eggholder Function Contours')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_landscape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Genetic Algorithms: Evolution-Inspired Optimization\n",
    "\n",
    "Genetic algorithms draw inspiration from natural selection to solve optimization problems. They maintain a population of candidate solutions and evolve them over generations through selection, crossover, and mutation.\n",
    "\n",
    "Here's a implementation of a genetic algorithm for our challenging landscape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GeneticOptimizer:\n",
    "    def __init__(self, bounds, pop_size=100, elite_size=10):\n",
    "        \"\"\"\n",
    "        Initialize genetic algorithm optimizer.\n",
    "        \n",
    "        Args:\n",
    "            bounds: List of (min, max) tuples for each dimension\n",
    "            pop_size: Population size\n",
    "            elite_size: Number of best individuals to preserve\n",
    "        \"\"\"\n",
    "        self.bounds = bounds\n",
    "        self.pop_size = pop_size\n",
    "        self.elite_size = elite_size\n",
    "        self.dim = len(bounds)\n",
    "        \n",
    "        # Initialize random population\n",
    "        self.population = np.array([\n",
    "            [np.random.uniform(low, high) \n",
    "             for (low, high) in bounds]\n",
    "            for _ in range(pop_size)\n",
    "        ])\n",
    "        \n",
    "    def select_parents(self, fitness_scores):\n",
    "        \"\"\"Tournament selection of parents.\"\"\"\n",
    "        selected = []\n",
    "        for _ in range(self.pop_size - self.elite_size):\n",
    "            # Random tournament\n",
    "            tournament_idx = np.random.choice(\n",
    "                self.pop_size, size=3, replace=False)\n",
    "            tournament_fitness = fitness_scores[tournament_idx]\n",
    "            winner_idx = tournament_idx[np.argmin(tournament_fitness)]\n",
    "            selected.append(self.population[winner_idx])\n",
    "        return np.array(selected)\n",
    "    \n",
    "    def crossover(self, parents):\n",
    "        \"\"\"Create offspring through crossover.\"\"\"\n",
    "        offspring = np.zeros((len(parents), self.dim))\n",
    "        \n",
    "        for i in range(0, len(parents), 2):\n",
    "            if i + 1 < len(parents):\n",
    "                # Blend crossover\n",
    "                alpha = np.random.random(self.dim)\n",
    "                offspring[i] = alpha * parents[i] + \\\n",
    "                             (1 - alpha) * parents[i+1]\n",
    "                offspring[i+1] = alpha * parents[i+1] + \\\n",
    "                               (1 - alpha) * parents[i]\n",
    "        \n",
    "        return offspring\n",
    "    \n",
    "    def mutate(self, offspring, mutation_rate=0.1):\n",
    "        \"\"\"Apply random mutations.\"\"\"\n",
    "        for i in range(len(offspring)):\n",
    "            if np.random.random() < mutation_rate:\n",
    "                # Add random noise to genes\n",
    "                noise = np.random.normal(0, 0.1, self.dim)\n",
    "                offspring[i] += noise\n",
    "                \n",
    "                # Ensure within bounds\n",
    "                for j, (low, high) in enumerate(self.bounds):\n",
    "                    offspring[i, j] = np.clip(offspring[i, j], \n",
    "                                            low, high)\n",
    "        \n",
    "        return offspring\n",
    "    \n",
    "    def optimize(self, objective_fn, n_generations=100):\n",
    "        \"\"\"Run genetic algorithm optimization.\"\"\"\n",
    "        best_fitness_history = []\n",
    "        best_solution = None\n",
    "        best_fitness = float('inf')\n",
    "        \n",
    "        for generation in range(n_generations):\n",
    "            # Evaluate fitness\n",
    "            fitness_scores = np.array([\n",
    "                objective_fn(*ind) for ind in self.population\n",
    "            ])\n",
    "            \n",
    "            # Track best solution\n",
    "            min_idx = np.argmin(fitness_scores)\n",
    "            if fitness_scores[min_idx] < best_fitness:\n",
    "                best_fitness = fitness_scores[min_idx]\n",
    "                best_solution = self.population[min_idx].copy()\n",
    "            \n",
    "            best_fitness_history.append(best_fitness)\n",
    "            \n",
    "            # Select elite individuals\n",
    "            elite_idx = np.argsort(fitness_scores)[:self.elite_size]\n",
    "            elite = self.population[elite_idx]\n",
    "            \n",
    "            # Select parents for next generation\n",
    "            parents = self.select_parents(fitness_scores)\n",
    "            \n",
    "            # Create offspring through crossover\n",
    "            offspring = self.crossover(parents)\n",
    "            \n",
    "            # Apply mutations\n",
    "            offspring = self.mutate(offspring)\n",
    "            \n",
    "            # Create new generation\n",
    "            self.population = np.vstack([elite, offspring])\n",
    "        \n",
    "        return best_solution, best_fitness_history\n",
    "\n",
    "def compare_optimization_methods():\n",
    "    \"\"\"Compare gradient descent vs genetic algorithm.\"\"\"\n",
    "    # Initialize optimizers\n",
    "    bounds = [(-512, 512), (-512, 512)]\n",
    "    ga_opt = GeneticOptimizer(bounds, pop_size=100)\n",
    "    \n",
    "    # Gradient descent path\n",
    "    x_gd = np.array([200., 200.])  # Starting point\n",
    "    lr = 0.1\n",
    "    gd_path = [x_gd.copy()]\n",
    "    \n",
    "    # Approximate gradients with finite differences\n",
    "    def numerical_gradient(x):\n",
    "        eps = 1e-7\n",
    "        grad = np.zeros_like(x)\n",
    "        for i in range(len(x)):\n",
    "            x_plus = x.copy()\n",
    "            x_plus[i] += eps\n",
    "            x_minus = x.copy()\n",
    "            x_minus[i] -= eps\n",
    "            grad[i] = (eggholder_function(*x_plus) - \n",
    "                      eggholder_function(*x_minus)) / (2*eps)\n",
    "        return grad\n",
    "    \n",
    "    # Run gradient descent\n",
    "    for _ in range(100):\n",
    "        grad = numerical_gradient(x_gd)\n",
    "        x_gd = x_gd - lr * grad\n",
    "        x_gd = np.clip(x_gd, -512, 512)  # Stay in bounds\n",
    "        gd_path.append(x_gd.copy())\n",
    "    \n",
    "    # Run genetic algorithm\n",
    "    best_solution, ga_history = ga_opt.optimize(eggholder_function)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot optimization landscape with paths\n",
    "    plt.subplot(121)\n",
    "    x = np.linspace(-512, 512, 100)\n",
    "    y = np.linspace(-512, 512, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = eggholder_function(X, Y)\n",
    "    \n",
    "    plt.contour(X, Y, Z, levels=30)\n",
    "    gd_path = np.array(gd_path)\n",
    "    plt.plot(gd_path[:, 0], gd_path[:, 1], 'r.-', \n",
    "            label='Gradient Descent', alpha=0.7)\n",
    "    plt.plot(best_solution[0], best_solution[1], 'g*', \n",
    "            markersize=15, label='GA Solution')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Optimization Paths')\n",
    "    plt.legend()\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Plot convergence\n",
    "    plt.subplot(122)\n",
    "    gd_values = [eggholder_function(*x) for x in gd_path]\n",
    "    plt.plot(gd_values, 'r-', label='Gradient Descent')\n",
    "    plt.plot(ga_history, 'g-', label='Genetic Algorithm')\n",
    "    plt.xlabel('Iteration/Generation')\n",
    "    plt.ylabel('Best Function Value')\n",
    "    plt.title('Convergence Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_optimization_methods()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Understanding the Results\n",
    "\n",
    "The comparison reveals several key insights:\n",
    "\n",
    "1. **Local vs Global Search**\n",
    "   - Gradient descent quickly finds a local minimum but gets stuck\n",
    "   - Genetic algorithm explores more broadly and can find better solutions\n",
    "\n",
    "2. **Exploration vs Exploitation**\n",
    "   - GA balances exploration (mutation) with exploitation (selection)\n",
    "   - This balance helps avoid premature convergence\n",
    "\n",
    "3. **Robustness to Landscape**\n",
    "   - GA doesn't require gradient information\n",
    "   - Can handle discontinuities and multiple local minima\n",
    "\n",
    "## 7.4 When to Use Alternative Methods\n",
    "\n",
    "Choose alternative optimization methods when:\n",
    "\n",
    "1. **Problem Characteristics**\n",
    "   - Non-differentiable or discrete objectives\n",
    "   - Many local minima\n",
    "   - Black-box optimization\n",
    "\n",
    "2. **Computational Resources**\n",
    "   - Parallel evaluation possible (GA population)\n",
    "   - Function evaluation is cheap\n",
    "   - Global optimum is important\n",
    "\n",
    "3. **Problem Knowledge**\n",
    "   - Limited understanding of objective landscape\n",
    "   - No gradient information available\n",
    "   - Complex constraints\n",
    "\n",
    "The Eggholder function demonstrates why these alternatives are valuable:\n",
    "- Multiple deep local minima\n",
    "- Sharp ridges and valleys\n",
    "- Large search space\n",
    "- Challenging gradient computation\n",
    "\n",
    "Let's see how the genetic algorithm handles different population sizes and mutation rates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_ga_parameters():\n",
    "    \"\"\"Analyze how GA parameters affect optimization.\"\"\"\n",
    "    pop_sizes = [20, 100, 500]\n",
    "    mutation_rates = [0.01, 0.1, 0.3]\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Test population size\n",
    "    plt.subplot(121)\n",
    "    for pop_size in pop_sizes:\n",
    "        ga_opt = GeneticOptimizer(bounds=[(-512, 512), (-512, 512)],\n",
    "                                 pop_size=pop_size)\n",
    "        _, history = ga_opt.optimize(eggholder_function)\n",
    "        plt.plot(history, label=f'Pop Size = {pop_size}')\n",
    "    \n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Best Fitness')\n",
    "    plt.title('Effect of Population Size')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Test mutation rate\n",
    "    plt.subplot(122)\n",
    "    ga_opt = GeneticOptimizer(bounds=[(-512, 512), (-512, 512)])\n",
    "    \n",
    "    for rate in mutation_rates:\n",
    "        ga_opt.population = np.random.uniform(-512, 512, \n",
    "                                            (100, 2))  # Reset\n",
    "        _, history = ga_opt.optimize(\n",
    "            eggholder_function,\n",
    "            mutation_rate=rate\n",
    "        )\n",
    "        plt.plot(history, label=f'Mutation = {rate}')\n",
    "    \n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Best Fitness')\n",
    "    plt.title('Effect of Mutation Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_ga_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Other Alternative Methods\n",
    "\n",
    "While we focused on genetic algorithms, several other alternative optimization methods exist:\n",
    "\n",
    "1. **Particle Swarm Optimization (PSO)**\n",
    "   - Inspired by social behavior of bird flocking\n",
    "   - Particles move through search space influenced by local and global best solutions\n",
    "   - Good for continuous optimization problems\n",
    "\n",
    "2. **Simulated Annealing**\n",
    "   - Inspired by physical annealing process in metallurgy\n",
    "   - Gradually decreases randomness in search\n",
    "   - Good for discrete optimization problems\n",
    "\n",
    "3. **Differential Evolution**\n",
    "   - Similar to GA but with different mutation and crossover strategies\n",
    "   - Often performs well on numerical optimization problems\n",
    "   - Good for high-dimensional continuous optimization\n",
    "\n",
    "These methods share common themes:\n",
    "- Population-based search\n",
    "- Stochastic exploration\n",
    "- No gradient requirements\n",
    "- Natural parallelization\n",
    "\n",
    "Understanding these alternatives broadens our optimization toolkit and helps us tackle a wider range of problems effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"practical\"></a>\n",
    "# 6. Practical Considerations in Optimization\n",
    "\n",
    "When we move from theory to practice in neural network optimization, several key considerations become crucial for success. Understanding these practical aspects helps us build more effective models and troubleshoot problems when they arise.\n",
    "\n",
    "## 6.1 Weight Initialization Strategies\n",
    "\n",
    "The choice of initial weights significantly impacts optimization success. Poor initialization can lead to either vanishing or exploding gradients, making learning difficult or impossible. Let's understand why initialization matters and how to do it effectively.\n",
    "\n",
    "When we initialize weights, we need to consider several factors:\n",
    "\n",
    "First, the variance of the initialized weights affects the variance of activations flowing through the network. If we make the weights too large, activations can explode; if we make them too small, signals can vanish as they propagate through layers. We typically want activations to maintain a reasonable scale throughout the network.\n",
    "\n",
    "Second, the relationship between input and output dimensions matters. A layer with many inputs needs smaller weights than one with few inputs to maintain consistent activation scales. This insight leads to popular initialization schemes like Xavier/Glorot initialization, which scales weights based on the number of input and output connections.\n",
    "\n",
    "Third, initialization can affect the symmetry of learning. If we initialize all weights to the same value, different units in a layer will compute identical outputs and receive identical gradient updates, effectively reducing the network's capacity. Adding randomness helps break this symmetry and allows different units to learn different features.\n",
    "\n",
    "Let's look at an optimization landscape under different initialization schemes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_initialization_impact():\n",
    "    \"\"\"Visualize how different initializations affect optimization.\"\"\"\n",
    "    # Create a simple 2D loss landscape\n",
    "    x = np.linspace(-2, 2, 100)\n",
    "    y = np.linspace(-2, 2, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = 0.1*(X**2 + Y**2) + np.exp(-5*(X**2 + Y**2))*np.sin(10*X)*np.cos(10*Y)\n",
    "    \n",
    "    # Different initialization strategies\n",
    "    inits = {\n",
    "        'Too Small': np.array([-0.01, -0.01]),\n",
    "        'Too Large': np.array([-2.0, 2.0]),\n",
    "        'Good': np.array([-0.5, 0.5])\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, (name, init) in enumerate(inits.items(), 1):\n",
    "        plt.subplot(1, 3, i)\n",
    "        plt.contour(X, Y, Z, levels=20)\n",
    "        plt.plot(init[0], init[1], 'r*', markersize=15, label='Init point')\n",
    "        \n",
    "        # Show optimization trajectory\n",
    "        point = init.copy()\n",
    "        trajectory = [point.copy()]\n",
    "        \n",
    "        for _ in range(50):\n",
    "            # Compute gradient (using finite differences for visualization)\n",
    "            grad_x = (Z[50, 51] - Z[50, 49]) / (x[1] - x[0])\n",
    "            grad_y = (Z[51, 50] - Z[49, 50]) / (y[1] - y[0])\n",
    "            grad = np.array([grad_x, grad_y])\n",
    "            \n",
    "            # Update point\n",
    "            point = point - 0.1 * grad\n",
    "            trajectory.append(point.copy())\n",
    "        \n",
    "        trajectory = np.array(trajectory)\n",
    "        plt.plot(trajectory[:, 0], trajectory[:, 1], 'b.-', alpha=0.5)\n",
    "        plt.title(f'{name} Initialization')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Learning Rate Dynamics\n",
    "\n",
    "Understanding how learning rates affect optimization helps us choose appropriate values and schedules. The learning rate isn't just a speed parameter – it fundamentally affects whether and how we reach good solutions.\n",
    "\n",
    "The effective learning rate depends on several factors:\n",
    "\n",
    "First, gradient magnitudes vary throughout training. Early in training, gradients are often larger as the model makes big corrections. Later, as the model approaches a good solution, gradients typically become smaller. This suggests that we might want to adjust our learning rate over time.\n",
    "\n",
    "Second, different parts of the model might need different learning rates. Some parameters might be fine-tuned versions of pretrained features, while others might be learning from scratch. This insight led to the development of adaptive learning rate methods like Adam.\n",
    "\n",
    "Third, the batch size affects the appropriate learning rate. Larger batches give more stable gradient estimates, allowing larger learning rates. However, there's often a sweet spot beyond which larger batches don't help much.\n",
    "\n",
    "## 6.3 Monitoring and Debugging\n",
    "\n",
    "When optimization isn't working well, systematic monitoring and debugging become essential. Several key metrics can help us identify and fix problems:\n",
    "\n",
    "The loss curve provides our first indication of training progress. A healthy loss curve should generally decrease over time, though not necessarily monotonically. Plateaus, spikes, or oscillations can indicate different types of problems:\n",
    "\n",
    "First, if the loss plateaus very early, we might have a learning rate that's too small, causing the optimization to stall. Alternatively, we might have initialized the weights poorly, leaving the model stuck in a bad region of the loss landscape.\n",
    "\n",
    "Second, if the loss oscillates wildly, our learning rate might be too large, causing the optimization to overshoot good solutions. This often shows up as spikes in the loss curve.\n",
    "\n",
    "Third, if the loss decreases very slowly, we might need to adjust our batch size or consider a different optimizer. Sometimes, the slow progress indicates that our model architecture needs revision.\n",
    "\n",
    "Let's examine some common debugging metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_debugging_metrics():\n",
    "    \"\"\"Visualize key metrics for debugging optimization.\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Generate synthetic training data\n",
    "    iterations = np.arange(100)\n",
    "    \n",
    "    # Simulate different problematic scenarios\n",
    "    plt.subplot(131)\n",
    "    # Healthy vs problematic loss curves\n",
    "    plt.plot(iterations, 1/(1 + 0.1*iterations), 'b-', \n",
    "            label='Healthy')\n",
    "    plt.plot(iterations, 1/(1 + 0.02*iterations) + \n",
    "             0.2*np.sin(iterations*0.5), 'r-', \n",
    "            label='Oscillating')\n",
    "    plt.plot(iterations, 1 - 0.1*np.exp(-0.05*iterations), 'g-',\n",
    "             label='Plateauing')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Gradient norms\n",
    "    plt.subplot(132)\n",
    "    plt.plot(iterations, np.exp(-0.05*iterations), 'b-',\n",
    "             label='Healthy')\n",
    "    plt.plot(iterations, 10*np.exp(-0.05*iterations), 'r-',\n",
    "             label='Too large')\n",
    "    plt.plot(iterations, 0.1*np.exp(-0.05*iterations), 'g-',\n",
    "             label='Too small')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.title('Gradient Behavior')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Learning progress\n",
    "    plt.subplot(133)\n",
    "    plt.plot(iterations, 1 - np.exp(-0.05*iterations), 'b-',\n",
    "             label='Training')\n",
    "    plt.plot(iterations, \n",
    "            0.8*(1 - np.exp(-0.05*iterations)) + 0.1, 'r--',\n",
    "            label='Validation')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Learning Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_debugging_metrics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Practical Guidelines\n",
    "\n",
    "Drawing from our understanding of optimization fundamentals, here are key guidelines for practice:\n",
    "\n",
    "Start with established defaults. For basic neural networks, certain configurations tend to work well as starting points. These include:\n",
    "- Learning rate around 0.001\n",
    "- Adam optimizer\n",
    "- Xavier/Glorot initialization\n",
    "- Batch size of 32 or 64\n",
    "\n",
    "Monitor the right metrics. Beyond just watching the loss, pay attention to:\n",
    "- Gradient norms (for detecting vanishing/exploding gradients)\n",
    "- Parameter updates (for checking learning progress)\n",
    "- Validation metrics (for catching overfitting early)\n",
    "\n",
    "Use appropriate validation strategies. Cross-validation helps assess model performance, but be mindful of:\n",
    "- Training/validation split ratios\n",
    "- Stratification for imbalanced data\n",
    "- Proper shuffling of data\n",
    "\n",
    "These practical considerations form the foundation for successful neural network optimization. As we move forward to study specific neural network architectures, we'll build upon these principles and learn how they apply in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "# 8. Conclusion: The Art and Science of Optimization\n",
    "\n",
    "Throughout this exploration of optimization, we've journeyed from mathematical foundations to practical implementations, building a comprehensive understanding of how we can efficiently find solutions to complex problems. Let's synthesize what we've learned and consider how these concepts will serve as building blocks for our future exploration of neural networks.\n",
    "\n",
    "## The Optimization Landscape\n",
    "\n",
    "We began by understanding optimization as a mathematical challenge: finding the minimum of a function in a potentially vast and complex space. This seemingly abstract concept turned out to be deeply practical – it's exactly what we're doing when we train any machine learning model. The loss landscape represents all possible configurations of our model, and optimization helps us navigate this landscape to find configurations that perform well.\n",
    "\n",
    "We learned that optimization landscapes can be deceptively complex. Even simple neural networks can create loss surfaces with multiple local minima, saddle points, and regions of varying curvature. This understanding helps explain why optimization is as much an art as it is a science – we're not just following gradients blindly, but rather crafting strategies to navigate these challenging terrains effectively.\n",
    "\n",
    "## The Evolution of Methods\n",
    "\n",
    "Our journey through optimization methods revealed an interesting progression. We started with simple gradient descent, understanding its fundamental principles and limitations. This foundation helped us appreciate why more sophisticated methods were developed:\n",
    "\n",
    "Stochastic methods taught us about the trade-off between computation speed and gradient accuracy. By using mini-batches, we could make faster progress while maintaining reasonable gradient estimates. This insight is crucial for practical deep learning, where data sets are often too large to process all at once.\n",
    "\n",
    "Adaptive methods like Adam showed us how we can automatically adjust learning rates for different parameters. This advancement addressed a key limitation of simple gradient descent – the fact that different parameters often need different scales of updates. Understanding these methods helps us make informed choices about which optimizer to use for different problems.\n",
    "\n",
    "The exploration of alternative approaches, like genetic algorithms, reminded us that gradients aren't the only way to optimize. Sometimes, when our problem has certain characteristics (like being non-differentiable or having many local minima), we might need to think outside the gradient-based box.\n",
    "\n",
    "## Practical Wisdom\n",
    "\n",
    "Perhaps most importantly, we've gained practical insights that will serve us well in actual implementation:\n",
    "\n",
    "Initialization matters more than might be immediately obvious. Poor initialization can doom our optimization before it begins, while good initialization can make the difference between a model that learns effectively and one that struggles to make progress.\n",
    "\n",
    "Learning rate dynamics require careful attention. We learned that the learning rate isn't just a speed parameter – it fundamentally affects whether and how we reach good solutions. Understanding learning rate scheduling and adaptation helps us design more effective training procedures.\n",
    "\n",
    "Monitoring and debugging are essential skills. We now know what metrics to watch, what patterns might indicate problems, and how to systematically address issues when they arise. This practical knowledge will be invaluable as we move forward to build and train actual neural networks.\n",
    "\n",
    "## Looking Ahead\n",
    "\n",
    "As we prepare to dive into neural networks in our next notebook, the optimization principles we've learned will become concrete tools in our machine learning toolkit. When we build our first multilayer perceptron, we'll see these concepts in action:\n",
    "\n",
    "The initialization strategies we discussed will help us start training from a good position. We'll understand why certain initialization schemes work better than others because we understand the underlying optimization dynamics.\n",
    "\n",
    "Our knowledge of optimizers will help us make informed choices about training procedures. Whether we're using SGD with momentum or Adam, we'll understand what's happening under the hood and why we might prefer one approach over another.\n",
    "\n",
    "The debugging and monitoring techniques we've learned will help us diagnose and fix problems when they arise. We'll be able to look at a learning curve and understand what it's telling us about our training process.\n",
    "\n",
    "## The Road Forward\n",
    "\n",
    "As we continue our journey into deep learning, remember that optimization is a foundational skill that underlies everything else. The principles we've learned here will appear repeatedly as we explore more advanced architectures and techniques. Each new type of neural network will present its own optimization challenges, but the fundamental concepts we've mastered will help us understand and address them.\n",
    "\n",
    "Most importantly, we've learned that optimization is not just about following a recipe. It's about understanding the underlying principles and using them to make informed decisions. As we move forward, we'll build on this foundation, applying these concepts to increasingly sophisticated models and problems.\n",
    "\n",
    "The next time you're training a neural network and watching those loss curves descend, you'll have a deep understanding of what's really happening under the hood. You'll know why certain choices were made in the implementation, and you'll be equipped to make informed decisions when you need to adapt these methods to your own unique problems.\n",
    "\n",
    "Our journey through optimization has given us the tools we need to begin building and training neural networks effectively. In our next notebook, we'll put these tools to use as we explore the fundamental building block of deep learning: the multilayer perceptron. We'll see how the optimization concepts we've learned help us train these networks effectively, setting the stage for even more advanced architectures to come.\n",
    "\n",
    "The art and science of optimization will continue to evolve, but the fundamental principles we've learned will remain valuable throughout your journey in machine learning and deep learning. Keep these concepts in mind as we move forward – they'll serve as a strong foundation for everything that follows."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
